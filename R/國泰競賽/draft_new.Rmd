---
title: "Cathay_EDA"
author: ""
date: "2019/9/14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(pacman)
p_load(tidyverse, funModeling, mice, corrplot, random, caret, plyr, magrittr, caretEnsemble, keras, FactoMineR,doParallel)
#setwd("~/Desktop/Ｒ/data")
```

刪除是否有訂閱電子報58-60、採用其他理財工具的資料  

```{r, message=FALSE}
train <- read_csv('train.csv', locale = locale(encoding = 'big5')) 
test <- read_csv('test.csv', locale = locale(encoding = 'big5')) 
test_ID <- read_csv('test.csv', locale = locale(encoding = 'big5')) [, 1]
```

117-121附約資訊，各項附約為N則全為NA
122 NA時，主約數均為0，改N

125-131拿掉

========
定義函數
========
```{r}
##########
#移除欄位#
##########
data_cleaning <- function(data){
  yes_no <- c('Y' = 1, 'N' = 0)
  gender_index <- c('M' = 0, 'F' = 1)
  level_4num <- c("低" = 1, "中" = 2, "中高" = 3, "高" = 4)
  level_3num <- c("低" = 1, "中" = 2, "高" = 3)
  
  data$GENDER <- as.integer(revalue(data$GENDER, gender_index))
  data$AGE <- as.integer(revalue(data$AGE, level_4num))
  data$APC_1ST_AGE <- as.integer(revalue(data$APC_1ST_AGE, level_4num))
  data$INSD_1ST_AGE <- as.integer(revalue(data$INSD_1ST_AGE, level_4num))
  data$RFM_R <- as.integer(revalue(data$RFM_R, level_4num))
  data$REBUY_TIMES_CNT <- as.integer(revalue(data$REBUY_TIMES_CNT, level_4num))
  data$LIFE_CNT <- as.integer(revalue(data$LIFE_CNT, level_3num))
  
  
  data[is.na(data[, 2]), 2] <- 
    sample(as.list(c(0,1)), 1, replace = T, prob = c(463, 537))  #性別依原始比例隨機填入
  data[is.na(data[, 6]), 6] <- 0 #edu
  data[is.na(data[, 7]), 7] <- 0 #marriage
  data[is.na(data[, 15]), 15] <- 0 #occupation
  data[is.na(data[, 25]), 25] <- 0 #total(level)
  data[is.na(data[, 62]), 62] <- 0 #last year in C
  data[is.na(data[, 18]), 22] <- 4 #上次要保人身份投保距今間隔時間(年)(級距)，設最長
  data[is.na(data[, 18]), 18] <- 5 #首次擔任要保人年齡級距
  data[is.na(data[, 22]), 22] <- 4 #上次要保人身份投保距今間隔時間(年)(級距)
  data[is.na(data[, 19]), 19] <- 5 #首次擔任被保人年齡級距
  data[is.na(data[, 23]), 23] <- 1 #再購次數
  data[is.na(data[, 24]), 24] <- 0 #往來關係
  data[is.na(data[, 50]), 50] <- 0 #annual premium
  data[is.na(data[, 54]), 54] <- mean(data$ANNUAL_INCOME_AMT, na.rm = T) #INCOME 平均補
  data[is.na(data[, 64]), 64] <- 1 #最近一次為要被保人時間間格，設最長
  data[is.na(data[, 21]), 21] <- 1 #最近一次為保人時間間格，設最長
  data[is.na(data[, 73]), 73] <- 0 #違約 NA為未購買任何主約
  
  #for (i in which(is.na(train$BMI))) {
  #  train[i, 'BMI'] <- sample(train$BMI[c(which(!is.na(train$BMI)))], 1)
  #    #rnorm(1, mean(train$BMI, na.rm = T), sd(train$BMI, na.rm = T))
  #}
  
  for (i in 74:81) {
    data[is.na(data[, i]), i] <- "N"
  }
  
  for (i in 83:97) {
    data[is.na(data[, i]), i] <- 0
  }
  
  for (i in 100:116) {
    data[is.na(data[, i]), i] <- "N"
  }
  
  for (i in 117:122) {
    data[is.na(data[, i]), i] <- "N"
  }
  
  for (i in 58:60) {
    data[is.na(data[, i]), i] <- "N"
  }
  
  for (i in 125:131) {
    data[is.na(data[, i]), i] <- "N"
  }
  #data <- data %>% select(-c(100:121)) # 被保主附約
  #data <- data %>% select(-c(74:81)) #要保主約
  #data <- data %>% select(-c(58:60)) #電子報
  #data <- data %>% select(-c(27:48))
  #data <- data %>% 
  data <- data %>% select(-c('REBUY_TIMES_CNT', 
              'AG_CNT', 'LEVEL', 
              'LIFE_CNT', 
              'INSD_1ST_AGE', 
              'DIEACCIDENT_AMT', 
              'DISEASES_HOSPITAL_REC_AMT', 
              'OUTPATIENT_SURGERY_AMT', 
              'BMI'))
  #data <- data %>% select(-c("BMI"))
  return(data)
}

##########
#輸出結果#
##########
write_pred <- function(idx, pred) {
  #pred %<>% as_tibble()
  test_01 <- cbind(idx, pred) %>% as_tibble()
  colnames(test_01) <- c("CUS_ID", "Ypred")
  test_01$Ypred <- as.character(test_01$Ypred)
  test_01$Ypred[test_01$Ypred == "N"] <- 0
  test_01$Ypred[test_01$Ypred == "Y"] <- 1
  write.csv(test_01,
          '/Users/tommy84729/Desktop/test_01.csv',
            row.names = F)
}

##########
#樣本抽樣#
##########
new_data_oversampling <-
  function(data, oversampTimes, downsampFrac) {
    train_n <- data %>% filter(Y1 == 'N')
    train_y <- data %>% filter(Y1 == 'Y')
    train_y_big <-
      lapply(train_y, rep.int, times = oversampTimes) %>% as_tibble()
    train_n_small <-
      sample_frac(train_n, downsampFrac) %>% as_tibble()
    train_new <- rbind(train_y_big, train_n_small)
    return(train_new)
  }

##########
#變數萃取#
##########
extract_imp <- function(model, number = 20) {
  model_imp <- varImp(model)
  model_imp <-model_imp[["importance"]] %>% mutate(var = row.names(model_imp[["importance"]]))
  row.names(model_imp) <- c()
  model_imp %<>% select(c("var", "Overall")) %>% arrange(desc(Overall))
  return(model_imp[1:number, 1])
}
```

```{r}
train %<>% data_cleaning()
test %<>% data_cleaning()
df_status(train)
```


==========
文字轉類別
==========
```{r}
for (i in colnames(train)) {
  if (is.character(train[[i]]) == T) {
    train[[i]] %<>% as_factor()
  }
}

for (i in colnames(test)) {
  if (is.character(test[[i]]) == T) {
    test[[i]] %<>% as_factor()
  }
}
#
levels <- c("CUS_ID", "GENDER", "MARRIAGE_CD")

for (i in levels) {
  train[[i]] %<>% as_factor()
}
for (i in levels) {
  test[[i]] %<>% as_factor()
}
```

====
Log
====
```{r}
nums_tr <- unlist(lapply(train, is.numeric))
train[,nums_tr]<-log1p(train[,nums_tr])

nums_te <- unlist(lapply(test, is.numeric))
test[,nums_te]<-log1p(test[,nums_te])
```




============
standardized
============
```{r}
nums_tr <- unlist(lapply(train, is.numeric))
pre_train <- preProcess(train[, nums_tr], method = c("center", "scale"))
train[, nums_tr] <- predict(pre_train, train[, nums_tr])
train$CUS_ID <- as.numeric(as.character(train$CUS_ID))

nums_te <- unlist(lapply(test, is.numeric))
pre_test <- preProcess(test[, nums_te], method = c("center", "scale"))
test[, nums_te] <- predict(pre_test, test[, nums_te])
test$CUS_ID <- as.numeric(as.character(test$CUS_ID))


```

========
上下抽樣
========
```{r}
train_over <- new_data_oversampling(train, 1, 0.02)
```

============
萃取數值變數
============
```{r}
corrplot.mixed(
  cor(train[, nums_tr], use = "na.or.complete"),
  upper = 'ellipse',
  order = 'hclust',
  sig.level = 0.01,
  insig = "blank",
  tl.pos = 'lt',
  tl.cex = 0.5,
  cl.cex = 0.7,
  number.cex = 0.5,
  addCoefasPercent = TRUE
)
```

========
mycontrol
========
```{r}
myControl <-
  trainControl(
    method = "cv",
    number = 5,
    verboseIter = T,
    savePredictions = T,
    classProbs = F
  )
```

===
重複隨機抽樣SVM
===
```{r}
train_y <- train %>% filter(Y1 == 'Y')
train_n <- train %>% filter(Y1 == 'N')
pred<-NULL

cl <- makePSOCKcluster(4)#幾核
registerDoParallel(cl)
for (i in 1:11) {
  train_idx <- sample(1:nrow(train_n), 2000, F)
  train_new <- rbind(train_n[train_idx, ], train_y)
  model.svm <- train(
    Y1 ~ . - CUS_ID - IF_ISSUE_INSD_E_IND - IF_ISSUE_E_IND - X_F_IND - IF_ISSUE_H_IND -
      IF_ISSUE_INSD_H_IND - X_G_IND - IF_ISSUE_K_IND - FINANCETOOLS_E - L1YR_B_ISSUE_CNT -
      IF_ISSUE_INSD_K_IND - IF_ISSUE_M_IND - IF_ISSUE_O_IND - LAST_B_ISSUE_DT -
      C_IND - IF_ADD_G_IND - IF_ISSUE_INSD_M_IND - IM_IS_A_IND - IF_ISSUE_A_IND -
      IF_ISSUE_B_IND - IF_ISSUE_L_IND - IF_ISSUE_INSD_O_IND - IF_ADD_INSD_G_IND -
      IF_ISSUE_F_IND - IF_ISSUE_INSD_A_IND - IF_ISSUE_INSD_L_IND - FINANCETOOLS_D -
      X_A_IND - CHANNEL_B_POL_CNT - IF_ISSUE_INSD_F_IND - FINANCETOOLS_F - IF_ISSUE_C_IND -
      IF_ISSUE_INSD_B_IND,
    data = train_new,
    tunlength = 5,
    #tuneGrid = expand.grid(C = seq(1, 2, 0.25), sigma = 0.008, Weight = seq(1, 3, 1)),
    method = "svmRadial",
    trControl = myControl,
    na.action = na.omit
  )
  pred <- cbind(pred, predict(model.svm, newdata = test))
}
stopCluster(cl)

pred<-as.data.frame(pred)
pred$sum<-rowSums(pred)
pred$Y1<-ifelse(pred$sum>=16,1,0)
count(pred$Y1)
#write_pred(test_ID$CUS_ID, pred$Y1)


```

===
重複隨機抽樣RF
===

```{r}
train_y <- train %>% filter(Y1 == 'Y')
train_n <- train %>% filter(Y1 == 'N')
pred1 <- NULL

cl <- makePSOCKcluster(4)#幾核
registerDoParallel(cl)
for (i in 1:11){
  train_idx <- sample(1:nrow(train_n), 2000, F)
  train_new <- rbind(train_n[train_idx,], train_y)
  model.ranger <- train(
  Y1 ~ . - CUS_ID - IF_ISSUE_INSD_E_IND - IF_ISSUE_E_IND - X_F_IND - IF_ISSUE_H_IND -
    IF_ISSUE_INSD_H_IND - X_G_IND - IF_ISSUE_K_IND - FINANCETOOLS_E - L1YR_B_ISSUE_CNT -
    IF_ISSUE_INSD_K_IND - IF_ISSUE_M_IND - IF_ISSUE_O_IND - LAST_B_ISSUE_DT -
    C_IND - IF_ADD_G_IND - IF_ISSUE_INSD_M_IND - IM_IS_A_IND - IF_ISSUE_A_IND -
    IF_ISSUE_B_IND - IF_ISSUE_L_IND - IF_ISSUE_INSD_O_IND - IF_ADD_INSD_G_IND -
    IF_ISSUE_F_IND - IF_ISSUE_INSD_A_IND - IF_ISSUE_INSD_L_IND - FINANCETOOLS_D -
    X_A_IND - CHANNEL_B_POL_CNT - IF_ISSUE_INSD_F_IND - FINANCETOOLS_F - IF_ISSUE_C_IND -
    IF_ISSUE_INSD_B_IND, 
  data = train_new,
  tuneLength = 3,
  method = "ranger",
  trControl = myControl,
  importance = "impurity",
  na.action = na.omit
)
  pred1 <- cbind(pred1, predict(model.ranger, newdata = test))
}
stopCluster(cl)

pred1 <- as.data.frame(pred1)
pred1$sum <- rowSums(pred1)
pred1$Y1 <- ifelse(pred1$sum >= 16, 1, 0)
count(pred1$Y1)
#write_pred(test_ID$CUS_ID, pred$Y1)

```

===
合併結果（SVM+RF+XGB）
===
```{r}
setwd("~/Desktop/Ｒ/data")
all<-read_csv('test_01_XGB.csv')
all<-cbind(all,pred$Y1,pred1$Y1)
all$sum<-rowSums(all[,2:4])
all$Y1<-ifelse(all$sum>=2,1,0)
count(all$Y1)
write_pred(test_ID,all$Y1)
```



====
PCA
====
```{r}
nums_tr <- unlist(lapply(train, is.numeric))
train_num <- train[, nums_tr]
pca <- prcomp(formula = ~ .,  #選擇七個變數 
              data = train_num,                           # 資料
              scale = TRUE) 

pca$rotation
plot(pca, type = "line")
abline(h=1, col="blue")

top8 <- pca$x[, 1:8]
top8 %<>% as_tibble()
top8 <- cbind(top8, train$Y1)
colnames(top8)[length(top8)] <- "Y1"
imp_fac <- c('OCCUPATION_CLASS_CD', 'AGE', 'LAST_A_CCONTACT_DT', 'LAST_A_ISSUE_DT', 'X_H_IND')
top8 <- bind_cols(top8, train %>% select(imp_fac))


top8 <- new_data_oversampling(top8, 1, 0.05)
model.ranger <- train(
  Y1 ~ .,
  data = top8,
  tuneLength = 3,
  method = "ranger",
  trControl = myControl,
  importance = "permutation",
  na.action = na.omit
)

pca_test <- predict(pca, newdata = test)
pca_test <- bind_cols(pca_test[, 1:8] %<>% as_tibble(), test %>% select(imp_fac))
pred_ranger <- predict(model.ranger, newdata = pca_test)
count(pred_ranger)
write_pred(test$CUS_ID, pred_ranger)
```

```{r}
train_fac <- train[ , !nums_tr]
train_mca <- cbind(train_num, train_fac)

res.mca <- MCA(train_mca, 
              quanti.sup = 1:40, # Supplementary quantitative variable
              #quali.sup = 42:79,  # Supplementary qualitative variable
              graph=F)
```

===========
XG Boosting
===========
```{r}
grid <- expand.grid(
  max_depth = 5,
  nrounds = 50,
  eta =  .01,
  colsample_bytree = 0.7,
  gamma = 0.1,
  min_child_weight = 1,
  subsample = .6,
  rate_drop = c(.1, .3),
  skip_drop = c(.1, .3)
)
model.xgb <- train(
  Y1 ~ .,
  data = train_over,
  #tuneLength = 3,
  tunegrid = grid,
  method = "xgbDART",
  metric = 'Accuracy',
  trControl = myControl
  #na.action = na.omit
)
model.xgb

pred_xgb <- predict(model.xgb, newdata = test)

count(pred_xgb)
```

========
logistic
========
```{r}
model.logistic <- train(
  Y1 ~ . - CUS_ID,
  data = train_over,
  tuneLength = 5,
  method = "glmnet",
  trControl = myControl,
  family = "binomial",
  na.action = na.omit
)
model.logistic

pred_logistic <- predict(model.logistic, newdata = test)
count(pred_logistic)
```

========
SVM
========
```{r}
cl <- makePSOCKcluster(4)#幾核
registerDoParallel(cl)
model.svm <- train(
  Y1 ~ . - CUS_ID - IF_ISSUE_INSD_E_IND - IF_ISSUE_E_IND - X_F_IND - IF_ISSUE_H_IND -
    IF_ISSUE_INSD_H_IND - X_G_IND - IF_ISSUE_K_IND - FINANCETOOLS_E - L1YR_B_ISSUE_CNT -
    IF_ISSUE_INSD_K_IND - IF_ISSUE_M_IND - IF_ISSUE_O_IND - LAST_B_ISSUE_DT -
    C_IND - IF_ADD_G_IND - IF_ISSUE_INSD_M_IND - IM_IS_A_IND - IF_ISSUE_A_IND -
    IF_ISSUE_B_IND - IF_ISSUE_L_IND - IF_ISSUE_INSD_O_IND - IF_ADD_INSD_G_IND -
    IF_ISSUE_F_IND - IF_ISSUE_INSD_A_IND - IF_ISSUE_INSD_L_IND - FINANCETOOLS_D -
    X_A_IND - CHANNEL_B_POL_CNT - IF_ISSUE_INSD_F_IND - FINANCETOOLS_F - IF_ISSUE_C_IND -
    IF_ISSUE_INSD_B_IND,
  data = train_new,
  tunlength = 3,
  #tuneGrid = expand.grid(C = seq(1, 2, 0.25), sigma = 0.008, Weight = seq(1, 3, 1)),
  method = "svmRadial",
  trControl = myControl,
  na.action = na.omit
)
stopCluster(cl)
model.svm

pred_svm <- predict(model.svm, newdata = test)
count(pred_svm)
```


=============
random forest
=============
```{r}
cl <- makePSOCKcluster(4)#幾核
registerDoParallel(cl)
model.ranger <- train(
  Y1 ~ . - CUS_ID - IF_ISSUE_INSD_E_IND - IF_ISSUE_E_IND - X_F_IND - IF_ISSUE_H_IND -
    IF_ISSUE_INSD_H_IND - X_G_IND - IF_ISSUE_K_IND - FINANCETOOLS_E - L1YR_B_ISSUE_CNT -
    IF_ISSUE_INSD_K_IND - IF_ISSUE_M_IND - IF_ISSUE_O_IND - LAST_B_ISSUE_DT -
    C_IND - IF_ADD_G_IND - IF_ISSUE_INSD_M_IND - IM_IS_A_IND - IF_ISSUE_A_IND -
    IF_ISSUE_B_IND - IF_ISSUE_L_IND - IF_ISSUE_INSD_O_IND - IF_ADD_INSD_G_IND -
    IF_ISSUE_F_IND - IF_ISSUE_INSD_A_IND - IF_ISSUE_INSD_L_IND - FINANCETOOLS_D -
    X_A_IND - CHANNEL_B_POL_CNT - IF_ISSUE_INSD_F_IND - FINANCETOOLS_F - IF_ISSUE_C_IND -
    IF_ISSUE_INSD_B_IND, 
  data = train_over,
  tuneLength = 5,
  method = "ranger",
  trControl = myControl,
  importance = "impurity",
  na.action = na.omit
)
stopCluster(cl)
model.ranger
pred_ranger <- predict(model.ranger, newdata = test)
count(pred_ranger)
#write_pred(test$CUS_ID, pred_ranger)
```


```{r}
imp<-varImp(model.ranger,scale = F)$importance
rowname <- rownames(imp)
imp<-varImp(model.ranger,scale = F)$importance%>%mutate(rowname = rowname)
imp<-arrange(imp,Overall)

```



==============
model ensemble
==============
```{r}
Control <- trainControl(method = "cv",
                        number = 5,
                        savePredictions = "final")
algorithmList <-
  c('glm', 'ranger', 'xgbDART', 'svmRadial')
set.seed(1)
models.ensemble <-
  caretList(Y1 ~ .,
            data = train_new,
            trControl = Control,
            methodList = algorithmList)

pred_ensem <- predict(models.ensemble, newdata = test)
```

```{r}
train_over_data <- train_over[, 2:60] %>% to_categorical()
train_over_label <- train_over[, 61] %>% to_categorical()

library(keras)
k_model <- keras_model_sequential()

k_model %>% 
  layer_dense(units = 100, activation = 'relu', input_shape = 60) %>% 
  layer_dense(units = 2, activation = 'softmax') 

summary(k_model)

k_model %>% compile(optimize = 'adam',
                    loss = 'categorical_crossentropy',
                    metrics = c('accuracy'))
keras_fit <- keras::fit(train_over_data, train_over_label, epoch = 20, batch_size = 200, validation_split = 0.2)
```

```{r}
train_n <- filter(train, Y1 == "N")
train_y <- filter(train, Y1 == "Y")

plot_num(train_n)
```





