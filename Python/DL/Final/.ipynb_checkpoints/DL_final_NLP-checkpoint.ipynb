{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stackprinter\n",
    "import os\n",
    "import pysnooper\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import optimizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.nn import CrossEntropyLoss,BCEWithLogitsLoss\n",
    "from tqdm import tqdm_notebook, trange\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "from sklearn.metrics import precision_recall_curve,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "stackprinter.set_excepthook(style='darkbg2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/tommy84729/python/DL/dl-course-final-competition')\n",
    "train = pd.read_csv('train_data.csv', encoding = 'utf-8')\n",
    "test = pd.read_csv('test_data.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train.loc[:,['title', 'keyword', 'label']]\n",
    "df_test = test.loc[:,['title', 'keyword']]\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "NUM_LABELS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練樣本數： 165992\n"
     ]
    }
   ],
   "source": [
    "empty_title = (df_train['keyword'].isnull())\n",
    "df_train = df_train[~empty_title]\n",
    "df_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"訓練樣本數：\", len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.162604\n",
       "3    0.132874\n",
       "1    0.122319\n",
       "5    0.117247\n",
       "9    0.098607\n",
       "4    0.086293\n",
       "8    0.077666\n",
       "7    0.076468\n",
       "6    0.076142\n",
       "2    0.049780\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.value_counts() / len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "預測樣本數： 59908\n"
     ]
    }
   ],
   "source": [
    "df_test.to_csv(\"test.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"預測樣本數：\", len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wordclassification_Dataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]  # 一般訓練你會需要 dev set\n",
    "        self.mode = mode\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = dict(zip(train['label_name'].unique().tolist(),train['label'].unique().tolist()))\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    #@pysnooper.snoop()\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text_a, text_b = self.df.iloc[idx, :2].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text_a, text_b, label = self.df.iloc[idx, :].values\n",
    "            # 將 label 文字也轉換成索引方便轉換成 tensor\n",
    "            label_id = label\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        # 第二個句子的 BERT tokens\n",
    "        tokens_b = self.tokenizer.tokenize(text_b)\n",
    "        word_pieces += tokens_b + [\"[SEP]\"]\n",
    "        len_b = len(word_pieces) - len_a\n",
    "        \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
    "                                        dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "# 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
    "trainset = wordclassification_Dataset(\"train\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[原始文本]\n",
      "句子 1：包贝尔带娇妻外出就餐被拍，大家把注意力放在了第3张！\n",
      "句子 2：娇妻,娇妻外出就餐,包贝尔\n",
      "分類  ：0\n",
      "\n",
      "--------------------\n",
      "\n",
      "[Dataset 回傳的 tensors]\n",
      "tokens_tensor  ：tensor([ 101, 1259, 6564, 2209, 2372, 2019, 1988, 1912, 1139, 2218, 7623, 6158,\n",
      "        2864, 8024, 1920, 2157, 2828, 3800, 2692, 1213, 3123, 1762,  749, 5018,\n",
      "         124, 2476, 8013,  102, 2019, 1988,  117, 2019, 1988, 1912, 1139, 2218,\n",
      "        7623,  117, 1259, 6564, 2209,  102])\n",
      "\n",
      "segments_tensor：tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "label_tensor   ：0\n",
      "\n",
      "--------------------\n",
      "\n",
      "[還原 tokens_tensors]\n",
      "[CLS]包贝尔带娇妻外出就餐被拍，大家把注意力放在了第3张！[SEP]娇妻,娇妻外出就餐,包贝尔[SEP]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source path:... <ipython-input-11-761edc141ad5>\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 1\n",
      "23:43:21.710082 call        14     def __getitem__(self, idx):\n",
      "23:43:21.710546 line        15         if self.mode == \"test\":\n",
      "23:43:21.710627 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '包贝尔带娇妻外出就餐被拍，大家把注意力放在了第3张！'\n",
      "New var:....... text_b = '娇妻,娇妻外出就餐,包贝尔'\n",
      "New var:....... label = 0\n",
      "23:43:21.711821 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:43:21.712017 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:43:21.712408 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:43:21.713157 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['包', '贝', '尔', '带', '娇', '妻', '外', '出', '就', '餐...'注', '意', '力', '放', '在', '了', '第', '3', '张', '！']\n",
      "23:43:21.714362 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '包', '贝', '尔', '带', '娇', '妻', '外', '出'... '力', '放', '在', '了', '第', '3', '张', '！', '[SEP]']\n",
      "23:43:21.714618 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "23:43:21.714822 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['娇', '妻', ',', '娇', '妻', '外', '出', '就', '餐', ',', '包', '贝', '尔']\n",
      "23:43:21.715378 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '包', '贝', '尔', '带', '娇', '妻', '外', '出'... '外', '出', '就', '餐', ',', '包', '贝', '尔', '[SEP]']\n",
      "23:43:21.715814 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "23:43:21.716134 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1259, 6564, 2209, 2372, 2019, 1988, 1912, ...12, 1139, 2218, 7623, 117, 1259, 6564, 2209, 102]\n",
      "23:43:21.716602 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1259, 6564, 2209, 2372, 2019, 1988...2218,        7623,  117, 1259, 6564, 2209,  102])\n",
      "23:43:21.717601 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:43:21.718481 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:43:21.719360 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:43:21.720825 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1259, 6564, 2209, 2372, 2019, 198... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013372\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 1\n",
    "\n",
    "# 將原始文本拿出做比較\n",
    "text_a, text_b, label = trainset.df.iloc[sample_idx].values\n",
    "\n",
    "# 利用剛剛建立的 Dataset 取出轉換後的 id tensors\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "# 將 tokens_tensor 還原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \"\".join(tokens)\n",
    "print(f\"\"\"[原始文本]\n",
    "句子 1：{text_a}\n",
    "句子 2：{text_b}\n",
    "分類  ：{label}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[Dataset 回傳的 tensors]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "\n",
    "segments_tensor：{segments_tensor}\n",
    "\n",
    "label_tensor   ：{label_tensor}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[還原 tokens_tensors]\n",
    "{combined_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "# 初始化一個每次回傳 64 個訓練樣本的 DataLoader\n",
    "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵\n",
    "BATCH_SIZE = 64\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 0\n",
      "23:48:58.840948 call        14     def __getitem__(self, idx):\n",
      "23:48:58.841050 line        15         if self.mode == \"test\":\n",
      "23:48:58.841091 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '古力娜扎再次成为焦点，这一身招摇大方，掳获了网友们的心'\n",
      "New var:....... text_b = '古力娜扎,粉丝'\n",
      "New var:....... label = 0\n",
      "23:48:58.841702 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.841858 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.842044 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.842519 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['古', '力', '娜', '扎', '再', '次', '成', '为', '焦', '点...'方', '，', '掳', '获', '了', '网', '友', '们', '的', '心']\n",
      "23:48:58.843350 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '古', '力', '娜', '扎', '再', '次', '成', '为'... '掳', '获', '了', '网', '友', '们', '的', '心', '[SEP]']\n",
      "23:48:58.843859 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "23:48:58.844450 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['古', '力', '娜', '扎', ',', '粉', '丝']\n",
      "23:48:58.844798 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '古', '力', '娜', '扎', '再', '次', '成', '为'...EP]', '古', '力', '娜', '扎', ',', '粉', '丝', '[SEP]']\n",
      "23:48:58.845039 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 8\n",
      "23:48:58.845205 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1367, 1213, 2025, 2799, 1086, 3613, 2768, ...102, 1367, 1213, 2025, 2799, 117, 5106, 692, 102]\n",
      "23:48:58.845372 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1367, 1213, 2025, 2799, 1086, 3613...1213, 2025, 2799,  117, 5106,  692,         102])\n",
      "23:48:58.845811 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.846409 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0,        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:58.847609 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.848896 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1367, 1213, 2025, 2799, 1086, 361... 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009931\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 1\n",
      "23:48:58.850911 call        14     def __getitem__(self, idx):\n",
      "23:48:58.850990 line        15         if self.mode == \"test\":\n",
      "23:48:58.851029 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '包贝尔带娇妻外出就餐被拍，大家把注意力放在了第3张！'\n",
      "New var:....... text_b = '娇妻,娇妻外出就餐,包贝尔'\n",
      "New var:....... label = 0\n",
      "23:48:58.851627 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.851751 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.851970 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.852343 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['包', '贝', '尔', '带', '娇', '妻', '外', '出', '就', '餐...'注', '意', '力', '放', '在', '了', '第', '3', '张', '！']\n",
      "23:48:58.853127 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '包', '贝', '尔', '带', '娇', '妻', '外', '出'... '力', '放', '在', '了', '第', '3', '张', '！', '[SEP]']\n",
      "23:48:58.853408 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "23:48:58.853601 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['娇', '妻', ',', '娇', '妻', '外', '出', '就', '餐', ',', '包', '贝', '尔']\n",
      "23:48:58.854039 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '包', '贝', '尔', '带', '娇', '妻', '外', '出'... '外', '出', '就', '餐', ',', '包', '贝', '尔', '[SEP]']\n",
      "23:48:58.854244 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "23:48:58.854508 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1259, 6564, 2209, 2372, 2019, 1988, 1912, ...12, 1139, 2218, 7623, 117, 1259, 6564, 2209, 102]\n",
      "23:48:58.854722 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1259, 6564, 2209, 2372, 2019, 1988...2218,        7623,  117, 1259, 6564, 2209,  102])\n",
      "23:48:58.855004 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.855561 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:58.856119 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.857157 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1259, 6564, 2209, 2372, 2019, 198... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008730\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 2\n",
      "23:48:58.859703 call        14     def __getitem__(self, idx):\n",
      "23:48:58.859800 line        15         if self.mode == \"test\":\n",
      "23:48:58.859841 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '娱乐圈娶了豪门的5位男星，事业开挂，最后一位想离婚门都没有'\n",
      "New var:....... text_b = '豪门,迟重瑞,周立波,吕良伟,石贞善'\n",
      "New var:....... label = 0\n",
      "23:48:58.860585 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.860750 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.861044 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.861527 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['娱', '乐', '圈', '娶', '了', '豪', '门', '的', '5', '位...'后', '一', '位', '想', '离', '婚', '门', '都', '没', '有']\n",
      "23:48:58.862718 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '娶', '了', '豪', '门', '的'... '位', '想', '离', '婚', '门', '都', '没', '有', '[SEP]']\n",
      "23:48:58.863344 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "23:48:58.863679 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['豪', '门', ',', '迟', '重', '瑞', ',', '周', '立', '波', ',', '吕', '良', '伟', ',', '石', '贞', '善']\n",
      "23:48:58.864519 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '娶', '了', '豪', '门', '的'... ',', '吕', '良', '伟', ',', '石', '贞', '善', '[SEP]']\n",
      "23:48:58.864750 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "23:48:58.864951 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2031, 727, 1750, 2034, 749, 6498, 7305, 46...117, 1406, 5679, 836, 117, 4767, 6565, 1587, 102]\n",
      "23:48:58.865171 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2031,  727, 1750, 2034,  749, 6498...5679,  836,  117, 4767, 6565,        1587,  102])\n",
      "23:48:58.865453 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.866303 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "23:48:58.866926 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.867935 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2031,  727, 1750, 2034,  749, 649...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010948\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 3\n",
      "23:48:58.870687 call        14     def __getitem__(self, idx):\n",
      "23:48:58.870776 line        15         if self.mode == \"test\":\n",
      "23:48:58.870815 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '陈学冬参加节目被爆童年照，果然胖子都是潜力股啊！'\n",
      "New var:....... text_b = '薛之谦,陈学冬,谭维维,跨界歌王'\n",
      "New var:....... label = 0\n",
      "23:48:58.871431 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.871746 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.871909 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.872175 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['陈', '学', '冬', '参', '加', '节', '目', '被', '爆', '童...'然', '胖', '子', '都', '是', '潜', '力', '股', '啊', '！']\n",
      "23:48:58.872817 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '学', '冬', '参', '加', '节', '目', '被'... '子', '都', '是', '潜', '力', '股', '啊', '！', '[SEP]']\n",
      "23:48:58.873108 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "23:48:58.873577 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['薛', '之', '谦', ',', '陈', '学', '冬', ',', '谭', '维', '维', ',', '跨', '界', '歌', '王']\n",
      "23:48:58.874391 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '学', '冬', '参', '加', '节', '目', '被'... '谭', '维', '维', ',', '跨', '界', '歌', '王', '[SEP]']\n",
      "23:48:58.874816 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 17\n",
      "23:48:58.875050 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7357, 2110, 1100, 1346, 1217, 5688, 4680, ...78, 5335, 5335, 117, 6659, 4518, 3625, 4374, 102]\n",
      "23:48:58.875271 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7357, 2110, 1100, 1346, 1217, 5688...       5335,  117, 6659, 4518, 3625, 4374,  102])\n",
      "23:48:58.875554 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.876281 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:58.876859 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.878165 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7357, 2110, 1100, 1346, 1217, 568... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011414\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 4\n",
      "23:48:58.882159 call        14     def __getitem__(self, idx):\n",
      "23:48:58.882279 line        15         if self.mode == \"test\":\n",
      "23:48:58.882334 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '福斯进军华语迷你剧开山之作定檔，“东方华尔街”吴镇宇张孝全'\n",
      "New var:....... text_b = '金融圈,吴镇宇,王牌逗王牌,桃姐,张孝全,东方华尔街,拆弹专家,女朋友·男朋友,刘德华'\n",
      "New var:....... label = 0\n",
      "23:48:58.883232 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.883447 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.883597 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.884061 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['福', '斯', '进', '军', '华', '语', '迷', '你', '剧', '开... '尔', '街', '[UNK]', '吴', '镇', '宇', '张', '孝', '全']\n",
      "23:48:58.885114 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '福', '斯', '进', '军', '华', '语', '迷', '你'..., '[UNK]', '吴', '镇', '宇', '张', '孝', '全', '[SEP]']\n",
      "23:48:58.885921 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "23:48:58.886444 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['金', '融', '圈', ',', '吴', '镇', '宇', ',', '王', '牌...'朋', '友', '·', '男', '朋', '友', ',', '刘', '德', '华']\n",
      "23:48:58.888204 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '福', '斯', '进', '军', '华', '语', '迷', '你'... '·', '男', '朋', '友', ',', '刘', '德', '华', '[SEP]']\n",
      "23:48:58.889042 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 44\n",
      "23:48:58.889700 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4886, 3172, 6822, 1092, 1290, 6427, 6837, ...85, 4511, 3301, 1351, 117, 1155, 2548, 1290, 102]\n",
      "23:48:58.889993 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4886, 3172, 6822, 1092, 1290, 6427...3301, 1351,  117, 1155,        2548, 1290,  102])\n",
      "23:48:58.890298 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.891219 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "23:48:58.892115 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.893478 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4886, 3172, 6822, 1092, 1290, 642...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.014767\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 5\n",
      "23:48:58.896969 call        14     def __getitem__(self, idx):\n",
      "23:48:58.897068 line        15         if self.mode == \"test\":\n",
      "23:48:58.897107 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '马伊琍就是马伊琍，蕾丝也能穿出女王范，文章还是乖乖做小男人吧'\n",
      "New var:....... text_b = '乖乖做小男人,小男人,马伊琍,蕾丝,穿出女王范'\n",
      "New var:....... label = 0\n",
      "23:48:58.897948 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.898211 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.898420 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.898706 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['马', '伊', '琍', '就', '是', '马', '伊', '琍', '，', '蕾...'章', '还', '是', '乖', '乖', '做', '小', '男', '人', '吧']\n",
      "23:48:58.900059 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '马', '伊', '琍', '就', '是', '马', '伊', '琍'... '是', '乖', '乖', '做', '小', '男', '人', '吧', '[SEP]']\n",
      "23:48:58.900484 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "23:48:58.900713 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['乖', '乖', '做', '小', '男', '人', ',', '小', '男', '人...'琍', ',', '蕾', '丝', ',', '穿', '出', '女', '王', '范']\n",
      "23:48:58.901272 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '马', '伊', '琍', '就', '是', '马', '伊', '琍'... '蕾', '丝', ',', '穿', '出', '女', '王', '范', '[SEP]']\n",
      "23:48:58.901645 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "23:48:58.901878 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7716, 823, 4419, 2218, 3221, 7716, 823, 44...945, 692, 117, 4959, 1139, 1957, 4374, 5745, 102]\n",
      "23:48:58.902169 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7716,  823, 4419, 2218, 3221, 7716...  692,  117, 4959, 1139, 1957, 4374, 5745,  102])\n",
      "23:48:58.902841 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.903728 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:58.904474 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.905690 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7716,  823, 4419, 2218, 3221, 771...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010925\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 6\n",
      "23:48:58.907924 call        14     def __getitem__(self, idx):\n",
      "23:48:58.907995 line        15         if self.mode == \"test\":\n",
      "23:48:58.908032 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '这就是街舞 收官：石头秀肌肉，抖动胸肌！易烊千玺坐地，尖叫！'\n",
      "New var:....... text_b = '街舞,胸肌,烊千'\n",
      "New var:....... label = 0\n",
      "23:48:58.908615 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.908707 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.908781 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.908962 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['这', '就', '是', '街', '舞', '收', '官', '：', '石', '头...'易', '烊', '千', '玺', '坐', '地', '，', '尖', '叫', '！']\n",
      "23:48:58.909678 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '就', '是', '街', '舞', '收', '官', '：'... '千', '玺', '坐', '地', '，', '尖', '叫', '！', '[SEP]']\n",
      "23:48:58.909901 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "23:48:58.910148 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['街', '舞', ',', '胸', '肌', ',', '烊', '千']\n",
      "23:48:58.910497 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '就', '是', '街', '舞', '收', '官', '：'... '街', '舞', ',', '胸', '肌', ',', '烊', '千', '[SEP]']\n",
      "23:48:58.910707 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 9\n",
      "23:48:58.910912 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6821, 2218, 3221, 6125, 5659, 3119, 2135, ...125, 5659, 117, 5541, 5491, 117, 4165, 1283, 102]\n",
      "23:48:58.911121 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6821, 2218, 3221, 6125, 5659, 3119... 117, 5541, 5491,         117, 4165, 1283,  102])\n",
      "23:48:58.911341 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.912419 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:58.913207 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.914181 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6821, 2218, 3221, 6125, 5659, 311... 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008263\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 7\n",
      "23:48:58.916222 call        14     def __getitem__(self, idx):\n",
      "23:48:58.916297 line        15         if self.mode == \"test\":\n",
      "23:48:58.916335 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张歆艺现身上海出席红毯活动 网友：好V好比刚路过的飞机场！'\n",
      "New var:....... text_b = '红毯活动,红毯,张歆艺,东方IC,东方ic'\n",
      "New var:....... label = 0\n",
      "23:48:58.916921 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.917016 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.917179 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.917486 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '歆', '艺', '现', '身', '上', '海', '出', '席', '红...'好', '比', '刚', '路', '过', '的', '飞', '机', '场', '！']\n",
      "23:48:58.918235 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '歆', '艺', '现', '身', '上', '海', '出'... '刚', '路', '过', '的', '飞', '机', '场', '！', '[SEP]']\n",
      "23:48:58.918464 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "23:48:58.918658 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['红', '毯', '活', '动', ',', '红', '毯', ',', '张', '歆', '艺', ',', '东', '方', 'ic', ',', '东', '方', 'ic']\n",
      "23:48:58.919212 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '歆', '艺', '现', '身', '上', '海', '出'...,', '东', '方', 'ic', ',', '东', '方', 'ic', '[SEP]']\n",
      "23:48:58.919413 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "23:48:58.919745 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 3622, 5686, 4385, 6716, 677, 3862, 1... 117, 691, 3175, 8577, 117, 691, 3175, 8577, 102]\n",
      "23:48:58.919949 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476, 3622, 5686, 4385, 6716,  677...3175, 8577,  117,  691, 3175,        8577,  102])\n",
      "23:48:58.920154 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.920769 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "23:48:58.921375 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.922501 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2476, 3622, 5686, 4385, 6716,  67...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008262\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 8\n",
      "23:48:58.924512 call        14     def __getitem__(self, idx):\n",
      "23:48:58.924647 line        15         if self.mode == \"test\":\n",
      "23:48:58.924688 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '李小璐数月首曝光！女儿做爱心餐庆祝母亲节，贾乃亮却不在现场？'\n",
      "New var:....... text_b = '甜馨,母亲节,李小璐,粉丝,贾乃亮'\n",
      "New var:....... label = 0\n",
      "23:48:58.925228 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.925320 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.925391 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.925570 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['李', '小', '璐', '数', '月', '首', '曝', '光', '！', '女...'，', '贾', '乃', '亮', '却', '不', '在', '现', '场', '？']\n",
      "23:48:58.926291 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '小', '璐', '数', '月', '首', '曝', '光'... '乃', '亮', '却', '不', '在', '现', '场', '？', '[SEP]']\n",
      "23:48:58.926524 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "23:48:58.926706 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['甜', '馨', ',', '母', '亲', '节', ',', '李', '小', '璐', ',', '粉', '丝', ',', '贾', '乃', '亮']\n",
      "23:48:58.927266 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '小', '璐', '数', '月', '首', '曝', '光'... '璐', ',', '粉', '丝', ',', '贾', '乃', '亮', '[SEP]']\n",
      "23:48:58.927500 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "23:48:58.927723 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3330, 2207, 4466, 3144, 3299, 7674, 3284, ..., 4466, 117, 5106, 692, 117, 6593, 718, 778, 102]\n",
      "23:48:58.927940 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3330, 2207, 4466, 3144, 3299, 7674...5106,  692,  117, 6593,  718,         778,  102])\n",
      "23:48:58.928153 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.928759 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "23:48:58.929555 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.931144 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3330, 2207, 4466, 3144, 3299, 767...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008998\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 9\n",
      "23:48:58.933548 call        14     def __getitem__(self, idx):\n",
      "23:48:58.933624 line        15         if self.mode == \"test\":\n",
      "23:48:58.933663 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '你觉得独孤曼陀丑？那是没看过她演的虞姬！'\n",
      "New var:....... text_b = '安以轩,宇文护,虞姬,李依晓,陇西郡,独孤天下,楚汉传奇,新洛神'\n",
      "New var:....... label = 0\n",
      "23:48:58.934241 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.934335 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.934534 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.934810 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['你', '觉', '得', '独', '孤', '曼', '陀', '丑', '？', '那', '是', '没', '看', '过', '她', '演', '的', '虞', '姬', '！']\n",
      "23:48:58.935399 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '你', '觉', '得', '独', '孤', '曼', '陀', '丑'... '看', '过', '她', '演', '的', '虞', '姬', '！', '[SEP]']\n",
      "23:48:58.935594 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 22\n",
      "23:48:58.935777 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['安', '以', '轩', ',', '宇', '文', '护', ',', '虞', '姬...'下', ',', '楚', '汉', '传', '奇', ',', '新', '洛', '神']\n",
      "23:48:58.936532 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '你', '觉', '得', '独', '孤', '曼', '陀', '丑'... '楚', '汉', '传', '奇', ',', '新', '洛', '神', '[SEP]']\n",
      "23:48:58.936803 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 33\n",
      "23:48:58.937071 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 872, 6230, 2533, 4324, 2109, 3294, 7351, 6...504, 3727, 837, 1936, 117, 3173, 3821, 4868, 102]\n",
      "23:48:58.937287 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  872, 6230, 2533, 4324, 2109, 3294...        837, 1936,  117, 3173, 3821, 4868,  102])\n",
      "23:48:58.937502 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.938185 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:58.939107 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.940237 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  872, 6230, 2533, 4324, 2109, 329...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008934\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 10\n",
      "23:48:58.942512 call        14     def __getitem__(self, idx):\n",
      "23:48:58.942583 line        15         if self.mode == \"test\":\n",
      "23:48:58.942620 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = 'Tik杰西达邦和Chakrit联手演绎《傲世双雄》，这部泰剧不狗血哦'\n",
      "New var:....... text_b = 'Falconer,Tik,杰西达邦,雷人,傲世双雄,Mat,Chakrit'\n",
      "New var:....... label = 0\n",
      "23:48:58.943190 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.943283 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.943356 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.943576 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['ti', '##k', '杰', '西', '达', '邦', '和', 'ch', '##...'》', '，', '这', '部', '泰', '剧', '不', '狗', '血', '哦']\n",
      "23:48:58.944319 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'ti', '##k', '杰', '西', '达', '邦', '和', ... '这', '部', '泰', '剧', '不', '狗', '血', '哦', '[SEP]']\n",
      "23:48:58.944511 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "23:48:58.944762 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['fa', '##lc', '##one', '##r', ',', 'ti', '##k',...', 'ma', '##t', ',', 'ch', '##ak', '##ri', '##t']\n",
      "23:48:58.945435 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'ti', '##k', '杰', '西', '达', '邦', '和', ...'##t', ',', 'ch', '##ak', '##ri', '##t', '[SEP]']\n",
      "23:48:58.945674 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 29\n",
      "23:48:58.945887 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 9654, 8197, 3345, 6205, 6809, 6930, 1469, ...17, 9622, 8165, 117, 9537, 9896, 8641, 8165, 102]\n",
      "23:48:58.946123 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  9654,  8197,  3345,  6205,  6809...  117,  9537,  9896,  8641,  8165,          102])\n",
      "23:48:58.946348 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.947270 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:58.947975 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.949133 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  9654,  8197,  3345,  6205,  680... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009017\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 11\n",
      "23:48:58.951560 call        14     def __getitem__(self, idx):\n",
      "23:48:58.951630 line        15         if self.mode == \"test\":\n",
      "23:48:58.951666 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '小男孩模仿薛之谦唱歌，薛之谦一脸错愕，台下尖叫！声音简直一样'\n",
      "New var:....... text_b = '薛之谦'\n",
      "New var:....... label = 0\n",
      "23:48:58.952244 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.952335 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.952407 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.952689 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['小', '男', '孩', '模', '仿', '薛', '之', '谦', '唱', '歌...'下', '尖', '叫', '！', '声', '音', '简', '直', '一', '样']\n",
      "23:48:58.953416 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '小', '男', '孩', '模', '仿', '薛', '之', '谦'... '叫', '！', '声', '音', '简', '直', '一', '样', '[SEP]']\n",
      "23:48:58.953602 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "23:48:58.953785 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['薛', '之', '谦']\n",
      "23:48:58.954054 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '小', '男', '孩', '模', '仿', '薛', '之', '谦'..., '直', '一', '样', '[SEP]', '薛', '之', '谦', '[SEP]']\n",
      "23:48:58.954243 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 4\n",
      "23:48:58.954427 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2207, 4511, 2111, 3563, 820, 5955, 722, 64...5042, 4684, 671, 3416, 102, 5955, 722, 6472, 102]\n",
      "23:48:58.954663 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2207, 4511, 2111, 3563,  820, 5955... 4684,  671, 3416,  102, 5955,  722, 6472,  102])\n",
      "23:48:58.954929 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.955492 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0,        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1])\n",
      "23:48:58.956105 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.957086 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2207, 4511, 2111, 3563,  820, 595... 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007253\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 12\n",
      "23:48:58.958841 call        14     def __getitem__(self, idx):\n",
      "23:48:58.958910 line        15         if self.mode == \"test\":\n",
      "23:48:58.958948 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '上官燕被秋若枫毁了清白，可是她觉得伤害她最深的是她父亲上官云'\n",
      "New var:....... text_b = '上官云,上官燕'\n",
      "New var:....... label = 0\n",
      "23:48:58.959448 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.959611 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.959686 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.959869 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['上', '官', '燕', '被', '秋', '若', '枫', '毁', '了', '清...'最', '深', '的', '是', '她', '父', '亲', '上', '官', '云']\n",
      "23:48:58.960583 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '上', '官', '燕', '被', '秋', '若', '枫', '毁'... '的', '是', '她', '父', '亲', '上', '官', '云', '[SEP]']\n",
      "23:48:58.960770 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "23:48:58.960990 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['上', '官', '云', ',', '上', '官', '燕']\n",
      "23:48:58.961320 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '上', '官', '燕', '被', '秋', '若', '枫', '毁'...EP]', '上', '官', '云', ',', '上', '官', '燕', '[SEP]']\n",
      "23:48:58.961512 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 8\n",
      "23:48:58.961764 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 677, 2135, 4242, 6158, 4904, 5735, 3367, 3..., 102, 677, 2135, 756, 117, 677, 2135, 4242, 102]\n",
      "23:48:58.962018 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  677, 2135, 4242, 6158, 4904, 5735...2135,  756,  117,         677, 2135, 4242,  102])\n",
      "23:48:58.962267 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.962857 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:58.963564 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.964531 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  677, 2135, 4242, 6158, 4904, 573... 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007474\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 13\n",
      "23:48:58.966345 call        14     def __getitem__(self, idx):\n",
      "23:48:58.966414 line        15         if self.mode == \"test\":\n",
      "23:48:58.966450 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '会唠嗑儿的直男赢了！说你娶到刘亦菲我也信……'\n",
      "New var:....... text_b = '范冰冰,刘亦菲'\n",
      "New var:....... label = 0\n",
      "23:48:58.966982 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.967143 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.967218 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.967392 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['会', '唠', '嗑', '儿', '的', '直', '男', '赢', '了', '！..., '刘', '亦', '菲', '我', '也', '信', '[UNK]', '[UNK]']\n",
      "23:48:58.968019 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '会', '唠', '嗑', '儿', '的', '直', '男', '赢'...', '菲', '我', '也', '信', '[UNK]', '[UNK]', '[SEP]']\n",
      "23:48:58.968206 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "23:48:58.968387 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['范', '冰', '冰', ',', '刘', '亦', '菲']\n",
      "23:48:58.968713 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '会', '唠', '嗑', '儿', '的', '直', '男', '赢'...EP]', '范', '冰', '冰', ',', '刘', '亦', '菲', '[SEP]']\n",
      "23:48:58.968904 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 8\n",
      "23:48:58.969089 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 833, 1541, 1622, 1036, 4638, 4684, 4511, 6...102, 5745, 1102, 1102, 117, 1155, 771, 5838, 102]\n",
      "23:48:58.969349 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  833, 1541, 1622, 1036, 4638, 4684... 5745, 1102, 1102,  117, 1155,  771, 5838,  102])\n",
      "23:48:58.969552 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.970081 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0, 0, 0,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:58.970571 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.971403 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  833, 1541, 1622, 1036, 4638, 468...0, 0,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006643\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 14\n",
      "23:48:58.973015 call        14     def __getitem__(self, idx):\n",
      "23:48:58.973083 line        15         if self.mode == \"test\":\n",
      "23:48:58.973120 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '漫威新电影复仇者联盟3无限战争豪夺票房王座'\n",
      "New var:....... text_b = '复仇者联盟3 ：无限之战,星际异攻队,漫威电影宇宙,复仇者联盟2 ：奥创纪元,IMAX,漫威,电影,漫威影业'\n",
      "New var:....... label = 0\n",
      "23:48:58.973602 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.973693 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.973763 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.973923 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['漫', '威', '新', '电', '影', '复', '仇', '者', '联', '盟...'无', '限', '战', '争', '豪', '夺', '票', '房', '王', '座']\n",
      "23:48:58.974537 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '漫', '威', '新', '电', '影', '复', '仇', '者'... '战', '争', '豪', '夺', '票', '房', '王', '座', '[SEP]']\n",
      "23:48:58.974786 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "23:48:58.974969 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['复', '仇', '者', '联', '盟', '3', '：', '无', '限', '之...'漫', '威', ',', '电', '影', ',', '漫', '威', '影', '业']\n",
      "23:48:58.976033 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '漫', '威', '新', '电', '影', '复', '仇', '者'... ',', '电', '影', ',', '漫', '威', '影', '业', '[SEP]']\n",
      "23:48:58.976282 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 50\n",
      "23:48:58.976499 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4035, 2014, 3173, 4510, 2512, 1908, 790, 5...117, 4510, 2512, 117, 4035, 2014, 2512, 689, 102]\n",
      "23:48:58.976726 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  4035,  2014,  3173,  4510,  2512...  117,  4035,  2014,         2512,   689,   102])\n",
      "23:48:58.977017 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.977773 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "23:48:58.978527 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.979947 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  4035,  2014,  3173,  4510,  251...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009656\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 15\n",
      "23:48:58.982700 call        14     def __getitem__(self, idx):\n",
      "23:48:58.982769 line        15         if self.mode == \"test\":\n",
      "23:48:58.982805 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《上海女子图鉴》女主上海十年男友比戚薇少，每个大有来头超有钱'\n",
      "New var:....... text_b = '女强人,刘孜,女主角,北京女子图鉴,罗海燕,戚薇,上海女子图鉴'\n",
      "New var:....... label = 0\n",
      "23:48:58.983282 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.983373 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.983445 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.983646 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '上', '海', '女', '子', '图', '鉴', '》', '女', '主...'，', '每', '个', '大', '有', '来', '头', '超', '有', '钱']\n",
      "23:48:58.984366 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '上', '海', '女', '子', '图', '鉴', '》'... '个', '大', '有', '来', '头', '超', '有', '钱', '[SEP]']\n",
      "23:48:58.984560 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "23:48:58.984741 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['女', '强', '人', ',', '刘', '孜', ',', '女', '主', '角...',', '戚', '薇', ',', '上', '海', '女', '子', '图', '鉴']\n",
      "23:48:58.985536 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '上', '海', '女', '子', '图', '鉴', '》'... '薇', ',', '上', '海', '女', '子', '图', '鉴', '[SEP]']\n",
      "23:48:58.985781 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 32\n",
      "23:48:58.985973 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 677, 3862, 1957, 2094, 1745, 7063, 51...948, 117, 677, 3862, 1957, 2094, 1745, 7063, 102]\n",
      "23:48:58.986182 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517,  677, 3862, 1957, 2094, 1745... 677, 3862, 1957,        2094, 1745, 7063,  102])\n",
      "23:48:58.986452 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.987232 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:58.987924 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.989167 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517,  677, 3862, 1957, 2094, 174... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008970\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 16\n",
      "23:48:58.991698 call        14     def __getitem__(self, idx):\n",
      "23:48:58.991766 line        15         if self.mode == \"test\":\n",
      "23:48:58.991803 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '跟王菲学的“新潮流”? 谢霆锋穿着“鸳鸯袜”喊话网友自寻亮点！'\n",
      "New var:....... text_b = '新潮流,王菲,张柏芝,谢霆锋,牛仔裤'\n",
      "New var:....... label = 0\n",
      "23:48:58.992295 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:58.992386 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:58.992459 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:58.992707 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['跟', '王', '菲', '学', '的', '[UNK]', '新', '潮', '流'...K]', '喊', '话', '网', '友', '自', '寻', '亮', '点', '！']\n",
      "23:48:58.993428 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '跟', '王', '菲', '学', '的', '[UNK]', '新',... '话', '网', '友', '自', '寻', '亮', '点', '！', '[SEP]']\n",
      "23:48:58.993617 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "23:48:58.993839 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['新', '潮', '流', ',', '王', '菲', ',', '张', '柏', '芝', ',', '谢', '霆', '锋', ',', '牛', '仔', '裤']\n",
      "23:48:58.994358 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '跟', '王', '菲', '学', '的', '[UNK]', '新',... ',', '谢', '霆', '锋', ',', '牛', '仔', '裤', '[SEP]']\n",
      "23:48:58.994551 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "23:48:58.994739 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6656, 4374, 5838, 2110, 4638, 100, 3173, 4...117, 6468, 7447, 7226, 117, 4281, 798, 6175, 102]\n",
      "23:48:58.995009 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6656, 4374, 5838, 2110, 4638,  100...7447, 7226,  117, 4281,         798, 6175,  102])\n",
      "23:48:58.995220 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:58.995826 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "23:48:58.996521 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:58.997717 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6656, 4374, 5838, 2110, 4638,  10...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009118\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 17\n",
      "23:48:59.000863 call        14     def __getitem__(self, idx):\n",
      "23:48:59.000980 line        15         if self.mode == \"test\":\n",
      "23:48:59.001019 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '4岁成名，比张一山杨紫还红，被父母败光千万身家，如今长成这样'\n",
      "New var:....... text_b = '演艺圈,笑林小子,朱延平,新乌龙院,哪吒大战美猴王,释小龙,郝劭文,张一山'\n",
      "New var:....... label = 0\n",
      "23:48:59.001613 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.001711 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.001898 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.002155 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['4', '岁', '成', '名', '，', '比', '张', '一', '山', '杨...'万', '身', '家', '，', '如', '今', '长', '成', '这', '样']\n",
      "23:48:59.002940 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '4', '岁', '成', '名', '，', '比', '张', '一'... '家', '，', '如', '今', '长', '成', '这', '样', '[SEP]']\n",
      "23:48:59.003226 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "23:48:59.003444 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['演', '艺', '圈', ',', '笑', '林', '小', '子', ',', '朱...'小', '龙', ',', '郝', '劭', '文', ',', '张', '一', '山']\n",
      "23:48:59.004296 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '4', '岁', '成', '名', '，', '比', '张', '一'... ',', '郝', '劭', '文', ',', '张', '一', '山', '[SEP]']\n",
      "23:48:59.004550 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 38\n",
      "23:48:59.004746 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 125, 2259, 2768, 1399, 8024, 3683, 2476, 6...117, 6950, 1224, 3152, 117, 2476, 671, 2255, 102]\n",
      "23:48:59.004963 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  125, 2259, 2768, 1399, 8024, 3683... 6950, 1224, 3152,  117, 2476,  671, 2255,  102])\n",
      "23:48:59.005182 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.006003 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.006705 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.008170 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  125, 2259, 2768, 1399, 8024, 368... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009987\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 18\n",
      "23:48:59.010879 call        14     def __getitem__(self, idx):\n",
      "23:48:59.010948 line        15         if self.mode == \"test\":\n",
      "23:48:59.010985 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张杰外网发文回应《101》风波：解释无果那就沉默'\n",
      "New var:....... text_b = '张杰,粉丝,创造101'\n",
      "New var:....... label = 0\n",
      "23:48:59.011507 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.011599 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.011670 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.011886 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '杰', '外', '网', '发', '文', '回', '应', '《', '1...'波', '：', '解', '释', '无', '果', '那', '就', '沉', '默']\n",
      "23:48:59.012528 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '杰', '外', '网', '发', '文', '回', '应'... '解', '释', '无', '果', '那', '就', '沉', '默', '[SEP]']\n",
      "23:48:59.012735 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "23:48:59.012923 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['张', '杰', ',', '粉', '丝', ',', '创', '造', '101']\n",
      "23:48:59.013401 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '杰', '外', '网', '发', '文', '回', '应'...杰', ',', '粉', '丝', ',', '创', '造', '101', '[SEP]']\n",
      "23:48:59.013619 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 10\n",
      "23:48:59.013815 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 3345, 1912, 5381, 1355, 3152, 1726, ...3345, 117, 5106, 692, 117, 1158, 6863, 8359, 102]\n",
      "23:48:59.014057 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476, 3345, 1912, 5381, 1355, 3152...  117, 5106,  692,  117, 1158, 6863, 8359,  102])\n",
      "23:48:59.014264 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.014769 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.015334 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.016134 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2476, 3345, 1912, 5381, 1355, 315...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007020\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 19\n",
      "23:48:59.017928 call        14     def __getitem__(self, idx):\n",
      "23:48:59.017997 line        15         if self.mode == \"test\":\n",
      "23:48:59.018034 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '娱乐圈又一气质女神出柜了？她还是柳岩韩雪的闺蜜！'\n",
      "New var:....... text_b = '吴卓林,祝你幸福,毛林林,兰陵王,柳岩,韩雪'\n",
      "New var:....... label = 0\n",
      "23:48:59.018520 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.018612 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.018683 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.018899 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['娱', '乐', '圈', '又', '一', '气', '质', '女', '神', '出...'还', '是', '柳', '岩', '韩', '雪', '的', '闺', '蜜', '！']\n",
      "23:48:59.019509 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '又', '一', '气', '质', '女'... '柳', '岩', '韩', '雪', '的', '闺', '蜜', '！', '[SEP]']\n",
      "23:48:59.019695 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "23:48:59.019875 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['吴', '卓', '林', ',', '祝', '你', '幸', '福', ',', '毛...',', '兰', '陵', '王', ',', '柳', '岩', ',', '韩', '雪']\n",
      "23:48:59.020517 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '又', '一', '气', '质', '女'... '陵', '王', ',', '柳', '岩', ',', '韩', '雪', '[SEP]']\n",
      "23:48:59.020712 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "23:48:59.020953 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2031, 727, 1750, 1348, 671, 3698, 6574, 19...377, 4374, 117, 3394, 2272, 117, 7506, 7434, 102]\n",
      "23:48:59.021159 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2031,  727, 1750, 1348,  671, 3698... 117, 3394, 2272,  117, 7506, 7434,         102])\n",
      "23:48:59.021368 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.021968 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "23:48:59.022628 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.023670 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2031,  727, 1750, 1348,  671, 369...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007800\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 20\n",
      "23:48:59.025756 call        14     def __getitem__(self, idx):\n",
      "23:48:59.025824 line        15         if self.mode == \"test\":\n",
      "23:48:59.025861 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = 'Jasper胖得下巴和脖子连成线，这样的小小春反而更加呆萌可爱'\n",
      "New var:....... text_b = '连成线,下巴'\n",
      "New var:....... label = 0\n",
      "23:48:59.026334 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.026424 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.026495 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.026670 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['j', '##as', '##per', '胖', '得', '下', '巴', '和', ...'小', '春', '反', '而', '更', '加', '呆', '萌', '可', '爱']\n",
      "23:48:59.027368 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'j', '##as', '##per', '胖', '得', '下', '... '反', '而', '更', '加', '呆', '萌', '可', '爱', '[SEP]']\n",
      "23:48:59.027593 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "23:48:59.027774 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['连', '成', '线', ',', '下', '巴']\n",
      "23:48:59.028149 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'j', '##as', '##per', '胖', '得', '下', '..., '[SEP]', '连', '成', '线', ',', '下', '巴', '[SEP]']\n",
      "23:48:59.028340 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 7\n",
      "23:48:59.028525 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 152, 8576, 9063, 5523, 2533, 678, 2349, 14...4263, 102, 6825, 2768, 5296, 117, 678, 2349, 102]\n",
      "23:48:59.028724 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  152, 8576, 9063, 5523, 2533,  678...6825, 2768, 5296,  117,  678, 2349,         102])\n",
      "23:48:59.028927 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.029646 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0,        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.030334 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.031322 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  152, 8576, 9063, 5523, 2533,  67... 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007377\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 21\n",
      "23:48:59.033163 call        14     def __getitem__(self, idx):\n",
      "23:48:59.033232 line        15         if self.mode == \"test\":\n",
      "23:48:59.033269 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '黄子韬：我演哭戏不用眼药水'\n",
      "New var:....... text_b = '黄子韬,眼药水,杨采钰,女主角,大秘密'\n",
      "New var:....... label = 0\n",
      "23:48:59.033793 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.033885 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.033957 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.034098 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['黄', '子', '韬', '：', '我', '演', '哭', '戏', '不', '用', '眼', '药', '水']\n",
      "23:48:59.034513 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '黄', '子', '韬', '：', '我', '演', '哭', '戏', '不', '用', '眼', '药', '水', '[SEP]']\n",
      "23:48:59.034767 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "23:48:59.034954 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['黄', '子', '韬', ',', '眼', '药', '水', ',', '杨', '采', '钰', ',', '女', '主', '角', ',', '大', '秘', '密']\n",
      "23:48:59.035489 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '黄', '子', '韬', '：', '我', '演', '哭', '戏'... ',', '女', '主', '角', ',', '大', '秘', '密', '[SEP]']\n",
      "23:48:59.035747 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "23:48:59.035935 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7942, 2094, 7507, 8038, 2769, 4028, 1526, ...117, 1957, 712, 6235, 117, 1920, 4908, 2166, 102]\n",
      "23:48:59.036132 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7942, 2094, 7507, 8038, 2769, 4028... 1957,  712, 6235,  117, 1920, 4908, 2166,  102])\n",
      "23:48:59.036336 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.036946 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.037552 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.038462 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7942, 2094, 7507, 8038, 2769, 402...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007232\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 22\n",
      "23:48:59.040425 call        14     def __getitem__(self, idx):\n",
      "23:48:59.040493 line        15         if self.mode == \"test\":\n",
      "23:48:59.040599 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '吴尊的肌肉是怎么练出来的？'\n",
      "New var:....... text_b = 'Fitness,飞轮海,吴尊,健身中心,文莱'\n",
      "New var:....... label = 0\n",
      "23:48:59.041131 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.041223 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.041296 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.041528 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['吴', '尊', '的', '肌', '肉', '是', '怎', '么', '练', '出', '来', '的', '？']\n",
      "23:48:59.041958 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '吴', '尊', '的', '肌', '肉', '是', '怎', '么', '练', '出', '来', '的', '？', '[SEP]']\n",
      "23:48:59.042139 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "23:48:59.042316 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['fit', '##ness', ',', '飞', '轮', '海', ',', '吴', '尊', ',', '健', '身', '中', '心', ',', '文', '莱']\n",
      "23:48:59.042838 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '吴', '尊', '的', '肌', '肉', '是', '怎', '么'... ',', '健', '身', '中', '心', ',', '文', '莱', '[SEP]']\n",
      "23:48:59.043695 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "23:48:59.043820 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1426, 2203, 4638, 5491, 5489, 3221, 2582, ... 117, 978, 6716, 704, 2552, 117, 3152, 5812, 102]\n",
      "23:48:59.043987 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  1426,  2203,  4638,  5491,  5489...  704,  2552,   117,         3152,  5812,   102])\n",
      "23:48:59.044272 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.044859 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.045932 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.047034 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  1426,  2203,  4638,  5491,  548...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009476\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 23\n",
      "23:48:59.049945 call        14     def __getitem__(self, idx):\n",
      "23:48:59.050046 line        15         if self.mode == \"test\":\n",
      "23:48:59.050095 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '韩瑜：出席活动，网友：真羡慕这个肌肉男！女神身材太好了'\n",
      "New var:....... text_b = '女神身材太,真羡慕,肌肉男,韩瑜,女神'\n",
      "New var:....... label = 0\n",
      "23:48:59.050764 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.051056 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.051278 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.051718 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['韩', '瑜', '：', '出', '席', '活', '动', '，', '网', '友...'肉', '男', '！', '女', '神', '身', '材', '太', '好', '了']\n",
      "23:48:59.052533 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '瑜', '：', '出', '席', '活', '动', '，'... '！', '女', '神', '身', '材', '太', '好', '了', '[SEP]']\n",
      "23:48:59.052818 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "23:48:59.053228 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['女', '神', '身', '材', '太', ',', '真', '羡', '慕', ',', '肌', '肉', '男', ',', '韩', '瑜', ',', '女', '神']\n",
      "23:48:59.053835 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '瑜', '：', '出', '席', '活', '动', '，'... '肉', '男', ',', '韩', '瑜', ',', '女', '神', '[SEP]']\n",
      "23:48:59.054120 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "23:48:59.054366 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7506, 4447, 8038, 1139, 2375, 3833, 1220, ...489, 4511, 117, 7506, 4447, 117, 1957, 4868, 102]\n",
      "23:48:59.054624 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7506, 4447, 8038, 1139, 2375, 3833... 117, 7506, 4447,  117, 1957, 4868,         102])\n",
      "23:48:59.054891 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.055872 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "23:48:59.056746 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.058091 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7506, 4447, 8038, 1139, 2375, 383...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010608\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 24\n",
      "23:48:59.060581 call        14     def __getitem__(self, idx):\n",
      "23:48:59.060652 line        15         if self.mode == \"test\":\n",
      "23:48:59.060689 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '陈都灵、沈月出演女主角'\n",
      "New var:....... text_b = '韩东,流星花园,七月与安生,沈月,七月,邹廷威,陈都灵,左耳'\n",
      "New var:....... label = 0\n",
      "23:48:59.061268 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.061361 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.061574 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.061799 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['陈', '都', '灵', '、', '沈', '月', '出', '演', '女', '主', '角']\n",
      "23:48:59.062250 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '都', '灵', '、', '沈', '月', '出', '演', '女', '主', '角', '[SEP]']\n",
      "23:48:59.062478 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 13\n",
      "23:48:59.062812 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['韩', '东', ',', '流', '星', '花', '园', ',', '七', '月...'邹', '廷', '威', ',', '陈', '都', '灵', ',', '左', '耳']\n",
      "23:48:59.063575 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '都', '灵', '、', '沈', '月', '出', '演'... '威', ',', '陈', '都', '灵', ',', '左', '耳', '[SEP]']\n",
      "23:48:59.063816 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 31\n",
      "23:48:59.064051 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7357, 6963, 4130, 510, 3755, 3299, 1139, 4...014, 117, 7357, 6963, 4130, 117, 2340, 5455, 102]\n",
      "23:48:59.064300 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7357, 6963, 4130,  510, 3755, 3299...  117, 7357, 6963, 4130,  117, 2340, 5455,  102])\n",
      "23:48:59.064556 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.065329 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.065985 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.067036 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7357, 6963, 4130,  510, 3755, 329... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009015\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 25\n",
      "23:48:59.069625 call        14     def __getitem__(self, idx):\n",
      "23:48:59.069693 line        15         if self.mode == \"test\":\n",
      "23:48:59.069730 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '笑笑被曝携手七号西卡探险 七号Miss深夜发文帮助澄清'\n",
      "New var:....... text_b = '微博,Miss,探险,德云色'\n",
      "New var:....... label = 0\n",
      "23:48:59.070268 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.070360 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.070434 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.070773 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['笑', '笑', '被', '曝', '携', '手', '七', '号', '西', '卡..., 'miss', '深', '夜', '发', '文', '帮', '助', '澄', '清']\n",
      "23:48:59.071376 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '笑', '笑', '被', '曝', '携', '手', '七', '号'... '深', '夜', '发', '文', '帮', '助', '澄', '清', '[SEP]']\n",
      "23:48:59.071589 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "23:48:59.071818 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['微', '博', ',', 'miss', ',', '探', '险', ',', '德', '云', '色']\n",
      "23:48:59.072276 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '笑', '笑', '被', '曝', '携', '手', '七', '号'...iss', ',', '探', '险', ',', '德', '云', '色', '[SEP]']\n",
      "23:48:59.072515 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 12\n",
      "23:48:59.072751 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5010, 5010, 6158, 3284, 3025, 2797, 673, 1...9368, 117, 2968, 7372, 117, 2548, 756, 5682, 102]\n",
      "23:48:59.073107 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5010, 5010, 6158, 3284, 3025, 2797...2968, 7372,  117, 2548,  756, 5682,         102])\n",
      "23:48:59.073362 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.074095 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0,        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.075123 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.076293 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5010, 5010, 6158, 3284, 3025, 279... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009817\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 26\n",
      "23:48:59.079486 call        14     def __getitem__(self, idx):\n",
      "23:48:59.079572 line        15         if self.mode == \"test\":\n",
      "23:48:59.079612 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '沈梦辰穿短裤秀美腿现身机场 和周觅、沈凌等人赴巴厘岛参加婚礼'\n",
      "New var:....... text_b = '付辛博,沈梦辰,巴厘岛,沈凌,周觅'\n",
      "New var:....... label = 0\n",
      "23:48:59.080325 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.080476 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.080577 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.080938 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['沈', '梦', '辰', '穿', '短', '裤', '秀', '美', '腿', '现...'等', '人', '赴', '巴', '厘', '岛', '参', '加', '婚', '礼']\n",
      "23:48:59.081716 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '沈', '梦', '辰', '穿', '短', '裤', '秀', '美'... '赴', '巴', '厘', '岛', '参', '加', '婚', '礼', '[SEP]']\n",
      "23:48:59.082253 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "23:48:59.082733 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['付', '辛', '博', ',', '沈', '梦', '辰', ',', '巴', '厘', '岛', ',', '沈', '凌', ',', '周', '觅']\n",
      "23:48:59.083732 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '沈', '梦', '辰', '穿', '短', '裤', '秀', '美'... '厘', '岛', ',', '沈', '凌', ',', '周', '觅', '[SEP]']\n",
      "23:48:59.084180 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "23:48:59.084689 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3755, 3457, 6801, 4959, 4764, 6175, 4899, ...330, 2270, 117, 3755, 1119, 117, 1453, 6227, 102]\n",
      "23:48:59.085040 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3755, 3457, 6801, 4959, 4764, 6175... 117, 3755, 1119,  117, 1453, 6227,         102])\n",
      "23:48:59.085714 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.086645 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "23:48:59.088448 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.089671 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3755, 3457, 6801, 4959, 4764, 617...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013031\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 27\n",
      "23:48:59.092555 call        14     def __getitem__(self, idx):\n",
      "23:48:59.092633 line        15         if self.mode == \"test\":\n",
      "23:48:59.092672 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '女星穿搭很重要，一不小心变槽点：盘点女星的扑街丑照'\n",
      "New var:....... text_b = '邓紫棋,马思纯,吴昕,快乐大本营,穿搭,刘亦菲'\n",
      "New var:....... label = 0\n",
      "23:48:59.093269 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.093362 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.093441 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.093796 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['女', '星', '穿', '搭', '很', '重', '要', '，', '一', '不...'：', '盘', '点', '女', '星', '的', '扑', '街', '丑', '照']\n",
      "23:48:59.094806 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '星', '穿', '搭', '很', '重', '要', '，'... '点', '女', '星', '的', '扑', '街', '丑', '照', '[SEP]']\n",
      "23:48:59.096430 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "23:48:59.097030 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['邓', '紫', '棋', ',', '马', '思', '纯', ',', '吴', '昕...'大', '本', '营', ',', '穿', '搭', ',', '刘', '亦', '菲']\n",
      "23:48:59.097775 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '星', '穿', '搭', '很', '重', '要', '，'... '营', ',', '穿', '搭', ',', '刘', '亦', '菲', '[SEP]']\n",
      "23:48:59.098241 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "23:48:59.098517 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1957, 3215, 4959, 3022, 2523, 7028, 6206, ...5852, 117, 4959, 3022, 117, 1155, 771, 5838, 102]\n",
      "23:48:59.098791 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1957, 3215, 4959, 3022, 2523, 7028...4959, 3022,  117, 1155,         771, 5838,  102])\n",
      "23:48:59.099178 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.099946 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "23:48:59.101182 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.103350 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1957, 3215, 4959, 3022, 2523, 702...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013516\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 28\n",
      "23:48:59.106106 call        14     def __getitem__(self, idx):\n",
      "23:48:59.106180 line        15         if self.mode == \"test\":\n",
      "23:48:59.106220 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '刘浩强：演员、导演、编剧、策划人'\n",
      "New var:....... text_b = '演员,编剧,电影,婚礼,山乡溢彩,叮当遇险记,克隆出租,刘浩强'\n",
      "New var:....... label = 0\n",
      "23:48:59.106803 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.106898 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.107078 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.107305 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['刘', '浩', '强', '：', '演', '员', '、', '导', '演', '、', '编', '剧', '、', '策', '划', '人']\n",
      "23:48:59.107895 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刘', '浩', '强', '：', '演', '员', '、', '导', '演', '、', '编', '剧', '、', '策', '划', '人', '[SEP]']\n",
      "23:48:59.108134 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 18\n",
      "23:48:59.108479 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['演', '员', ',', '编', '剧', ',', '电', '影', ',', '婚...'记', ',', '克', '隆', '出', '租', ',', '刘', '浩', '强']\n",
      "23:48:59.109268 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刘', '浩', '强', '：', '演', '员', '、', '导'... '克', '隆', '出', '租', ',', '刘', '浩', '强', '[SEP]']\n",
      "23:48:59.109557 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 32\n",
      "23:48:59.109796 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1155, 3856, 2487, 8038, 4028, 1447, 510, 2...46, 7384, 1139, 4909, 117, 1155, 3856, 2487, 102]\n",
      "23:48:59.110048 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1155, 3856, 2487, 8038, 4028, 1447...1139, 4909,  117, 1155, 3856,        2487,  102])\n",
      "23:48:59.110307 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.111430 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "23:48:59.112281 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.113724 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1155, 3856, 2487, 8038, 4028, 144...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009191\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 29\n",
      "23:48:59.115330 call        14     def __getitem__(self, idx):\n",
      "23:48:59.115403 line        15         if self.mode == \"test\":\n",
      "23:48:59.115441 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '唐嫣首部穿越剧开播，《天意》配角比主角演技还好，实力圈粉！'\n",
      "New var:....... text_b = '张丹峰,秦始皇,唐嫣,天意,剧中饰演,乔振宇'\n",
      "New var:....... label = 0\n",
      "23:48:59.116044 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.116280 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.116448 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.116628 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['唐', '嫣', '首', '部', '穿', '越', '剧', '开', '播', '，...'演', '技', '还', '好', '，', '实', '力', '圈', '粉', '！']\n",
      "23:48:59.117341 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '唐', '嫣', '首', '部', '穿', '越', '剧', '开'... '还', '好', '，', '实', '力', '圈', '粉', '！', '[SEP]']\n",
      "23:48:59.117533 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "23:48:59.117719 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['张', '丹', '峰', ',', '秦', '始', '皇', ',', '唐', '嫣...'意', ',', '剧', '中', '饰', '演', ',', '乔', '振', '宇']\n",
      "23:48:59.118325 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '唐', '嫣', '首', '部', '穿', '越', '剧', '开'... '剧', '中', '饰', '演', ',', '乔', '振', '宇', '[SEP]']\n",
      "23:48:59.118714 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "23:48:59.118914 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1538, 2073, 7674, 6956, 4959, 6632, 1196, ...1196, 704, 7652, 4028, 117, 730, 2920, 2126, 102]\n",
      "23:48:59.119125 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1538, 2073, 7674, 6956, 4959, 6632...7652,        4028,  117,  730, 2920, 2126,  102])\n",
      "23:48:59.119341 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.119970 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "23:48:59.120599 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.121737 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1538, 2073, 7674, 6956, 4959, 663...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008564\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 30\n",
      "23:48:59.123922 call        14     def __getitem__(self, idx):\n",
      "23:48:59.123990 line        15         if self.mode == \"test\":\n",
      "23:48:59.124026 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '时尚扫街·“暮光女”克里斯汀·斯图尔特，个性短裙下车大摆Pose'\n",
      "New var:....... text_b = '克里斯汀,MTV电影奖最佳女演员奖,暮光之城,奇幻爱情电影,暮光之城：新月,东方IC,暮光之城：月食,届MTV'\n",
      "New var:....... label = 0\n",
      "23:48:59.124519 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.124611 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.124683 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.124862 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['时', '尚', '扫', '街', '·', '[UNK]', '暮', '光', '女'..., '个', '性', '短', '裙', '下', '车', '大', '摆', 'pose']\n",
      "23:48:59.125634 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '时', '尚', '扫', '街', '·', '[UNK]', '暮',...', '短', '裙', '下', '车', '大', '摆', 'pose', '[SEP]']\n",
      "23:48:59.125823 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "23:48:59.126070 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['克', '里', '斯', '汀', ',', 'mtv', '电', '影', '奖', ...', '光', '之', '城', '：', '月', '食', ',', '届', 'mtv']\n",
      "23:48:59.127160 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '时', '尚', '扫', '街', '·', '[UNK]', '暮',...之', '城', '：', '月', '食', ',', '届', 'mtv', '[SEP]']\n",
      "23:48:59.127427 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 51\n",
      "23:48:59.127670 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3198, 2213, 2812, 6125, 185, 100, 3272, 10...2, 1814, 8038, 3299, 7608, 117, 2237, 11529, 102]\n",
      "23:48:59.127908 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  3198,  2213,  2812,  6125,   185... 3299,  7608,   117,  2237,        11529,   102])\n",
      "23:48:59.128138 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.129029 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.129968 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.131604 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  3198,  2213,  2812,  6125,   18...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010655\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 31\n",
      "23:48:59.134607 call        14     def __getitem__(self, idx):\n",
      "23:48:59.134678 line        15         if self.mode == \"test\":\n",
      "23:48:59.134715 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '被消失五年，这是近几年最让人不寒而栗的国产佳作'\n",
      "New var:....... text_b = '李玩,中国式,烈日灼心,鸟人,时间简史,曹保平,爱因斯坦,李米的猜想,追凶者也,周迅'\n",
      "New var:....... label = 0\n",
      "23:48:59.135211 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.135302 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.135375 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.135517 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['被', '消', '失', '五', '年', '，', '这', '是', '近', '几...'人', '不', '寒', '而', '栗', '的', '国', '产', '佳', '作']\n",
      "23:48:59.136101 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '被', '消', '失', '五', '年', '，', '这', '是'... '寒', '而', '栗', '的', '国', '产', '佳', '作', '[SEP]']\n",
      "23:48:59.136337 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "23:48:59.136517 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['李', '玩', ',', '中', '国', '式', ',', '烈', '日', '灼...'猜', '想', ',', '追', '凶', '者', '也', ',', '周', '迅']\n",
      "23:48:59.137529 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '被', '消', '失', '五', '年', '，', '这', '是'... ',', '追', '凶', '者', '也', ',', '周', '迅', '[SEP]']\n",
      "23:48:59.137743 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 43\n",
      "23:48:59.137940 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6158, 3867, 1927, 758, 2399, 8024, 6821, 3...117, 6841, 1136, 5442, 738, 117, 1453, 6813, 102]\n",
      "23:48:59.138158 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6158, 3867, 1927,  758, 2399, 8024... 6841, 1136, 5442,  738,  117, 1453, 6813,  102])\n",
      "23:48:59.138414 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.139186 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.140036 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.141429 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6158, 3867, 1927,  758, 2399, 802... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009224\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 32\n",
      "23:48:59.143859 call        14     def __getitem__(self, idx):\n",
      "23:48:59.143924 line        15         if self.mode == \"test\":\n",
      "23:48:59.143961 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '她出演了韩国的综艺节目，她的美被人发现'\n",
      "New var:....... text_b = '塞尔维亚语,综艺节目,Angelina,圣彼得堡,俄罗斯'\n",
      "New var:....... label = 0\n",
      "23:48:59.144455 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.144540 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.144607 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.144715 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['她', '出', '演', '了', '韩', '国', '的', '综', '艺', '节', '目', '，', '她', '的', '美', '被', '人', '发', '现']\n",
      "23:48:59.145344 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '她', '出', '演', '了', '韩', '国', '的', '综'... '，', '她', '的', '美', '被', '人', '发', '现', '[SEP]']\n",
      "23:48:59.145544 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "23:48:59.145731 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['塞', '尔', '维', '亚', '语', ',', '综', '艺', '节', '目...na', ',', '圣', '彼', '得', '堡', ',', '俄', '罗', '斯']\n",
      "23:48:59.146418 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '她', '出', '演', '了', '韩', '国', '的', '综'... '圣', '彼', '得', '堡', ',', '俄', '罗', '斯', '[SEP]']\n",
      "23:48:59.146616 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "23:48:59.146806 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1961, 1139, 4028, 749, 7506, 1744, 4638, 5...760, 2516, 2533, 1836, 117, 915, 5384, 3172, 102]\n",
      "23:48:59.147044 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1961, 1139, 4028,  749, 7506, 1744... 2516, 2533, 1836,  117,  915, 5384, 3172,  102])\n",
      "23:48:59.147256 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.147875 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.148562 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.149613 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1961, 1139, 4028,  749, 7506, 174... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007804\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 33\n",
      "23:48:59.151695 call        14     def __getitem__(self, idx):\n",
      "23:48:59.151765 line        15         if self.mode == \"test\":\n",
      "23:48:59.151802 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = 'TVB艺人吴家乐拍新剧遭人排挤，好友李珊珊安慰：遇神杀神！'\n",
      "New var:....... text_b = '吴家乐,包青天再起风云,花蝴蝶,谭俊彦,姚子羚,城寨英雄,李珊珊,胡定欣'\n",
      "New var:....... label = 0\n",
      "23:48:59.152440 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.152541 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.152730 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.153115 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['tvb', '艺', '人', '吴', '家', '乐', '拍', '新', '剧', ...'珊', '珊', '安', '慰', '：', '遇', '神', '杀', '神', '！']\n",
      "23:48:59.153863 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'tvb', '艺', '人', '吴', '家', '乐', '拍', '... '安', '慰', '：', '遇', '神', '杀', '神', '！', '[SEP]']\n",
      "23:48:59.154125 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "23:48:59.154312 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['吴', '家', '乐', ',', '包', '青', '天', '再', '起', '风...'英', '雄', ',', '李', '珊', '珊', ',', '胡', '定', '欣']\n",
      "23:48:59.155139 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'tvb', '艺', '人', '吴', '家', '乐', '拍', '... ',', '李', '珊', '珊', ',', '胡', '定', '欣', '[SEP]']\n",
      "23:48:59.155356 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 37\n",
      "23:48:59.155551 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 9312, 5686, 782, 1426, 2157, 727, 2864, 31...17, 3330, 4396, 4396, 117, 5529, 2137, 3615, 102]\n",
      "23:48:59.155768 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 9312, 5686,  782, 1426, 2157,  727...4396,        4396,  117, 5529, 2137, 3615,  102])\n",
      "23:48:59.155989 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.156754 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.157529 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.158764 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 9312, 5686,  782, 1426, 2157,  72... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009583\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 34\n",
      "23:48:59.161308 call        14     def __getitem__(self, idx):\n",
      "23:48:59.161377 line        15         if self.mode == \"test\":\n",
      "23:48:59.161415 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '言承旭带娃俨然慈父，端小碗耐心喂饭超宠溺，网友喊话林志玲'\n",
      "New var:....... text_b = '放开我北鼻,言承旭,经纪人,真人秀,林志玲'\n",
      "New var:....... label = 0\n",
      "23:48:59.161970 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.162062 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.162133 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.162376 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['言', '承', '旭', '带', '娃', '俨', '然', '慈', '父', '，...'宠', '溺', '，', '网', '友', '喊', '话', '林', '志', '玲']\n",
      "23:48:59.163184 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '言', '承', '旭', '带', '娃', '俨', '然', '慈'... '，', '网', '友', '喊', '话', '林', '志', '玲', '[SEP]']\n",
      "23:48:59.163308 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "23:48:59.163423 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['放', '开', '我', '北', '鼻', ',', '言', '承', '旭', ',...'纪', '人', ',', '真', '人', '秀', ',', '林', '志', '玲']\n",
      "23:48:59.164025 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '言', '承', '旭', '带', '娃', '俨', '然', '慈'... ',', '真', '人', '秀', ',', '林', '志', '玲', '[SEP]']\n",
      "23:48:59.164152 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 22\n",
      "23:48:59.164277 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6241, 2824, 3195, 2372, 2015, 929, 4197, 2...117, 4696, 782, 4899, 117, 3360, 2562, 4386, 102]\n",
      "23:48:59.164589 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6241, 2824, 3195, 2372, 2015,  929... 782, 4899,  117,        3360, 2562, 4386,  102])\n",
      "23:48:59.164859 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.165479 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "23:48:59.166098 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.167216 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6241, 2824, 3195, 2372, 2015,  92...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008171\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 35\n",
      "23:48:59.169507 call        14     def __getitem__(self, idx):\n",
      "23:48:59.169576 line        15         if self.mode == \"test\":\n",
      "23:48:59.169613 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '一部反应人性的电影，母女逃难被全村人欺负，让人心焦'\n",
      "New var:....... text_b = '全村人,电影,母女'\n",
      "New var:....... label = 0\n",
      "23:48:59.170089 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.170179 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.170251 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.170429 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['一', '部', '反', '应', '人', '性', '的', '电', '影', '，...'全', '村', '人', '欺', '负', '，', '让', '人', '心', '焦']\n",
      "23:48:59.171062 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '一', '部', '反', '应', '人', '性', '的', '电'... '人', '欺', '负', '，', '让', '人', '心', '焦', '[SEP]']\n",
      "23:48:59.171270 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "23:48:59.171430 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['全', '村', '人', ',', '电', '影', ',', '母', '女']\n",
      "23:48:59.171858 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '一', '部', '反', '应', '人', '性', '的', '电'... '村', '人', ',', '电', '影', ',', '母', '女', '[SEP]']\n",
      "23:48:59.172051 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 10\n",
      "23:48:59.172238 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 671, 6956, 1353, 2418, 782, 2595, 4638, 45...3333, 782, 117, 4510, 2512, 117, 3678, 1957, 102]\n",
      "23:48:59.172435 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  671, 6956, 1353, 2418,  782, 2595... 117, 4510, 2512,  117, 3678, 1957,         102])\n",
      "23:48:59.172678 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.173245 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0,        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.173882 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.174717 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  671, 6956, 1353, 2418,  782, 259... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007049\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 36\n",
      "23:48:59.176590 call        14     def __getitem__(self, idx):\n",
      "23:48:59.176663 line        15         if self.mode == \"test\":\n",
      "23:48:59.176701 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '女明星的高跟鞋谁最奇特？'\n",
      "New var:....... text_b = '女明星,景甜,冉莹颖,尚雯婕,欧阳娜娜,邹市明,女人味,范冰冰,袁姗姗,驴蹄鞋,恨天高,连衣裙'\n",
      "New var:....... label = 0\n",
      "23:48:59.177181 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.177272 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.177346 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.177592 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['女', '明', '星', '的', '高', '跟', '鞋', '谁', '最', '奇', '特', '？']\n",
      "23:48:59.178006 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '明', '星', '的', '高', '跟', '鞋', '谁', '最', '奇', '特', '？', '[SEP]']\n",
      "23:48:59.178188 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 14\n",
      "23:48:59.178362 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['女', '明', '星', ',', '景', '甜', ',', '冉', '莹', '颖...'蹄', '鞋', ',', '恨', '天', '高', ',', '连', '衣', '裙']\n",
      "23:48:59.179362 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '明', '星', '的', '高', '跟', '鞋', '谁'... ',', '恨', '天', '高', ',', '连', '衣', '裙', '[SEP]']\n",
      "23:48:59.179676 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 48\n",
      "23:48:59.180091 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1957, 3209, 3215, 4638, 7770, 6656, 7490, ...17, 2616, 1921, 7770, 117, 6825, 6132, 6170, 102]\n",
      "23:48:59.180238 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1957, 3209, 3215, 4638, 7770, 6656...1921, 7770,  117, 6825, 6132,        6170,  102])\n",
      "23:48:59.180383 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.180905 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.181443 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.182853 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1957, 3209, 3215, 4638, 7770, 665... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008701\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 37\n",
      "23:48:59.185321 call        14     def __getitem__(self, idx):\n",
      "23:48:59.185391 line        15         if self.mode == \"test\":\n",
      "23:48:59.185429 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '鹿晗出行澳门被网友偶遇，身旁同行的女孩亮了，有点眼熟'\n",
      "New var:....... text_b = '鹿晗,澳门,粉丝'\n",
      "New var:....... label = 0\n",
      "23:48:59.185906 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.185997 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.186069 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.186260 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['鹿', '晗', '出', '行', '澳', '门', '被', '网', '友', '偶...'的', '女', '孩', '亮', '了', '，', '有', '点', '眼', '熟']\n",
      "23:48:59.186920 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '鹿', '晗', '出', '行', '澳', '门', '被', '网'... '孩', '亮', '了', '，', '有', '点', '眼', '熟', '[SEP]']\n",
      "23:48:59.187110 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "23:48:59.187293 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['鹿', '晗', ',', '澳', '门', ',', '粉', '丝']\n",
      "23:48:59.187705 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '鹿', '晗', '出', '行', '澳', '门', '被', '网'... '鹿', '晗', ',', '澳', '门', ',', '粉', '丝', '[SEP]']\n",
      "23:48:59.187898 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 9\n",
      "23:48:59.188085 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7922, 3240, 1139, 6121, 4078, 7305, 6158, ...7922, 3240, 117, 4078, 7305, 117, 5106, 692, 102]\n",
      "23:48:59.188283 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7922, 3240, 1139, 6121, 4078, 7305... 117, 4078, 7305,  117, 5106,  692,         102])\n",
      "23:48:59.188487 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.189056 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0,        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.189726 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.190665 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7922, 3240, 1139, 6121, 4078, 730... 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007266\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 38\n",
      "23:48:59.192614 call        14     def __getitem__(self, idx):\n",
      "23:48:59.192682 line        15         if self.mode == \"test\":\n",
      "23:48:59.192718 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = 'Jasper为应采儿庆祝母亲节用这招，网友：心都化了'\n",
      "New var:....... text_b = '爸爸去哪儿,社交网站,母亲节,陈小春,爸爸去哪儿5,应采儿,Jasper'\n",
      "New var:....... label = 0\n",
      "23:48:59.193181 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.193272 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.193343 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.193562 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['j', '##as', '##per', '为', '应', '采', '儿', '庆', ...'这', '招', '，', '网', '友', '：', '心', '都', '化', '了']\n",
      "23:48:59.194175 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'j', '##as', '##per', '为', '应', '采', '... '，', '网', '友', '：', '心', '都', '化', '了', '[SEP]']\n",
      "23:48:59.194362 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "23:48:59.194542 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['爸', '爸', '去', '哪', '儿', ',', '社', '交', '网', '站...', ',', '应', '采', '儿', ',', 'j', '##as', '##per']\n",
      "23:48:59.195321 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'j', '##as', '##per', '为', '应', '采', '...应', '采', '儿', ',', 'j', '##as', '##per', '[SEP]']\n",
      "23:48:59.195558 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 34\n",
      "23:48:59.195751 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 152, 8576, 9063, 711, 2418, 7023, 1036, 24...117, 2418, 7023, 1036, 117, 152, 8576, 9063, 102]\n",
      "23:48:59.195960 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  152, 8576, 9063,  711, 2418, 7023... 2418, 7023, 1036,  117,  152, 8576, 9063,  102])\n",
      "23:48:59.196174 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.196827 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.197599 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.198693 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  152, 8576, 9063,  711, 2418, 702...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008415\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 39\n",
      "23:48:59.201056 call        14     def __getitem__(self, idx):\n",
      "23:48:59.201122 line        15         if self.mode == \"test\":\n",
      "23:48:59.201159 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '娱乐圈中男扮女装的男星们，有的变身萌妹，有的辣眼睛！'\n",
      "New var:....... text_b = '美人痣,男星,娱乐圈,马天宇,萌妹子'\n",
      "New var:....... label = 0\n",
      "23:48:59.201620 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.201709 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.201779 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.202072 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['娱', '乐', '圈', '中', '男', '扮', '女', '装', '的', '男...'身', '萌', '妹', '，', '有', '的', '辣', '眼', '睛', '！']\n",
      "23:48:59.202720 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '中', '男', '扮', '女', '装'... '妹', '，', '有', '的', '辣', '眼', '睛', '！', '[SEP]']\n",
      "23:48:59.202904 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "23:48:59.203083 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['美', '人', '痣', ',', '男', '星', ',', '娱', '乐', '圈', ',', '马', '天', '宇', ',', '萌', '妹', '子']\n",
      "23:48:59.203775 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '中', '男', '扮', '女', '装'... ',', '马', '天', '宇', ',', '萌', '妹', '子', '[SEP]']\n",
      "23:48:59.204287 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "23:48:59.204614 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2031, 727, 1750, 704, 4511, 2815, 1957, 61...17, 7716, 1921, 2126, 117, 5846, 1987, 2094, 102]\n",
      "23:48:59.204970 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2031,  727, 1750,  704, 4511, 2815... 7716, 1921, 2126,  117, 5846, 1987, 2094,  102])\n",
      "23:48:59.205312 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.205918 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.206672 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.207790 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2031,  727, 1750,  704, 4511, 281... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008804\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 40\n",
      "23:48:59.209894 call        14     def __getitem__(self, idx):\n",
      "23:48:59.209968 line        15         if self.mode == \"test\":\n",
      "23:48:59.210006 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '高颜值王源也会选错配件，贵气风衣却搭地摊包包，效果一言难尽'\n",
      "New var:....... text_b = '牛仔服,邮差包,王源,Salvatore,大主宰,Ferragamo'\n",
      "New var:....... label = 0\n",
      "23:48:59.210619 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.210711 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.210789 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.211041 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['高', '颜', '值', '王', '源', '也', '会', '选', '错', '配...'摊', '包', '包', '，', '效', '果', '一', '言', '难', '尽']\n",
      "23:48:59.211764 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '高', '颜', '值', '王', '源', '也', '会', '选'... '包', '，', '效', '果', '一', '言', '难', '尽', '[SEP]']\n",
      "23:48:59.211995 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "23:48:59.212180 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['牛', '仔', '服', ',', '邮', '差', '包', ',', '王', '源...salvatore', ',', '大', '主', '宰', ',', 'ferragamo']\n",
      "23:48:59.212793 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '高', '颜', '值', '王', '源', '也', '会', '选'...', ',', '大', '主', '宰', ',', 'ferragamo', '[SEP]']\n",
      "23:48:59.212991 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "23:48:59.213278 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7770, 7582, 966, 4374, 3975, 738, 833, 684...117, 10078, 117, 1920, 712, 2153, 117, 9992, 102]\n",
      "23:48:59.213522 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  7770,  7582,   966,  4374,  3975...  117,  1920,   712,  2153,   117,  9992,   102])\n",
      "23:48:59.213755 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.214406 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "23:48:59.215019 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.216119 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  7770,  7582,   966,  4374,  397...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008274\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 41\n",
      "23:48:59.218269 call        14     def __getitem__(self, idx):\n",
      "23:48:59.218340 line        15         if self.mode == \"test\":\n",
      "23:48:59.218377 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '王菲私人小号疑似曝光，分享生活日常还晒与谢霆锋合影'\n",
      "New var:....... text_b = '分享生活日常,王菲,谢霆锋,谢霆锋合影王菲私人小号,王菲私人小号疑似曝光'\n",
      "New var:....... label = 0\n",
      "23:48:59.218877 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.218968 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.219040 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.219219 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['王', '菲', '私', '人', '小', '号', '疑', '似', '曝', '光...'日', '常', '还', '晒', '与', '谢', '霆', '锋', '合', '影']\n",
      "23:48:59.219857 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '菲', '私', '人', '小', '号', '疑', '似'... '还', '晒', '与', '谢', '霆', '锋', '合', '影', '[SEP]']\n",
      "23:48:59.220045 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "23:48:59.220271 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['分', '享', '生', '活', '日', '常', ',', '王', '菲', ',...'王', '菲', '私', '人', '小', '号', '疑', '似', '曝', '光']\n",
      "23:48:59.221148 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '菲', '私', '人', '小', '号', '疑', '似'... '私', '人', '小', '号', '疑', '似', '曝', '光', '[SEP]']\n",
      "23:48:59.221355 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 37\n",
      "23:48:59.221543 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4374, 5838, 4900, 782, 2207, 1384, 4542, 8...900, 782, 2207, 1384, 4542, 849, 3284, 1045, 102]\n",
      "23:48:59.221781 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4374, 5838, 4900,  782, 2207, 1384...2207, 1384, 4542,         849, 3284, 1045,  102])\n",
      "23:48:59.222054 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.222890 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.223620 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.224858 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4374, 5838, 4900,  782, 2207, 138... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009267\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 42\n",
      "23:48:59.227562 call        14     def __getitem__(self, idx):\n",
      "23:48:59.227627 line        15         if self.mode == \"test\":\n",
      "23:48:59.227662 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '很快和鹿晗结婚？关晓彤的回答太甜蜜了！'\n",
      "New var:....... text_b = '鹿晗,关晓彤,粉丝,董子健,男明星'\n",
      "New var:....... label = 0\n",
      "23:48:59.228116 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.228204 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.228272 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.228536 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['很', '快', '和', '鹿', '晗', '结', '婚', '？', '关', '晓', '彤', '的', '回', '答', '太', '甜', '蜜', '了', '！']\n",
      "23:48:59.229085 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '很', '快', '和', '鹿', '晗', '结', '婚', '？'... '的', '回', '答', '太', '甜', '蜜', '了', '！', '[SEP]']\n",
      "23:48:59.229316 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "23:48:59.229519 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['鹿', '晗', ',', '关', '晓', '彤', ',', '粉', '丝', ',', '董', '子', '健', ',', '男', '明', '星']\n",
      "23:48:59.230030 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '很', '快', '和', '鹿', '晗', '结', '婚', '？'... ',', '董', '子', '健', ',', '男', '明', '星', '[SEP]']\n",
      "23:48:59.230278 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "23:48:59.230489 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2523, 2571, 1469, 7922, 3240, 5310, 2042, ...117, 5869, 2094, 978, 117, 4511, 3209, 3215, 102]\n",
      "23:48:59.230776 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2523, 2571, 1469, 7922, 3240, 5310...2094,  978,  117, 4511,        3209, 3215,  102])\n",
      "23:48:59.231009 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.231703 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.232338 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.233477 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2523, 2571, 1469, 7922, 3240, 531... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008029\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 43\n",
      "23:48:59.235625 call        14     def __getitem__(self, idx):\n",
      "23:48:59.235697 line        15         if self.mode == \"test\":\n",
      "23:48:59.235735 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '唐嫣身穿时尚现身机场，网友：正值颜值最巅峰'\n",
      "New var:....... text_b = '唐嫣,颜值,东方IC,机场,东方ic'\n",
      "New var:....... label = 0\n",
      "23:48:59.236254 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.236347 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.236422 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.236640 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['唐', '嫣', '身', '穿', '时', '尚', '现', '身', '机', '场...'网', '友', '：', '正', '值', '颜', '值', '最', '巅', '峰']\n",
      "23:48:59.237280 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '唐', '嫣', '身', '穿', '时', '尚', '现', '身'... '：', '正', '值', '颜', '值', '最', '巅', '峰', '[SEP]']\n",
      "23:48:59.237472 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "23:48:59.237658 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['唐', '嫣', ',', '颜', '值', ',', '东', '方', 'ic', ',', '机', '场', ',', '东', '方', 'ic']\n",
      "23:48:59.238221 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '唐', '嫣', '身', '穿', '时', '尚', '现', '身'...ic', ',', '机', '场', ',', '东', '方', 'ic', '[SEP]']\n",
      "23:48:59.238415 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 17\n",
      "23:48:59.238604 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1538, 2073, 6716, 4959, 3198, 2213, 4385, ...8577, 117, 3322, 1767, 117, 691, 3175, 8577, 102]\n",
      "23:48:59.238805 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1538, 2073, 6716, 4959, 3198, 2213...3322, 1767,  117,         691, 3175, 8577,  102])\n",
      "23:48:59.239052 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.239642 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.240248 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.241126 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1538, 2073, 6716, 4959, 3198, 221... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007422\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 44\n",
      "23:48:59.243082 call        14     def __getitem__(self, idx):\n",
      "23:48:59.243159 line        15         if self.mode == \"test\":\n",
      "23:48:59.243201 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '这款小型SUV比宝骏510智能，难怪市场下滑也能连续6个月过万辆'\n",
      "New var:....... text_b = '极限挑战,远景X3,真质良品SUV,优等生,起跑线,吉利汽车'\n",
      "New var:....... label = 0\n",
      "23:48:59.243731 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.243831 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.243909 line        25         word_pieces = [\"[CLS]\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([64, 82]) \n",
      "tensor([[ 101, 1367, 1213,  ...,    0,    0,    0],\n",
      "        [ 101, 1259, 6564,  ...,    0,    0,    0],\n",
      "        [ 101, 2031,  727,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  517, 7478,  ...,    0,    0,    0],\n",
      "        [ 101, 6821,  697,  ...,    0,    0,    0],\n",
      "        [ 101, 2128, 1395,  ...,    0,    0,    0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([64, 82])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([64, 82])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([64])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.244135 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['这', '款', '小', '型', 'suv', '比', '宝', '骏', '510'...'也', '能', '连', '续', '6', '个', '月', '过', '万', '辆']\n",
      "23:48:59.244961 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '款', '小', '型', 'suv', '比', '宝', '... '连', '续', '6', '个', '月', '过', '万', '辆', '[SEP]']\n",
      "23:48:59.245092 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "23:48:59.245794 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['极', '限', '挑', '战', ',', '远', '景', 'x3', ',', '...'生', ',', '起', '跑', '线', ',', '吉', '利', '汽', '车']\n",
      "23:48:59.246455 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '款', '小', '型', 'suv', '比', '宝', '... '起', '跑', '线', ',', '吉', '利', '汽', '车', '[SEP]']\n",
      "23:48:59.246753 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 28\n",
      "23:48:59.246962 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6821, 3621, 2207, 1798, 8540, 3683, 2140, ...29, 6651, 5296, 117, 1395, 1164, 3749, 6756, 102]\n",
      "23:48:59.247179 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  6821,  3621,  2207,  1798,  8540... 5296,   117,  1395,  1164,  3749,  6756,   102])\n",
      "23:48:59.247488 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.248723 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.249737 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.251364 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  6821,  3621,  2207,  1798,  854...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010776\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 45\n",
      "23:48:59.253894 call        14     def __getitem__(self, idx):\n",
      "23:48:59.253968 line        15         if self.mode == \"test\":\n",
      "23:48:59.254007 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '后宫才华服饰比美，贵妃创意被皇后夺取，现代女子帮贵妃扭转乾坤'\n",
      "New var:....... text_b = '皇后,后宫,服饰'\n",
      "New var:....... label = 0\n",
      "23:48:59.254629 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.254723 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.254924 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.255152 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['后', '宫', '才', '华', '服', '饰', '比', '美', '，', '贵...'代', '女', '子', '帮', '贵', '妃', '扭', '转', '乾', '坤']\n",
      "23:48:59.255939 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '后', '宫', '才', '华', '服', '饰', '比', '美'... '子', '帮', '贵', '妃', '扭', '转', '乾', '坤', '[SEP]']\n",
      "23:48:59.256319 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "23:48:59.256600 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['皇', '后', ',', '后', '宫', ',', '服', '饰']\n",
      "23:48:59.257153 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '后', '宫', '才', '华', '服', '饰', '比', '美'... '皇', '后', ',', '后', '宫', ',', '服', '饰', '[SEP]']\n",
      "23:48:59.257419 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 9\n",
      "23:48:59.257660 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1400, 2151, 2798, 1290, 3302, 7652, 3683, ...640, 1400, 117, 1400, 2151, 117, 3302, 7652, 102]\n",
      "23:48:59.257912 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1400, 2151, 2798, 1290, 3302, 7652... 117, 1400,        2151,  117, 3302, 7652,  102])\n",
      "23:48:59.258173 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.258926 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.259567 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.260583 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1400, 2151, 2798, 1290, 3302, 765... 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008918\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 46\n",
      "23:48:59.262844 call        14     def __getitem__(self, idx):\n",
      "23:48:59.262916 line        15         if self.mode == \"test\":\n",
      "23:48:59.262955 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '杨幂旗下9位小花旦，我猜你只认识迪丽热巴，能叫出名字，算你牛'\n",
      "New var:....... text_b = '铁粉杨幂旗下,迪丽热巴,我猜,幂幂,花旦'\n",
      "New var:....... label = 0\n",
      "23:48:59.263540 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.263633 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.263709 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.264092 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['杨', '幂', '旗', '下', '9', '位', '小', '花', '旦', '，...'，', '能', '叫', '出', '名', '字', '，', '算', '你', '牛']\n",
      "23:48:59.264919 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '幂', '旗', '下', '9', '位', '小', '花'... '叫', '出', '名', '字', '，', '算', '你', '牛', '[SEP]']\n",
      "23:48:59.265193 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "23:48:59.265442 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['铁', '粉', '杨', '幂', '旗', '下', ',', '迪', '丽', '热', '巴', ',', '我', '猜', ',', '幂', '幂', ',', '花', '旦']\n",
      "23:48:59.266068 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '幂', '旗', '下', '9', '位', '小', '花'... '我', '猜', ',', '幂', '幂', ',', '花', '旦', '[SEP]']\n",
      "23:48:59.266430 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "23:48:59.266672 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3342, 2386, 3186, 678, 130, 855, 2207, 570...769, 4339, 117, 2386, 2386, 117, 5709, 3190, 102]\n",
      "23:48:59.266932 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3342, 2386, 3186,  678,  130,  855... 117, 2386,        2386,  117, 5709, 3190,  102])\n",
      "23:48:59.267357 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.268080 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1])\n",
      "23:48:59.269013 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.270248 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3342, 2386, 3186,  678,  130,  85...1, 1, 1, 1, 1,        1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010264\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 47\n",
      "23:48:59.273138 call        14     def __getitem__(self, idx):\n",
      "23:48:59.273208 line        15         if self.mode == \"test\":\n",
      "23:48:59.273245 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '舒淇出席某活动 网友：看到她的腿，就不在乎她的嘴！'\n",
      "New var:....... text_b = '舒淇出席,舒淇'\n",
      "New var:....... label = 0\n",
      "23:48:59.273847 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.274054 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.274130 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.274353 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['舒', '淇', '出', '席', '某', '活', '动', '网', '友', '：...'腿', '，', '就', '不', '在', '乎', '她', '的', '嘴', '！']\n",
      "23:48:59.275029 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '舒', '淇', '出', '席', '某', '活', '动', '网'... '就', '不', '在', '乎', '她', '的', '嘴', '！', '[SEP]']\n",
      "23:48:59.275260 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "23:48:59.275486 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['舒', '淇', '出', '席', ',', '舒', '淇']\n",
      "23:48:59.275860 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '舒', '淇', '出', '席', '某', '活', '动', '网'...EP]', '舒', '淇', '出', '席', ',', '舒', '淇', '[SEP]']\n",
      "23:48:59.276205 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 8\n",
      "23:48:59.276437 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5653, 3899, 1139, 2375, 3378, 3833, 1220, ...02, 5653, 3899, 1139, 2375, 117, 5653, 3899, 102]\n",
      "23:48:59.276680 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5653, 3899, 1139, 2375, 3378, 3833... 5653, 3899, 1139, 2375,  117, 5653, 3899,  102])\n",
      "23:48:59.276930 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.277558 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0,        0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.278177 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.279328 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5653, 3899, 1139, 2375, 3378, 383...       0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008162\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 48\n",
      "23:48:59.281329 call        14     def __getitem__(self, idx):\n",
      "23:48:59.281509 line        15         if self.mode == \"test\":\n",
      "23:48:59.281548 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '推五部将军为背景的古言情小说，第二部有点像女版的琅琊榜'\n",
      "New var:....... text_b = '将军有喜,琅琊榜,将军叼回个小娇娘,男主,女主,穿越文,大将军,将军策嫡女权谋'\n",
      "New var:....... label = 0\n",
      "23:48:59.282093 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.282186 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.282259 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.282483 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['推', '五', '部', '将', '军', '为', '背', '景', '的', '古...'部', '有', '点', '像', '女', '版', '的', '琅', '琊', '榜']\n",
      "23:48:59.283207 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '推', '五', '部', '将', '军', '为', '背', '景'... '点', '像', '女', '版', '的', '琅', '琊', '榜', '[SEP]']\n",
      "23:48:59.283334 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "23:48:59.283683 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['将', '军', '有', '喜', ',', '琅', '琊', '榜', ',', '将...'将', '军', ',', '将', '军', '策', '嫡', '女', '权', '谋']\n",
      "23:48:59.284604 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '推', '五', '部', '将', '军', '为', '背', '景'... ',', '将', '军', '策', '嫡', '女', '权', '谋', '[SEP]']\n",
      "23:48:59.284848 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 40\n",
      "23:48:59.285088 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2972, 758, 6956, 2199, 1092, 711, 5520, 32...7, 2199, 1092, 5032, 2072, 1957, 3326, 6450, 102]\n",
      "23:48:59.285347 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2972,  758, 6956, 2199, 1092,  711... 2199, 1092, 5032, 2072, 1957, 3326, 6450,  102])\n",
      "23:48:59.285580 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.286474 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.287311 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.288858 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2972,  758, 6956, 2199, 1092,  71... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010294\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 49\n",
      "23:48:59.291655 call        14     def __getitem__(self, idx):\n",
      "23:48:59.291725 line        15         if self.mode == \"test\":\n",
      "23:48:59.291762 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '朱亚文出身于什么样的家庭？'\n",
      "New var:....... text_b = '父辈的旗帜,父傲,艺人,朱亚文,娱乐圈,闯关东'\n",
      "New var:....... label = 0\n",
      "23:48:59.292316 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.292407 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.292481 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.292707 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['朱', '亚', '文', '出', '身', '于', '什', '么', '样', '的', '家', '庭', '？']\n",
      "23:48:59.293187 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '朱', '亚', '文', '出', '身', '于', '什', '么', '样', '的', '家', '庭', '？', '[SEP]']\n",
      "23:48:59.293414 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "23:48:59.293635 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['父', '辈', '的', '旗', '帜', ',', '父', '傲', ',', '艺...'亚', '文', ',', '娱', '乐', '圈', ',', '闯', '关', '东']\n",
      "23:48:59.294388 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '朱', '亚', '文', '出', '身', '于', '什', '么'... ',', '娱', '乐', '圈', ',', '闯', '关', '东', '[SEP]']\n",
      "23:48:59.294628 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "23:48:59.295176 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3319, 762, 3152, 1139, 6716, 754, 784, 720... 117, 2031, 727, 1750, 117, 7310, 1068, 691, 102]\n",
      "23:48:59.295441 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3319,  762, 3152, 1139, 6716,  754... 727, 1750,  117, 7310,        1068,  691,  102])\n",
      "23:48:59.295700 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.296438 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.297066 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.298156 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3319,  762, 3152, 1139, 6716,  75... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008993\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 50\n",
      "23:48:59.300677 call        14     def __getitem__(self, idx):\n",
      "23:48:59.300745 line        15         if self.mode == \"test\":\n",
      "23:48:59.300782 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '林心如曾是多少人的梦中情人，现在她当妈了，非常受宝宝喜欢'\n",
      "New var:....... text_b = '霍建华,林心如,小孩子'\n",
      "New var:....... label = 0\n",
      "23:48:59.301306 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.301398 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.301470 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.301812 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['林', '心', '如', '曾', '是', '多', '少', '人', '的', '梦...'妈', '了', '，', '非', '常', '受', '宝', '宝', '喜', '欢']\n",
      "23:48:59.302510 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '心', '如', '曾', '是', '多', '少', '人'... '，', '非', '常', '受', '宝', '宝', '喜', '欢', '[SEP]']\n",
      "23:48:59.302800 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "23:48:59.303086 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['霍', '建', '华', ',', '林', '心', '如', ',', '小', '孩', '子']\n",
      "23:48:59.303596 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '心', '如', '曾', '是', '多', '少', '人'... ',', '林', '心', '如', ',', '小', '孩', '子', '[SEP]']\n",
      "23:48:59.303959 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 12\n",
      "23:48:59.304201 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3360, 2552, 1963, 3295, 3221, 1914, 2208, ...17, 3360, 2552, 1963, 117, 2207, 2111, 2094, 102]\n",
      "23:48:59.304458 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3360, 2552, 1963, 3295, 3221, 1914...2552,        1963,  117, 2207, 2111, 2094,  102])\n",
      "23:48:59.304719 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.305373 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.306135 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.307181 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3360, 2552, 1963, 3295, 3221, 191... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008619\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 51\n",
      "23:48:59.309326 call        14     def __getitem__(self, idx):\n",
      "23:48:59.309395 line        15         if self.mode == \"test\":\n",
      "23:48:59.309433 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '陈乔恩携爱犬登封面 放慢脚步享受友爱生活'\n",
      "New var:....... text_b = '爱犬,陈乔恩'\n",
      "New var:....... label = 0\n",
      "23:48:59.310028 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.310121 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.310197 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.310537 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['陈', '乔', '恩', '携', '爱', '犬', '登', '封', '面', '放', '慢', '脚', '步', '享', '受', '友', '爱', '生', '活']\n",
      "23:48:59.311130 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '乔', '恩', '携', '爱', '犬', '登', '封'... '脚', '步', '享', '受', '友', '爱', '生', '活', '[SEP]']\n",
      "23:48:59.311429 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "23:48:59.311687 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['爱', '犬', ',', '陈', '乔', '恩']\n",
      "23:48:59.312061 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '乔', '恩', '携', '爱', '犬', '登', '封'..., '[SEP]', '爱', '犬', ',', '陈', '乔', '恩', '[SEP]']\n",
      "23:48:59.312303 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 7\n",
      "23:48:59.312538 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7357, 730, 2617, 3025, 4263, 4305, 4633, 2...3833, 102, 4263, 4305, 117, 7357, 730, 2617, 102]\n",
      "23:48:59.312899 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7357,  730, 2617, 3025, 4263, 4305...4263, 4305,  117,        7357,  730, 2617,  102])\n",
      "23:48:59.313155 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.313715 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,        1, 1, 1, 1])\n",
      "23:48:59.314379 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.315494 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7357,  730, 2617, 3025, 4263, 430...0, 0, 0, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007507\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 52\n",
      "23:48:59.316866 call        14     def __getitem__(self, idx):\n",
      "23:48:59.316938 line        15         if self.mode == \"test\":\n",
      "23:48:59.316976 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '周丽淇出席某活动，网友：她长得很有辨识度看着舒服，保养得很好'\n",
      "New var:....... text_b = '周丽淇,辨识度'\n",
      "New var:....... label = 0\n",
      "23:48:59.317606 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.317789 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.317940 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.318185 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['周', '丽', '淇', '出', '席', '某', '活', '动', '，', '网...'看', '着', '舒', '服', '，', '保', '养', '得', '很', '好']\n",
      "23:48:59.318898 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '周', '丽', '淇', '出', '席', '某', '活', '动'... '舒', '服', '，', '保', '养', '得', '很', '好', '[SEP]']\n",
      "23:48:59.319069 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "23:48:59.319253 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['周', '丽', '淇', ',', '辨', '识', '度']\n",
      "23:48:59.319619 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '周', '丽', '淇', '出', '席', '某', '活', '动'...EP]', '周', '丽', '淇', ',', '辨', '识', '度', '[SEP]']\n",
      "23:48:59.319889 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 8\n",
      "23:48:59.320080 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1453, 714, 3899, 1139, 2375, 3378, 3833, 1...102, 1453, 714, 3899, 117, 6795, 6399, 2428, 102]\n",
      "23:48:59.320287 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1453,  714, 3899, 1139, 2375, 3378... 714, 3899,  117,        6795, 6399, 2428,  102])\n",
      "23:48:59.320501 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.321045 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.321643 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.322586 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1453,  714, 3899, 1139, 2375, 337... 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007402\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 53\n",
      "23:48:59.324302 call        14     def __getitem__(self, idx):\n",
      "23:48:59.324374 line        15         if self.mode == \"test\":\n",
      "23:48:59.324411 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '维密超模后台化妆照，展示魅力天使容颜和独特的 风采！'\n",
      "New var:....... text_b = '安布罗,时装周,安布罗休,奥尔德里奇,维密'\n",
      "New var:....... label = 0\n",
      "23:48:59.325051 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.325224 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.325375 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.325625 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['维', '密', '超', '模', '后', '台', '化', '妆', '照', '，...'使', '容', '颜', '和', '独', '特', '的', '风', '采', '！']\n",
      "23:48:59.326256 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '维', '密', '超', '模', '后', '台', '化', '妆'... '颜', '和', '独', '特', '的', '风', '采', '！', '[SEP]']\n",
      "23:48:59.326382 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "23:48:59.326498 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['安', '布', '罗', ',', '时', '装', '周', ',', '安', '布...'休', ',', '奥', '尔', '德', '里', '奇', ',', '维', '密']\n",
      "23:48:59.327007 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '维', '密', '超', '模', '后', '台', '化', '妆'... '奥', '尔', '德', '里', '奇', ',', '维', '密', '[SEP]']\n",
      "23:48:59.327459 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 22\n",
      "23:48:59.327783 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5335, 2166, 6631, 3563, 1400, 1378, 1265, ...52, 2209, 2548, 7027, 1936, 117, 5335, 2166, 102]\n",
      "23:48:59.327993 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5335, 2166, 6631, 3563, 1400, 1378...2548, 7027, 1936,  117, 5335, 2166,         102])\n",
      "23:48:59.328439 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.329097 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "23:48:59.330001 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.331295 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5335, 2166, 6631, 3563, 1400, 137...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009514\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 54\n",
      "23:48:59.333853 call        14     def __getitem__(self, idx):\n",
      "23:48:59.333929 line        15         if self.mode == \"test\":\n",
      "23:48:59.333966 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '拿到多位业界大佬投资，她如何在内容和商业教育领域立下壁垒'\n",
      "New var:....... text_b = '毛大庆,吴婷,我有嘉宾,嘉宾派,嘉宾大学,科大讯飞'\n",
      "New var:....... label = 0\n",
      "23:48:59.334549 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.334643 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.334840 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.335118 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['拿', '到', '多', '位', '业', '界', '大', '佬', '投', '资...'商', '业', '教', '育', '领', '域', '立', '下', '壁', '垒']\n",
      "23:48:59.335852 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '拿', '到', '多', '位', '业', '界', '大', '佬'... '教', '育', '领', '域', '立', '下', '壁', '垒', '[SEP]']\n",
      "23:48:59.336083 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "23:48:59.336284 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['毛', '大', '庆', ',', '吴', '婷', ',', '我', '有', '嘉...',', '嘉', '宾', '大', '学', ',', '科', '大', '讯', '飞']\n",
      "23:48:59.336962 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '拿', '到', '多', '位', '业', '界', '大', '佬'... '宾', '大', '学', ',', '科', '大', '讯', '飞', '[SEP]']\n",
      "23:48:59.337242 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 26\n",
      "23:48:59.337447 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2897, 1168, 1914, 855, 689, 4518, 1920, 87...61, 1920, 2110, 117, 4906, 1920, 6380, 7607, 102]\n",
      "23:48:59.337668 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2897, 1168, 1914,  855,  689, 4518... 1920, 2110,  117, 4906, 1920, 6380, 7607,  102])\n",
      "23:48:59.337893 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.338605 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.339275 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.340509 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2897, 1168, 1914,  855,  689, 451...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008936\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 55\n",
      "23:48:59.342820 call        14     def __getitem__(self, idx):\n",
      "23:48:59.342894 line        15         if self.mode == \"test\":\n",
      "23:48:59.342935 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '贾斯汀比伯和薛之谦可比吗？'\n",
      "New var:....... text_b = 'Justin Bieber,Sorry,What Do You Mean ?,I ’ ll Show You,福布斯,薛之谦,贾斯汀比伯,Purpose,侃爷,FEAR'\n",
      "New var:....... label = 0\n",
      "23:48:59.343471 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.343569 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.343647 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.343847 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['贾', '斯', '汀', '比', '伯', '和', '薛', '之', '谦', '可', '比', '吗', '？']\n",
      "23:48:59.344338 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '贾', '斯', '汀', '比', '伯', '和', '薛', '之', '谦', '可', '比', '吗', '？', '[SEP]']\n",
      "23:48:59.344595 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "23:48:59.344789 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['justin', 'bi', '##e', '##ber', ',', 'sorry', '...##rp', '##ose', ',', '侃', '爷', ',', 'fe', '##ar']\n",
      "23:48:59.346169 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '贾', '斯', '汀', '比', '伯', '和', '薛', '之'...#ose', ',', '侃', '爷', ',', 'fe', '##ar', '[SEP]']\n",
      "23:48:59.346588 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 44\n",
      "23:48:59.346805 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6593, 3172, 3722, 3683, 843, 1469, 5955, 7...80, 10936, 117, 887, 4267, 117, 12605, 8458, 102]\n",
      "23:48:59.347049 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  6593,  3172,  3722,  3683,   843...  117,   887,  4267,   117, 12605,  8458,   102])\n",
      "23:48:59.347482 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.348339 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.349073 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.350259 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  6593,  3172,  3722,  3683,   84...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010211\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 56\n",
      "23:48:59.353089 call        14     def __getitem__(self, idx):\n",
      "23:48:59.353220 line        15         if self.mode == \"test\":\n",
      "23:48:59.353266 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《犬夜叉》里的玲是不是人？'\n",
      "New var:....... text_b = '犬夜叉,杀生丸,日暮戈薇,大结局,天生牙,四魂之玉'\n",
      "New var:....... label = 0\n",
      "23:48:59.354217 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.354330 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.354552 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.354909 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '犬', '夜', '叉', '》', '里', '的', '玲', '是', '不', '是', '人', '？']\n",
      "23:48:59.355363 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '犬', '夜', '叉', '》', '里', '的', '玲', '是', '不', '是', '人', '？', '[SEP]']\n",
      "23:48:59.355557 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "23:48:59.355738 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['犬', '夜', '叉', ',', '杀', '生', '丸', ',', '日', '暮...'局', ',', '天', '生', '牙', ',', '四', '魂', '之', '玉']\n",
      "23:48:59.356453 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '犬', '夜', '叉', '》', '里', '的', '玲'... '天', '生', '牙', ',', '四', '魂', '之', '玉', '[SEP]']\n",
      "23:48:59.356632 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 26\n",
      "23:48:59.357373 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 4305, 1915, 1349, 518, 7027, 4638, 43...921, 4495, 4280, 117, 1724, 7789, 722, 4373, 102]\n",
      "23:48:59.357550 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 4305, 1915, 1349,  518, 7027...4280,  117,        1724, 7789,  722, 4373,  102])\n",
      "23:48:59.357803 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.358370 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.358974 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.360075 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 4305, 1915, 1349,  518, 702... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008884\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 57\n",
      "23:48:59.362012 call        14     def __getitem__(self, idx):\n",
      "23:48:59.362104 line        15         if self.mode == \"test\":\n",
      "23:48:59.362254 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '片场偶遇刘涛新剧造型 网友：太有年代感了'\n",
      "New var:....... text_b = '刘涛,片场,新剧'\n",
      "New var:....... label = 0\n",
      "23:48:59.362853 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.362946 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.363020 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.363212 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['片', '场', '偶', '遇', '刘', '涛', '新', '剧', '造', '型', '网', '友', '：', '太', '有', '年', '代', '感', '了']\n",
      "23:48:59.363891 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '片', '场', '偶', '遇', '刘', '涛', '新', '剧'... '友', '：', '太', '有', '年', '代', '感', '了', '[SEP]']\n",
      "23:48:59.364227 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "23:48:59.364484 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['刘', '涛', ',', '片', '场', ',', '新', '剧']\n",
      "23:48:59.364837 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '片', '场', '偶', '遇', '刘', '涛', '新', '剧'... '刘', '涛', ',', '片', '场', ',', '新', '剧', '[SEP]']\n",
      "23:48:59.365029 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 9\n",
      "23:48:59.365214 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4275, 1767, 981, 6878, 1155, 3875, 3173, 1...155, 3875, 117, 4275, 1767, 117, 3173, 1196, 102]\n",
      "23:48:59.365409 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4275, 1767,  981, 6878, 1155, 3875... 117,        4275, 1767,  117, 3173, 1196,  102])\n",
      "23:48:59.365664 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.366140 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0, 0, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "23:48:59.366716 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.367645 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4275, 1767,  981, 6878, 1155, 387...0, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007278\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 58\n",
      "23:48:59.369324 call        14     def __getitem__(self, idx):\n",
      "23:48:59.369399 line        15         if self.mode == \"test\":\n",
      "23:48:59.369439 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '虽然不被大多数人知道，但她是个难得一见的好演员'\n",
      "New var:....... text_b = '万箭穿心,成泰燊,霸王别姬,李宝莉,戏曲,密阳,体验派,颜丙燕,香港大营救'\n",
      "New var:....... label = 0\n",
      "23:48:59.370016 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.370111 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.370188 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.370380 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['虽', '然', '不', '被', '大', '多', '数', '人', '知', '道...'是', '个', '难', '得', '一', '见', '的', '好', '演', '员']\n",
      "23:48:59.370988 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '虽', '然', '不', '被', '大', '多', '数', '人'... '难', '得', '一', '见', '的', '好', '演', '员', '[SEP]']\n",
      "23:48:59.371179 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "23:48:59.371473 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['万', '箭', '穿', '心', ',', '成', '泰', '燊', ',', '霸...',', '颜', '丙', '燕', ',', '香', '港', '大', '营', '救']\n",
      "23:48:59.372312 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '虽', '然', '不', '被', '大', '多', '数', '人'... '丙', '燕', ',', '香', '港', '大', '营', '救', '[SEP]']\n",
      "23:48:59.372513 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 38\n",
      "23:48:59.372708 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6006, 4197, 679, 6158, 1920, 1914, 3144, 7...88, 4242, 117, 7676, 3949, 1920, 5852, 3131, 102]\n",
      "23:48:59.372924 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6006, 4197,  679, 6158, 1920, 1914... 117, 7676, 3949, 1920,        5852, 3131,  102])\n",
      "23:48:59.373139 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.373932 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.374619 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.375757 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6006, 4197,  679, 6158, 1920, 191... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009073\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 59\n",
      "23:48:59.378427 call        14     def __getitem__(self, idx):\n",
      "23:48:59.378500 line        15         if self.mode == \"test\":\n",
      "23:48:59.378538 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '蒋欣生日“两美”送祝福 但与这两位却没互动'\n",
      "New var:....... text_b = '蒋欣,杨紫,欢乐颂,王子文'\n",
      "New var:....... label = 0\n",
      "23:48:59.379085 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.379250 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.379328 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.379505 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['蒋', '欣', '生', '日', '[UNK]', '两', '美', '[UNK]',...'福', '但', '与', '这', '两', '位', '却', '没', '互', '动']\n",
      "23:48:59.380063 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '蒋', '欣', '生', '日', '[UNK]', '两', '美',... '与', '这', '两', '位', '却', '没', '互', '动', '[SEP]']\n",
      "23:48:59.380216 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 22\n",
      "23:48:59.380432 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['蒋', '欣', ',', '杨', '紫', ',', '欢', '乐', '颂', ',', '王', '子', '文']\n",
      "23:48:59.380863 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '蒋', '欣', '生', '日', '[UNK]', '两', '美',... ',', '欢', '乐', '颂', ',', '王', '子', '文', '[SEP]']\n",
      "23:48:59.381053 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "23:48:59.381239 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5882, 3615, 4495, 3189, 100, 697, 5401, 10...117, 3614, 727, 7563, 117, 4374, 2094, 3152, 102]\n",
      "23:48:59.381502 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5882, 3615, 4495, 3189,  100,  697... 3614,  727, 7563,  117, 4374, 2094, 3152,  102])\n",
      "23:48:59.381707 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.382257 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.382954 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.383944 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5882, 3615, 4495, 3189,  100,  69... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007273\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 60\n",
      "23:48:59.385727 call        14     def __getitem__(self, idx):\n",
      "23:48:59.385793 line        15         if self.mode == \"test\":\n",
      "23:48:59.385829 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '演《破冰者》被称“社会我逗姐” 潘之琳：罗晋大师兄成表演明灯'\n",
      "New var:....... text_b = '破冰者,大师兄,真爱的谎言之破冰者,娘要嫁人,谭逗逗,职场是个技术活,罗晋,老房有喜,潘之琳'\n",
      "New var:....... label = 0\n",
      "23:48:59.386325 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.386595 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.386674 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.386987 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['演', '《', '破', '冰', '者', '》', '被', '称', '[UNK]'...'罗', '晋', '大', '师', '兄', '成', '表', '演', '明', '灯']\n",
      "23:48:59.387754 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '演', '《', '破', '冰', '者', '》', '被', '称'... '大', '师', '兄', '成', '表', '演', '明', '灯', '[SEP]']\n",
      "23:48:59.387933 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "23:48:59.388084 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['破', '冰', '者', ',', '大', '师', '兄', ',', '真', '爱...'晋', ',', '老', '房', '有', '喜', ',', '潘', '之', '琳']\n",
      "23:48:59.389036 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '演', '《', '破', '冰', '者', '》', '被', '称'... '老', '房', '有', '喜', ',', '潘', '之', '琳', '[SEP]']\n",
      "23:48:59.389387 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 47\n",
      "23:48:59.389589 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4028, 517, 4788, 1102, 5442, 518, 6158, 49...439, 2791, 3300, 1599, 117, 4050, 722, 4432, 102]\n",
      "23:48:59.389810 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4028,  517, 4788, 1102, 5442,  518...3300,        1599,  117, 4050,  722, 4432,  102])\n",
      "23:48:59.390031 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.390854 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "23:48:59.391784 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.393419 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4028,  517, 4788, 1102, 5442,  51...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010685\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 61\n",
      "23:48:59.396441 call        14     def __getitem__(self, idx):\n",
      "23:48:59.396510 line        15         if self.mode == \"test\":\n",
      "23:48:59.396548 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《非自然死亡》豆瓣评分9.2 国产剧比日剧到底差在哪'\n",
      "New var:....... text_b = '豆瓣,非自然死亡,石原里美'\n",
      "New var:....... label = 0\n",
      "23:48:59.397103 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.397267 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.397345 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.397626 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '非', '自', '然', '死', '亡', '》', '豆', '瓣', '评...'产', '剧', '比', '日', '剧', '到', '底', '差', '在', '哪']\n",
      "23:48:59.398287 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '非', '自', '然', '死', '亡', '》', '豆'... '比', '日', '剧', '到', '底', '差', '在', '哪', '[SEP]']\n",
      "23:48:59.398479 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "23:48:59.398663 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['豆', '瓣', ',', '非', '自', '然', '死', '亡', ',', '石', '原', '里', '美']\n",
      "23:48:59.399091 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '非', '自', '然', '死', '亡', '》', '豆'... '然', '死', '亡', ',', '石', '原', '里', '美', '[SEP]']\n",
      "23:48:59.399283 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "23:48:59.399535 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 7478, 5632, 4197, 3647, 767, 518, 648...197, 3647, 767, 117, 4767, 1333, 7027, 5401, 102]\n",
      "23:48:59.399783 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 7478, 5632, 4197, 3647,  767... 767,  117,        4767, 1333, 7027, 5401,  102])\n",
      "23:48:59.399990 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.400540 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.401189 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.402434 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 7478, 5632, 4197, 3647,  76... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007506\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 62\n",
      "23:48:59.403977 call        14     def __getitem__(self, idx):\n",
      "23:48:59.404044 line        15         if self.mode == \"test\":\n",
      "23:48:59.404082 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '这两个举动证明：黄圣依在教育孩子方面绝对是把好手'\n",
      "New var:....... text_b = '安迪,妈妈是超人,黄圣依'\n",
      "New var:....... label = 0\n",
      "23:48:59.404606 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.404855 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.404927 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.405032 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['这', '两', '个', '举', '动', '证', '明', '：', '黄', '圣...'孩', '子', '方', '面', '绝', '对', '是', '把', '好', '手']\n",
      "23:48:59.405681 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '两', '个', '举', '动', '证', '明', '：'... '方', '面', '绝', '对', '是', '把', '好', '手', '[SEP]']\n",
      "23:48:59.405873 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "23:48:59.406056 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['安', '迪', ',', '妈', '妈', '是', '超', '人', ',', '黄', '圣', '依']\n",
      "23:48:59.406472 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '两', '个', '举', '动', '证', '明', '：'... '妈', '是', '超', '人', ',', '黄', '圣', '依', '[SEP]']\n",
      "23:48:59.406664 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 13\n",
      "23:48:59.406974 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6821, 697, 702, 715, 1220, 6395, 3209, 803...1968, 3221, 6631, 782, 117, 7942, 1760, 898, 102]\n",
      "23:48:59.407175 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6821,  697,  702,  715, 1220, 6395...6631,  782,  117, 7942,        1760,  898,  102])\n",
      "23:48:59.407382 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.407911 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "23:48:59.408544 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.409616 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6821,  697,  702,  715, 1220, 639... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007402\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 63\n",
      "23:48:59.411407 call        14     def __getitem__(self, idx):\n",
      "23:48:59.411474 line        15         if self.mode == \"test\":\n",
      "23:48:59.411510 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '安吉劈木板，小鱼儿一字马，胡可为啥这么爱晒娃？这回答机智了'\n",
      "New var:....... text_b = '安吉,跆拳道,男子汉,胡可,小鱼儿'\n",
      "New var:....... label = 0\n",
      "23:48:59.411987 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "23:48:59.412147 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "23:48:59.412220 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "23:48:59.412398 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['安', '吉', '劈', '木', '板', '，', '小', '鱼', '儿', '一...'爱', '晒', '娃', '？', '这', '回', '答', '机', '智', '了']\n",
      "23:48:59.413102 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '安', '吉', '劈', '木', '板', '，', '小', '鱼'... '娃', '？', '这', '回', '答', '机', '智', '了', '[SEP]']\n",
      "23:48:59.413337 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "23:48:59.413517 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['安', '吉', ',', '跆', '拳', '道', ',', '男', '子', '汉', ',', '胡', '可', ',', '小', '鱼', '儿']\n",
      "23:48:59.414014 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '安', '吉', '劈', '木', '板', '，', '小', '鱼'... '汉', ',', '胡', '可', ',', '小', '鱼', '儿', '[SEP]']\n",
      "23:48:59.414237 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "23:48:59.414405 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2128, 1395, 1207, 3312, 3352, 8024, 2207, ...727, 117, 5529, 1377, 117, 2207, 7824, 1036, 102]\n",
      "23:48:59.414610 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2128, 1395, 1207, 3312, 3352, 8024...5529, 1377,  117, 2207, 7824, 1036,         102])\n",
      "23:48:59.414818 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "23:48:59.415564 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "23:48:59.416174 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "23:48:59.417407 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2128, 1395, 1207, 3312, 3352, 802...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008181\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([64, 82]) \n",
      "tensor([[ 101, 1367, 1213,  ...,    0,    0,    0],\n",
      "        [ 101, 1259, 6564,  ...,    0,    0,    0],\n",
      "        [ 101, 2031,  727,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  517, 7478,  ...,    0,    0,    0],\n",
      "        [ 101, 6821,  697,  ...,    0,    0,    0],\n",
      "        [ 101, 2128, 1395,  ...,    0,    0,    0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([64, 82])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([64, 82])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([64])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "NUM_LABELS = 10\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            #logits = outputs[0]\n",
    "            _, pred = torch.max(outputs.data, -1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 0\n",
      "00:24:48.144473 call        14     def __getitem__(self, idx):\n",
      "00:24:48.144633 line        15         if self.mode == \"test\":\n",
      "00:24:48.144679 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '古力娜扎再次成为焦点，这一身招摇大方，掳获了网友们的心'\n",
      "New var:....... text_b = '古力娜扎,粉丝'\n",
      "New var:....... label = 0\n",
      "00:24:48.145487 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.145586 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.145819 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.146171 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['古', '力', '娜', '扎', '再', '次', '成', '为', '焦', '点...'方', '，', '掳', '获', '了', '网', '友', '们', '的', '心']\n",
      "00:24:48.146873 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '古', '力', '娜', '扎', '再', '次', '成', '为'... '掳', '获', '了', '网', '友', '们', '的', '心', '[SEP]']\n",
      "00:24:48.147135 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:24:48.147322 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['古', '力', '娜', '扎', ',', '粉', '丝']\n",
      "00:24:48.147659 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '古', '力', '娜', '扎', '再', '次', '成', '为'...EP]', '古', '力', '娜', '扎', ',', '粉', '丝', '[SEP]']\n",
      "00:24:48.147852 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 8\n",
      "00:24:48.148040 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1367, 1213, 2025, 2799, 1086, 3613, 2768, ...102, 1367, 1213, 2025, 2799, 117, 5106, 692, 102]\n",
      "00:24:48.148254 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1367, 1213, 2025, 2799, 1086, 3613...1213, 2025, 2799,  117, 5106,  692,         102])\n",
      "00:24:48.148468 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.149317 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0,        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.149829 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.150756 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1367, 1213, 2025, 2799, 1086, 361... 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008737\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 1\n",
      "00:24:48.153218 call        14     def __getitem__(self, idx):\n",
      "00:24:48.153302 line        15         if self.mode == \"test\":\n",
      "00:24:48.153341 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '包贝尔带娇妻外出就餐被拍，大家把注意力放在了第3张！'\n",
      "New var:....... text_b = '娇妻,娇妻外出就餐,包贝尔'\n",
      "New var:....... label = 0\n",
      "00:24:48.153929 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.154106 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.154256 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.154500 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['包', '贝', '尔', '带', '娇', '妻', '外', '出', '就', '餐...'注', '意', '力', '放', '在', '了', '第', '3', '张', '！']\n",
      "00:24:48.155173 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '包', '贝', '尔', '带', '娇', '妻', '外', '出'... '力', '放', '在', '了', '第', '3', '张', '！', '[SEP]']\n",
      "00:24:48.155362 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:24:48.155545 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['娇', '妻', ',', '娇', '妻', '外', '出', '就', '餐', ',', '包', '贝', '尔']\n",
      "00:24:48.155973 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '包', '贝', '尔', '带', '娇', '妻', '外', '出'... '外', '出', '就', '餐', ',', '包', '贝', '尔', '[SEP]']\n",
      "00:24:48.156230 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "00:24:48.156419 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1259, 6564, 2209, 2372, 2019, 1988, 1912, ...12, 1139, 2218, 7623, 117, 1259, 6564, 2209, 102]\n",
      "00:24:48.156623 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1259, 6564, 2209, 2372, 2019, 1988...2218,        7623,  117, 1259, 6564, 2209,  102])\n",
      "00:24:48.156834 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.157394 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.157992 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.159130 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1259, 6564, 2209, 2372, 2019, 198... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007699\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 2\n",
      "00:24:48.160949 call        14     def __getitem__(self, idx):\n",
      "00:24:48.161020 line        15         if self.mode == \"test\":\n",
      "00:24:48.161058 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '娱乐圈娶了豪门的5位男星，事业开挂，最后一位想离婚门都没有'\n",
      "New var:....... text_b = '豪门,迟重瑞,周立波,吕良伟,石贞善'\n",
      "New var:....... label = 0\n",
      "00:24:48.161644 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.161812 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.161963 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.162202 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['娱', '乐', '圈', '娶', '了', '豪', '门', '的', '5', '位...'后', '一', '位', '想', '离', '婚', '门', '都', '没', '有']\n",
      "00:24:48.162927 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '娶', '了', '豪', '门', '的'... '位', '想', '离', '婚', '门', '都', '没', '有', '[SEP]']\n",
      "00:24:48.163124 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:24:48.163310 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['豪', '门', ',', '迟', '重', '瑞', ',', '周', '立', '波', ',', '吕', '良', '伟', ',', '石', '贞', '善']\n",
      "00:24:48.163836 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '娶', '了', '豪', '门', '的'... ',', '吕', '良', '伟', ',', '石', '贞', '善', '[SEP]']\n",
      "00:24:48.164213 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:24:48.164374 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2031, 727, 1750, 2034, 749, 6498, 7305, 46...117, 1406, 5679, 836, 117, 4767, 6565, 1587, 102]\n",
      "00:24:48.164535 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2031,  727, 1750, 2034,  749, 6498...5679,  836,  117, 4767, 6565,        1587,  102])\n",
      "00:24:48.164696 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.165697 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:24:48.166777 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.168299 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2031,  727, 1750, 2034,  749, 649...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010149\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 3\n",
      "00:24:48.171138 call        14     def __getitem__(self, idx):\n",
      "00:24:48.171217 line        15         if self.mode == \"test\":\n",
      "00:24:48.171254 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '陈学冬参加节目被爆童年照，果然胖子都是潜力股啊！'\n",
      "New var:....... text_b = '薛之谦,陈学冬,谭维维,跨界歌王'\n",
      "New var:....... label = 0\n",
      "00:24:48.171842 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.171938 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.172086 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.172326 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['陈', '学', '冬', '参', '加', '节', '目', '被', '爆', '童...'然', '胖', '子', '都', '是', '潜', '力', '股', '啊', '！']\n",
      "00:24:48.172960 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '学', '冬', '参', '加', '节', '目', '被'... '子', '都', '是', '潜', '力', '股', '啊', '！', '[SEP]']\n",
      "00:24:48.173146 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:24:48.173385 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['薛', '之', '谦', ',', '陈', '学', '冬', ',', '谭', '维', '维', ',', '跨', '界', '歌', '王']\n",
      "00:24:48.173907 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '学', '冬', '参', '加', '节', '目', '被'... '谭', '维', '维', ',', '跨', '界', '歌', '王', '[SEP]']\n",
      "00:24:48.174112 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 17\n",
      "00:24:48.174307 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7357, 2110, 1100, 1346, 1217, 5688, 4680, ...78, 5335, 5335, 117, 6659, 4518, 3625, 4374, 102]\n",
      "00:24:48.174615 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7357, 2110, 1100, 1346, 1217, 5688...       5335,  117, 6659, 4518, 3625, 4374,  102])\n",
      "00:24:48.174869 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.175532 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.177326 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.178766 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7357, 2110, 1100, 1346, 1217, 568... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009867\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 4\n",
      "00:24:48.181043 call        14     def __getitem__(self, idx):\n",
      "00:24:48.181123 line        15         if self.mode == \"test\":\n",
      "00:24:48.181161 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '福斯进军华语迷你剧开山之作定檔，“东方华尔街”吴镇宇张孝全'\n",
      "New var:....... text_b = '金融圈,吴镇宇,王牌逗王牌,桃姐,张孝全,东方华尔街,拆弹专家,女朋友·男朋友,刘德华'\n",
      "New var:....... label = 0\n",
      "00:24:48.181754 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.181855 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.182052 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.182308 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['福', '斯', '进', '军', '华', '语', '迷', '你', '剧', '开... '尔', '街', '[UNK]', '吴', '镇', '宇', '张', '孝', '全']\n",
      "00:24:48.183233 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '福', '斯', '进', '军', '华', '语', '迷', '你'..., '[UNK]', '吴', '镇', '宇', '张', '孝', '全', '[SEP]']\n",
      "00:24:48.183486 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:24:48.183698 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['金', '融', '圈', ',', '吴', '镇', '宇', ',', '王', '牌...'朋', '友', '·', '男', '朋', '友', ',', '刘', '德', '华']\n",
      "00:24:48.185038 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '福', '斯', '进', '军', '华', '语', '迷', '你'... '·', '男', '朋', '友', ',', '刘', '德', '华', '[SEP]']\n",
      "00:24:48.185670 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 44\n",
      "00:24:48.186005 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4886, 3172, 6822, 1092, 1290, 6427, 6837, ...85, 4511, 3301, 1351, 117, 1155, 2548, 1290, 102]\n",
      "00:24:48.186280 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4886, 3172, 6822, 1092, 1290, 6427...3301, 1351,  117, 1155,        2548, 1290,  102])\n",
      "00:24:48.186609 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.187545 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:24:48.188543 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.190278 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4886, 3172, 6822, 1092, 1290, 642...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.012126\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 5\n",
      "00:24:48.193200 call        14     def __getitem__(self, idx):\n",
      "00:24:48.193273 line        15         if self.mode == \"test\":\n",
      "00:24:48.193311 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '马伊琍就是马伊琍，蕾丝也能穿出女王范，文章还是乖乖做小男人吧'\n",
      "New var:....... text_b = '乖乖做小男人,小男人,马伊琍,蕾丝,穿出女王范'\n",
      "New var:....... label = 0\n",
      "00:24:48.193893 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.194090 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.194234 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.194409 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['马', '伊', '琍', '就', '是', '马', '伊', '琍', '，', '蕾...'章', '还', '是', '乖', '乖', '做', '小', '男', '人', '吧']\n",
      "00:24:48.195140 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '马', '伊', '琍', '就', '是', '马', '伊', '琍'... '是', '乖', '乖', '做', '小', '男', '人', '吧', '[SEP]']\n",
      "00:24:48.195330 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:48.195514 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['乖', '乖', '做', '小', '男', '人', ',', '小', '男', '人...'琍', ',', '蕾', '丝', ',', '穿', '出', '女', '王', '范']\n",
      "00:24:48.196111 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '马', '伊', '琍', '就', '是', '马', '伊', '琍'... '蕾', '丝', ',', '穿', '出', '女', '王', '范', '[SEP]']\n",
      "00:24:48.196445 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "00:24:48.196657 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7716, 823, 4419, 2218, 3221, 7716, 823, 44...945, 692, 117, 4959, 1139, 1957, 4374, 5745, 102]\n",
      "00:24:48.196872 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7716,  823, 4419, 2218, 3221, 7716...  692,  117, 4959, 1139, 1957, 4374, 5745,  102])\n",
      "00:24:48.197056 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.197680 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.198304 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.199649 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7716,  823, 4419, 2218, 3221, 771...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008813\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 6\n",
      "00:24:48.202060 call        14     def __getitem__(self, idx):\n",
      "00:24:48.202143 line        15         if self.mode == \"test\":\n",
      "00:24:48.202184 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '这就是街舞 收官：石头秀肌肉，抖动胸肌！易烊千玺坐地，尖叫！'\n",
      "New var:....... text_b = '街舞,胸肌,烊千'\n",
      "New var:....... label = 0\n",
      "00:24:48.202774 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.202869 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.203055 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.203313 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['这', '就', '是', '街', '舞', '收', '官', '：', '石', '头...'易', '烊', '千', '玺', '坐', '地', '，', '尖', '叫', '！']\n",
      "00:24:48.204047 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '就', '是', '街', '舞', '收', '官', '：'... '千', '玺', '坐', '地', '，', '尖', '叫', '！', '[SEP]']\n",
      "00:24:48.204314 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:24:48.204504 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['街', '舞', ',', '胸', '肌', ',', '烊', '千']\n",
      "00:24:48.204871 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '就', '是', '街', '舞', '收', '官', '：'... '街', '舞', ',', '胸', '肌', ',', '烊', '千', '[SEP]']\n",
      "00:24:48.205068 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 9\n",
      "00:24:48.205259 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6821, 2218, 3221, 6125, 5659, 3119, 2135, ...125, 5659, 117, 5541, 5491, 117, 4165, 1283, 102]\n",
      "00:24:48.205465 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6821, 2218, 3221, 6125, 5659, 3119... 117, 5541, 5491,         117, 4165, 1283,  102])\n",
      "00:24:48.205612 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.206007 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.206851 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.207998 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6821, 2218, 3221, 6125, 5659, 311... 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008355\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 7\n",
      "00:24:48.210452 call        14     def __getitem__(self, idx):\n",
      "00:24:48.210532 line        15         if self.mode == \"test\":\n",
      "00:24:48.210570 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张歆艺现身上海出席红毯活动 网友：好V好比刚路过的飞机场！'\n",
      "New var:....... text_b = '红毯活动,红毯,张歆艺,东方IC,东方ic'\n",
      "New var:....... label = 0\n",
      "00:24:48.211155 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.211249 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.211399 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.211670 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '歆', '艺', '现', '身', '上', '海', '出', '席', '红...'好', '比', '刚', '路', '过', '的', '飞', '机', '场', '！']\n",
      "00:24:48.212376 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '歆', '艺', '现', '身', '上', '海', '出'... '刚', '路', '过', '的', '飞', '机', '场', '！', '[SEP]']\n",
      "00:24:48.212564 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:24:48.212746 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['红', '毯', '活', '动', ',', '红', '毯', ',', '张', '歆', '艺', ',', '东', '方', 'ic', ',', '东', '方', 'ic']\n",
      "00:24:48.213288 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '歆', '艺', '现', '身', '上', '海', '出'...,', '东', '方', 'ic', ',', '东', '方', 'ic', '[SEP]']\n",
      "00:24:48.213483 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:24:48.213671 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 3622, 5686, 4385, 6716, 677, 3862, 1... 117, 691, 3175, 8577, 117, 691, 3175, 8577, 102]\n",
      "00:24:48.213986 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476, 3622, 5686, 4385, 6716,  677...3175, 8577,  117,  691, 3175,        8577,  102])\n",
      "00:24:48.214199 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.214797 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:24:48.215483 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.216664 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2476, 3622, 5686, 4385, 6716,  67...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008367\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 8\n",
      "00:24:48.218846 call        14     def __getitem__(self, idx):\n",
      "00:24:48.218914 line        15         if self.mode == \"test\":\n",
      "00:24:48.218951 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '李小璐数月首曝光！女儿做爱心餐庆祝母亲节，贾乃亮却不在现场？'\n",
      "New var:....... text_b = '甜馨,母亲节,李小璐,粉丝,贾乃亮'\n",
      "New var:....... label = 0\n",
      "00:24:48.219465 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.219558 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.219633 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.219913 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['李', '小', '璐', '数', '月', '首', '曝', '光', '！', '女...'，', '贾', '乃', '亮', '却', '不', '在', '现', '场', '？']\n",
      "00:24:48.220742 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '小', '璐', '数', '月', '首', '曝', '光'... '乃', '亮', '却', '不', '在', '现', '场', '？', '[SEP]']\n",
      "00:24:48.221061 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:48.221252 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['甜', '馨', ',', '母', '亲', '节', ',', '李', '小', '璐', ',', '粉', '丝', ',', '贾', '乃', '亮']\n",
      "00:24:48.221758 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '小', '璐', '数', '月', '首', '曝', '光'... '璐', ',', '粉', '丝', ',', '贾', '乃', '亮', '[SEP]']\n",
      "00:24:48.222080 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:24:48.222313 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3330, 2207, 4466, 3144, 3299, 7674, 3284, ..., 4466, 117, 5106, 692, 117, 6593, 718, 778, 102]\n",
      "00:24:48.222537 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3330, 2207, 4466, 3144, 3299, 7674...5106,  692,  117, 6593,  718,         778,  102])\n",
      "00:24:48.222823 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.223567 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:24:48.224181 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.225307 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3330, 2207, 4466, 3144, 3299, 767...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008441\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 9\n",
      "00:24:48.227316 call        14     def __getitem__(self, idx):\n",
      "00:24:48.227385 line        15         if self.mode == \"test\":\n",
      "00:24:48.227423 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '你觉得独孤曼陀丑？那是没看过她演的虞姬！'\n",
      "New var:....... text_b = '安以轩,宇文护,虞姬,李依晓,陇西郡,独孤天下,楚汉传奇,新洛神'\n",
      "New var:....... label = 0\n",
      "00:24:48.227980 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.228072 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.228145 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.228324 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['你', '觉', '得', '独', '孤', '曼', '陀', '丑', '？', '那', '是', '没', '看', '过', '她', '演', '的', '虞', '姬', '！']\n",
      "00:24:48.228987 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '你', '觉', '得', '独', '孤', '曼', '陀', '丑'... '看', '过', '她', '演', '的', '虞', '姬', '！', '[SEP]']\n",
      "00:24:48.229172 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 22\n",
      "00:24:48.229352 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['安', '以', '轩', ',', '宇', '文', '护', ',', '虞', '姬...'下', ',', '楚', '汉', '传', '奇', ',', '新', '洛', '神']\n",
      "00:24:48.230105 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '你', '觉', '得', '独', '孤', '曼', '陀', '丑'... '楚', '汉', '传', '奇', ',', '新', '洛', '神', '[SEP]']\n",
      "00:24:48.230299 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 33\n",
      "00:24:48.230494 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 872, 6230, 2533, 4324, 2109, 3294, 7351, 6...504, 3727, 837, 1936, 117, 3173, 3821, 4868, 102]\n",
      "00:24:48.230813 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  872, 6230, 2533, 4324, 2109, 3294...        837, 1936,  117, 3173, 3821, 4868,  102])\n",
      "00:24:48.231027 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.231751 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.232447 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.233647 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  872, 6230, 2533, 4324, 2109, 329...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008364\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 10\n",
      "00:24:48.235784 call        14     def __getitem__(self, idx):\n",
      "00:24:48.235859 line        15         if self.mode == \"test\":\n",
      "00:24:48.235896 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = 'Tik杰西达邦和Chakrit联手演绎《傲世双雄》，这部泰剧不狗血哦'\n",
      "New var:....... text_b = 'Falconer,Tik,杰西达邦,雷人,傲世双雄,Mat,Chakrit'\n",
      "New var:....... label = 0\n",
      "00:24:48.236447 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.236538 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.236610 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.236843 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['ti', '##k', '杰', '西', '达', '邦', '和', 'ch', '##...'》', '，', '这', '部', '泰', '剧', '不', '狗', '血', '哦']\n",
      "00:24:48.237591 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'ti', '##k', '杰', '西', '达', '邦', '和', ... '这', '部', '泰', '剧', '不', '狗', '血', '哦', '[SEP]']\n",
      "00:24:48.237782 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:48.238055 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['fa', '##lc', '##one', '##r', ',', 'ti', '##k',...', 'ma', '##t', ',', 'ch', '##ak', '##ri', '##t']\n",
      "00:24:48.238748 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'ti', '##k', '杰', '西', '达', '邦', '和', ...'##t', ',', 'ch', '##ak', '##ri', '##t', '[SEP]']\n",
      "00:24:48.238994 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 29\n",
      "00:24:48.239191 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 9654, 8197, 3345, 6205, 6809, 6930, 1469, ...17, 9622, 8165, 117, 9537, 9896, 8641, 8165, 102]\n",
      "00:24:48.239409 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  9654,  8197,  3345,  6205,  6809...  117,  9537,  9896,  8641,  8165,          102])\n",
      "00:24:48.239629 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.240377 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.241054 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.242235 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  9654,  8197,  3345,  6205,  680... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008851\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 11\n",
      "00:24:48.244663 call        14     def __getitem__(self, idx):\n",
      "00:24:48.244733 line        15         if self.mode == \"test\":\n",
      "00:24:48.244773 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '小男孩模仿薛之谦唱歌，薛之谦一脸错愕，台下尖叫！声音简直一样'\n",
      "New var:....... text_b = '薛之谦'\n",
      "New var:....... label = 0\n",
      "00:24:48.245335 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.245448 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.245532 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.246169 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['小', '男', '孩', '模', '仿', '薛', '之', '谦', '唱', '歌...'下', '尖', '叫', '！', '声', '音', '简', '直', '一', '样']\n",
      "00:24:48.247158 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '小', '男', '孩', '模', '仿', '薛', '之', '谦'... '叫', '！', '声', '音', '简', '直', '一', '样', '[SEP]']\n",
      "00:24:48.247464 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:48.247684 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['薛', '之', '谦']\n",
      "00:24:48.247957 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '小', '男', '孩', '模', '仿', '薛', '之', '谦'..., '直', '一', '样', '[SEP]', '薛', '之', '谦', '[SEP]']\n",
      "00:24:48.248150 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 4\n",
      "00:24:48.248402 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2207, 4511, 2111, 3563, 820, 5955, 722, 64...5042, 4684, 671, 3416, 102, 5955, 722, 6472, 102]\n",
      "00:24:48.248640 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2207, 4511, 2111, 3563,  820, 5955... 4684,  671, 3416,  102, 5955,  722, 6472,  102])\n",
      "00:24:48.248930 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.249505 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0,        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1])\n",
      "00:24:48.250109 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.251167 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2207, 4511, 2111, 3563,  820, 595... 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008288\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 12\n",
      "00:24:48.252981 call        14     def __getitem__(self, idx):\n",
      "00:24:48.253051 line        15         if self.mode == \"test\":\n",
      "00:24:48.253088 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '上官燕被秋若枫毁了清白，可是她觉得伤害她最深的是她父亲上官云'\n",
      "New var:....... text_b = '上官云,上官燕'\n",
      "New var:....... label = 0\n",
      "00:24:48.253658 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.253844 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.253923 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.254106 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['上', '官', '燕', '被', '秋', '若', '枫', '毁', '了', '清...'最', '深', '的', '是', '她', '父', '亲', '上', '官', '云']\n",
      "00:24:48.254881 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '上', '官', '燕', '被', '秋', '若', '枫', '毁'... '的', '是', '她', '父', '亲', '上', '官', '云', '[SEP]']\n",
      "00:24:48.255068 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:48.255250 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['上', '官', '云', ',', '上', '官', '燕']\n",
      "00:24:48.255580 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '上', '官', '燕', '被', '秋', '若', '枫', '毁'...EP]', '上', '官', '云', ',', '上', '官', '燕', '[SEP]']\n",
      "00:24:48.255770 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 8\n",
      "00:24:48.255967 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 677, 2135, 4242, 6158, 4904, 5735, 3367, 3..., 102, 677, 2135, 756, 117, 677, 2135, 4242, 102]\n",
      "00:24:48.256168 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  677, 2135, 4242, 6158, 4904, 5735...2135,  756,  117,         677, 2135, 4242,  102])\n",
      "00:24:48.256420 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.257009 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.257628 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.258702 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  677, 2135, 4242, 6158, 4904, 573... 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007671\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 13\n",
      "00:24:48.260682 call        14     def __getitem__(self, idx):\n",
      "00:24:48.260752 line        15         if self.mode == \"test\":\n",
      "00:24:48.260858 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '会唠嗑儿的直男赢了！说你娶到刘亦菲我也信……'\n",
      "New var:....... text_b = '范冰冰,刘亦菲'\n",
      "New var:....... label = 0\n",
      "00:24:48.261404 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.261498 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.261573 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.261803 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['会', '唠', '嗑', '儿', '的', '直', '男', '赢', '了', '！..., '刘', '亦', '菲', '我', '也', '信', '[UNK]', '[UNK]']\n",
      "00:24:48.262389 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '会', '唠', '嗑', '儿', '的', '直', '男', '赢'...', '菲', '我', '也', '信', '[UNK]', '[UNK]', '[SEP]']\n",
      "00:24:48.262576 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:24:48.262756 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['范', '冰', '冰', ',', '刘', '亦', '菲']\n",
      "00:24:48.263083 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '会', '唠', '嗑', '儿', '的', '直', '男', '赢'...EP]', '范', '冰', '冰', ',', '刘', '亦', '菲', '[SEP]']\n",
      "00:24:48.263337 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 8\n",
      "00:24:48.263524 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 833, 1541, 1622, 1036, 4638, 4684, 4511, 6...102, 5745, 1102, 1102, 117, 1155, 771, 5838, 102]\n",
      "00:24:48.263721 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  833, 1541, 1622, 1036, 4638, 4684... 5745, 1102, 1102,  117, 1155,  771, 5838,  102])\n",
      "00:24:48.263975 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.264505 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0, 0, 0,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.264978 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.265882 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  833, 1541, 1622, 1036, 4638, 468...0, 0,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006787\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 14\n",
      "00:24:48.267496 call        14     def __getitem__(self, idx):\n",
      "00:24:48.267562 line        15         if self.mode == \"test\":\n",
      "00:24:48.267597 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '漫威新电影复仇者联盟3无限战争豪夺票房王座'\n",
      "New var:....... text_b = '复仇者联盟3 ：无限之战,星际异攻队,漫威电影宇宙,复仇者联盟2 ：奥创纪元,IMAX,漫威,电影,漫威影业'\n",
      "New var:....... label = 0\n",
      "00:24:48.268071 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.268303 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.268387 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.268577 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['漫', '威', '新', '电', '影', '复', '仇', '者', '联', '盟...'无', '限', '战', '争', '豪', '夺', '票', '房', '王', '座']\n",
      "00:24:48.269155 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '漫', '威', '新', '电', '影', '复', '仇', '者'... '战', '争', '豪', '夺', '票', '房', '王', '座', '[SEP]']\n",
      "00:24:48.269344 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:24:48.269527 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['复', '仇', '者', '联', '盟', '3', '：', '无', '限', '之...'漫', '威', ',', '电', '影', ',', '漫', '威', '影', '业']\n",
      "00:24:48.270597 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '漫', '威', '新', '电', '影', '复', '仇', '者'... ',', '电', '影', ',', '漫', '威', '影', '业', '[SEP]']\n",
      "00:24:48.270927 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 50\n",
      "00:24:48.271150 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4035, 2014, 3173, 4510, 2512, 1908, 790, 5...117, 4510, 2512, 117, 4035, 2014, 2512, 689, 102]\n",
      "00:24:48.271377 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  4035,  2014,  3173,  4510,  2512...  117,  4035,  2014,         2512,   689,   102])\n",
      "00:24:48.271600 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.272393 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:24:48.273242 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.274513 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  4035,  2014,  3173,  4510,  251...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009721\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 15\n",
      "00:24:48.277245 call        14     def __getitem__(self, idx):\n",
      "00:24:48.277312 line        15         if self.mode == \"test\":\n",
      "00:24:48.277349 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《上海女子图鉴》女主上海十年男友比戚薇少，每个大有来头超有钱'\n",
      "New var:....... text_b = '女强人,刘孜,女主角,北京女子图鉴,罗海燕,戚薇,上海女子图鉴'\n",
      "New var:....... label = 0\n",
      "00:24:48.277816 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.277905 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.277975 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.278270 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '上', '海', '女', '子', '图', '鉴', '》', '女', '主...'，', '每', '个', '大', '有', '来', '头', '超', '有', '钱']\n",
      "00:24:48.278990 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '上', '海', '女', '子', '图', '鉴', '》'... '个', '大', '有', '来', '头', '超', '有', '钱', '[SEP]']\n",
      "00:24:48.279177 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:48.279358 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['女', '强', '人', ',', '刘', '孜', ',', '女', '主', '角...',', '戚', '薇', ',', '上', '海', '女', '子', '图', '鉴']\n",
      "00:24:48.280118 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '上', '海', '女', '子', '图', '鉴', '》'... '薇', ',', '上', '海', '女', '子', '图', '鉴', '[SEP]']\n",
      "00:24:48.280425 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 32\n",
      "00:24:48.280620 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 677, 3862, 1957, 2094, 1745, 7063, 51...948, 117, 677, 3862, 1957, 2094, 1745, 7063, 102]\n",
      "00:24:48.280831 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517,  677, 3862, 1957, 2094, 1745... 677, 3862, 1957,        2094, 1745, 7063,  102])\n",
      "00:24:48.281044 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.281776 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.282446 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.283792 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517,  677, 3862, 1957, 2094, 174... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008955\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 16\n",
      "00:24:48.286229 call        14     def __getitem__(self, idx):\n",
      "00:24:48.286297 line        15         if self.mode == \"test\":\n",
      "00:24:48.286334 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '跟王菲学的“新潮流”? 谢霆锋穿着“鸳鸯袜”喊话网友自寻亮点！'\n",
      "New var:....... text_b = '新潮流,王菲,张柏芝,谢霆锋,牛仔裤'\n",
      "New var:....... label = 0\n",
      "00:24:48.286873 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.286964 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.287042 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.287275 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['跟', '王', '菲', '学', '的', '[UNK]', '新', '潮', '流'...K]', '喊', '话', '网', '友', '自', '寻', '亮', '点', '！']\n",
      "00:24:48.288009 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '跟', '王', '菲', '学', '的', '[UNK]', '新',... '话', '网', '友', '自', '寻', '亮', '点', '！', '[SEP]']\n",
      "00:24:48.288197 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:48.288475 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['新', '潮', '流', ',', '王', '菲', ',', '张', '柏', '芝', ',', '谢', '霆', '锋', ',', '牛', '仔', '裤']\n",
      "00:24:48.289033 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '跟', '王', '菲', '学', '的', '[UNK]', '新',... ',', '谢', '霆', '锋', ',', '牛', '仔', '裤', '[SEP]']\n",
      "00:24:48.289295 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:24:48.289531 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6656, 4374, 5838, 2110, 4638, 100, 3173, 4...117, 6468, 7447, 7226, 117, 4281, 798, 6175, 102]\n",
      "00:24:48.289676 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6656, 4374, 5838, 2110, 4638,  100...7447, 7226,  117, 4281,         798, 6175,  102])\n",
      "00:24:48.289824 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.290302 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:24:48.290869 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.292022 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6656, 4374, 5838, 2110, 4638,  10...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008097\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 17\n",
      "00:24:48.294354 call        14     def __getitem__(self, idx):\n",
      "00:24:48.294422 line        15         if self.mode == \"test\":\n",
      "00:24:48.294458 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '4岁成名，比张一山杨紫还红，被父母败光千万身家，如今长成这样'\n",
      "New var:....... text_b = '演艺圈,笑林小子,朱延平,新乌龙院,哪吒大战美猴王,释小龙,郝劭文,张一山'\n",
      "New var:....... label = 0\n",
      "00:24:48.294949 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.295040 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.295112 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.295291 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['4', '岁', '成', '名', '，', '比', '张', '一', '山', '杨...'万', '身', '家', '，', '如', '今', '长', '成', '这', '样']\n",
      "00:24:48.296094 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '4', '岁', '成', '名', '，', '比', '张', '一'... '家', '，', '如', '今', '长', '成', '这', '样', '[SEP]']\n",
      "00:24:48.296324 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:48.296508 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['演', '艺', '圈', ',', '笑', '林', '小', '子', ',', '朱...'小', '龙', ',', '郝', '劭', '文', ',', '张', '一', '山']\n",
      "00:24:48.297359 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '4', '岁', '成', '名', '，', '比', '张', '一'... ',', '郝', '劭', '文', ',', '张', '一', '山', '[SEP]']\n",
      "00:24:48.297561 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 38\n",
      "00:24:48.297822 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 125, 2259, 2768, 1399, 8024, 3683, 2476, 6...117, 6950, 1224, 3152, 117, 2476, 671, 2255, 102]\n",
      "00:24:48.298040 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  125, 2259, 2768, 1399, 8024, 3683... 6950, 1224, 3152,  117, 2476,  671, 2255,  102])\n",
      "00:24:48.298258 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.299022 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.300085 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.301378 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  125, 2259, 2768, 1399, 8024, 368... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009569\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 18\n",
      "00:24:48.303953 call        14     def __getitem__(self, idx):\n",
      "00:24:48.304025 line        15         if self.mode == \"test\":\n",
      "00:24:48.304063 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张杰外网发文回应《101》风波：解释无果那就沉默'\n",
      "New var:....... text_b = '张杰,粉丝,创造101'\n",
      "New var:....... label = 0\n",
      "00:24:48.304613 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.304705 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.304779 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.305093 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '杰', '外', '网', '发', '文', '回', '应', '《', '1...'波', '：', '解', '释', '无', '果', '那', '就', '沉', '默']\n",
      "00:24:48.305730 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '杰', '外', '网', '发', '文', '回', '应'... '解', '释', '无', '果', '那', '就', '沉', '默', '[SEP]']\n",
      "00:24:48.305929 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:24:48.306113 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['张', '杰', ',', '粉', '丝', ',', '创', '造', '101']\n",
      "00:24:48.306492 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '杰', '外', '网', '发', '文', '回', '应'...杰', ',', '粉', '丝', ',', '创', '造', '101', '[SEP]']\n",
      "00:24:48.306686 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 10\n",
      "00:24:48.306873 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 3345, 1912, 5381, 1355, 3152, 1726, ...3345, 117, 5106, 692, 117, 1158, 6863, 8359, 102]\n",
      "00:24:48.307139 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476, 3345, 1912, 5381, 1355, 3152...  117, 5106,  692,  117, 1158, 6863, 8359,  102])\n",
      "00:24:48.307449 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.307971 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.308479 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.309378 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2476, 3345, 1912, 5381, 1355, 315...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007022\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 19\n",
      "00:24:48.311003 call        14     def __getitem__(self, idx):\n",
      "00:24:48.311070 line        15         if self.mode == \"test\":\n",
      "00:24:48.311106 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '娱乐圈又一气质女神出柜了？她还是柳岩韩雪的闺蜜！'\n",
      "New var:....... text_b = '吴卓林,祝你幸福,毛林林,兰陵王,柳岩,韩雪'\n",
      "New var:....... label = 0\n",
      "00:24:48.311626 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.311718 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.311790 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.312026 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['娱', '乐', '圈', '又', '一', '气', '质', '女', '神', '出...'还', '是', '柳', '岩', '韩', '雪', '的', '闺', '蜜', '！']\n",
      "00:24:48.312650 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '又', '一', '气', '质', '女'... '柳', '岩', '韩', '雪', '的', '闺', '蜜', '！', '[SEP]']\n",
      "00:24:48.312926 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:24:48.313130 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['吴', '卓', '林', ',', '祝', '你', '幸', '福', ',', '毛...',', '兰', '陵', '王', ',', '柳', '岩', ',', '韩', '雪']\n",
      "00:24:48.313747 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '又', '一', '气', '质', '女'... '陵', '王', ',', '柳', '岩', ',', '韩', '雪', '[SEP]']\n",
      "00:24:48.313950 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:24:48.314183 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2031, 727, 1750, 1348, 671, 3698, 6574, 19...377, 4374, 117, 3394, 2272, 117, 7506, 7434, 102]\n",
      "00:24:48.314360 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2031,  727, 1750, 1348,  671, 3698... 117, 3394, 2272,  117, 7506, 7434,         102])\n",
      "00:24:48.314551 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.315223 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:24:48.315824 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.316941 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2031,  727, 1750, 1348,  671, 369...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008123\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 20\n",
      "00:24:48.319154 call        14     def __getitem__(self, idx):\n",
      "00:24:48.319222 line        15         if self.mode == \"test\":\n",
      "00:24:48.319259 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = 'Jasper胖得下巴和脖子连成线，这样的小小春反而更加呆萌可爱'\n",
      "New var:....... text_b = '连成线,下巴'\n",
      "New var:....... label = 0\n",
      "00:24:48.319740 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.319831 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.319903 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.320082 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['j', '##as', '##per', '胖', '得', '下', '巴', '和', ...'小', '春', '反', '而', '更', '加', '呆', '萌', '可', '爱']\n",
      "00:24:48.320888 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'j', '##as', '##per', '胖', '得', '下', '... '反', '而', '更', '加', '呆', '萌', '可', '爱', '[SEP]']\n",
      "00:24:48.321078 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:24:48.321263 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['连', '成', '线', ',', '下', '巴']\n",
      "00:24:48.321576 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'j', '##as', '##per', '胖', '得', '下', '..., '[SEP]', '连', '成', '线', ',', '下', '巴', '[SEP]']\n",
      "00:24:48.321768 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 7\n",
      "00:24:48.321955 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 152, 8576, 9063, 5523, 2533, 678, 2349, 14...4263, 102, 6825, 2768, 5296, 117, 678, 2349, 102]\n",
      "00:24:48.322156 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  152, 8576, 9063, 5523, 2533,  678...6825, 2768, 5296,  117,  678, 2349,         102])\n",
      "00:24:48.322410 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.323046 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0,        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.323587 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.324441 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  152, 8576, 9063, 5523, 2533,  67... 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007019\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 21\n",
      "00:24:48.326199 call        14     def __getitem__(self, idx):\n",
      "00:24:48.326266 line        15         if self.mode == \"test\":\n",
      "00:24:48.326303 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '黄子韬：我演哭戏不用眼药水'\n",
      "New var:....... text_b = '黄子韬,眼药水,杨采钰,女主角,大秘密'\n",
      "New var:....... label = 0\n",
      "00:24:48.326768 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.326859 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.326929 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.327149 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['黄', '子', '韬', '：', '我', '演', '哭', '戏', '不', '用', '眼', '药', '水']\n",
      "00:24:48.327574 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '黄', '子', '韬', '：', '我', '演', '哭', '戏', '不', '用', '眼', '药', '水', '[SEP]']\n",
      "00:24:48.327820 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "00:24:48.327996 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['黄', '子', '韬', ',', '眼', '药', '水', ',', '杨', '采', '钰', ',', '女', '主', '角', ',', '大', '秘', '密']\n",
      "00:24:48.328526 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '黄', '子', '韬', '：', '我', '演', '哭', '戏'... ',', '女', '主', '角', ',', '大', '秘', '密', '[SEP]']\n",
      "00:24:48.328713 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:24:48.328898 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7942, 2094, 7507, 8038, 2769, 4028, 1526, ...117, 1957, 712, 6235, 117, 1920, 4908, 2166, 102]\n",
      "00:24:48.329097 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7942, 2094, 7507, 8038, 2769, 4028... 1957,  712, 6235,  117, 1920, 4908, 2166,  102])\n",
      "00:24:48.329349 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.329966 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.330526 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.331435 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7942, 2094, 7507, 8038, 2769, 402...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006958\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 22\n",
      "00:24:48.333187 call        14     def __getitem__(self, idx):\n",
      "00:24:48.333255 line        15         if self.mode == \"test\":\n",
      "00:24:48.333292 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '吴尊的肌肉是怎么练出来的？'\n",
      "New var:....... text_b = 'Fitness,飞轮海,吴尊,健身中心,文莱'\n",
      "New var:....... label = 0\n",
      "00:24:48.333758 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.333849 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.333919 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.334150 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['吴', '尊', '的', '肌', '肉', '是', '怎', '么', '练', '出', '来', '的', '？']\n",
      "00:24:48.334577 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '吴', '尊', '的', '肌', '肉', '是', '怎', '么', '练', '出', '来', '的', '？', '[SEP]']\n",
      "00:24:48.334824 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "00:24:48.335001 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['fit', '##ness', ',', '飞', '轮', '海', ',', '吴', '尊', ',', '健', '身', '中', '心', ',', '文', '莱']\n",
      "00:24:48.335522 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '吴', '尊', '的', '肌', '肉', '是', '怎', '么'... ',', '健', '身', '中', '心', ',', '文', '莱', '[SEP]']\n",
      "00:24:48.335712 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:24:48.335900 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1426, 2203, 4638, 5491, 5489, 3221, 2582, ... 117, 978, 6716, 704, 2552, 117, 3152, 5812, 102]\n",
      "00:24:48.336095 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  1426,  2203,  4638,  5491,  5489...  704,  2552,   117,         3152,  5812,   102])\n",
      "00:24:48.336342 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.336887 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.337496 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.338383 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  1426,  2203,  4638,  5491,  548...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006898\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 23\n",
      "00:24:48.340114 call        14     def __getitem__(self, idx):\n",
      "00:24:48.340183 line        15         if self.mode == \"test\":\n",
      "00:24:48.340221 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '韩瑜：出席活动，网友：真羡慕这个肌肉男！女神身材太好了'\n",
      "New var:....... text_b = '女神身材太,真羡慕,肌肉男,韩瑜,女神'\n",
      "New var:....... label = 0\n",
      "00:24:48.340689 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.340779 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.340850 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.341067 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['韩', '瑜', '：', '出', '席', '活', '动', '，', '网', '友...'肉', '男', '！', '女', '神', '身', '材', '太', '好', '了']\n",
      "00:24:48.341740 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '瑜', '：', '出', '席', '活', '动', '，'... '！', '女', '神', '身', '材', '太', '好', '了', '[SEP]']\n",
      "00:24:48.341967 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:24:48.342090 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['女', '神', '身', '材', '太', ',', '真', '羡', '慕', ',', '肌', '肉', '男', ',', '韩', '瑜', ',', '女', '神']\n",
      "00:24:48.342794 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '瑜', '：', '出', '席', '活', '动', '，'... '肉', '男', ',', '韩', '瑜', ',', '女', '神', '[SEP]']\n",
      "00:24:48.342959 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:24:48.343246 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7506, 4447, 8038, 1139, 2375, 3833, 1220, ...489, 4511, 117, 7506, 4447, 117, 1957, 4868, 102]\n",
      "00:24:48.343510 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7506, 4447, 8038, 1139, 2375, 3833... 117, 7506, 4447,  117, 1957, 4868,         102])\n",
      "00:24:48.343802 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.344743 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:24:48.345835 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.347872 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7506, 4447, 8038, 1139, 2375, 383...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011237\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 24\n",
      "00:24:48.351397 call        14     def __getitem__(self, idx):\n",
      "00:24:48.351482 line        15         if self.mode == \"test\":\n",
      "00:24:48.351522 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '陈都灵、沈月出演女主角'\n",
      "New var:....... text_b = '韩东,流星花园,七月与安生,沈月,七月,邹廷威,陈都灵,左耳'\n",
      "New var:....... label = 0\n",
      "00:24:48.352115 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.352212 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.352468 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.352910 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['陈', '都', '灵', '、', '沈', '月', '出', '演', '女', '主', '角']\n",
      "00:24:48.353841 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '都', '灵', '、', '沈', '月', '出', '演', '女', '主', '角', '[SEP]']\n",
      "00:24:48.354010 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 13\n",
      "00:24:48.354130 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['韩', '东', ',', '流', '星', '花', '园', ',', '七', '月...'邹', '廷', '威', ',', '陈', '都', '灵', ',', '左', '耳']\n",
      "00:24:48.354820 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '都', '灵', '、', '沈', '月', '出', '演'... '威', ',', '陈', '都', '灵', ',', '左', '耳', '[SEP]']\n",
      "00:24:48.355435 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 31\n",
      "00:24:48.356205 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7357, 6963, 4130, 510, 3755, 3299, 1139, 4...014, 117, 7357, 6963, 4130, 117, 2340, 5455, 102]\n",
      "00:24:48.357368 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7357, 6963, 4130,  510, 3755, 3299...  117, 7357, 6963, 4130,  117, 2340, 5455,  102])\n",
      "00:24:48.358525 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.359314 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.360068 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.361493 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7357, 6963, 4130,  510, 3755, 329... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.012938\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 25\n",
      "00:24:48.364369 call        14     def __getitem__(self, idx):\n",
      "00:24:48.364444 line        15         if self.mode == \"test\":\n",
      "00:24:48.364482 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '笑笑被曝携手七号西卡探险 七号Miss深夜发文帮助澄清'\n",
      "New var:....... text_b = '微博,Miss,探险,德云色'\n",
      "New var:....... label = 0\n",
      "00:24:48.365064 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.365159 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.365244 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.365729 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['笑', '笑', '被', '曝', '携', '手', '七', '号', '西', '卡..., 'miss', '深', '夜', '发', '文', '帮', '助', '澄', '清']\n",
      "00:24:48.366476 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '笑', '笑', '被', '曝', '携', '手', '七', '号'... '深', '夜', '发', '文', '帮', '助', '澄', '清', '[SEP]']\n",
      "00:24:48.366920 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:24:48.367198 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['微', '博', ',', 'miss', ',', '探', '险', ',', '德', '云', '色']\n",
      "00:24:48.367704 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '笑', '笑', '被', '曝', '携', '手', '七', '号'...iss', ',', '探', '险', ',', '德', '云', '色', '[SEP]']\n",
      "00:24:48.367990 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 12\n",
      "00:24:48.368269 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5010, 5010, 6158, 3284, 3025, 2797, 673, 1...9368, 117, 2968, 7372, 117, 2548, 756, 5682, 102]\n",
      "00:24:48.368559 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5010, 5010, 6158, 3284, 3025, 2797...2968, 7372,  117, 2548,  756, 5682,         102])\n",
      "00:24:48.368859 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.369792 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0,        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.370432 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.371733 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5010, 5010, 6158, 3284, 3025, 279... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009917\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 26\n",
      "00:24:48.374319 call        14     def __getitem__(self, idx):\n",
      "00:24:48.374392 line        15         if self.mode == \"test\":\n",
      "00:24:48.374430 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '沈梦辰穿短裤秀美腿现身机场 和周觅、沈凌等人赴巴厘岛参加婚礼'\n",
      "New var:....... text_b = '付辛博,沈梦辰,巴厘岛,沈凌,周觅'\n",
      "New var:....... label = 0\n",
      "00:24:48.375013 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.375265 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.375351 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.375620 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['沈', '梦', '辰', '穿', '短', '裤', '秀', '美', '腿', '现...'等', '人', '赴', '巴', '厘', '岛', '参', '加', '婚', '礼']\n",
      "00:24:48.376433 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '沈', '梦', '辰', '穿', '短', '裤', '秀', '美'... '赴', '巴', '厘', '岛', '参', '加', '婚', '礼', '[SEP]']\n",
      "00:24:48.376713 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:24:48.376987 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['付', '辛', '博', ',', '沈', '梦', '辰', ',', '巴', '厘', '岛', ',', '沈', '凌', ',', '周', '觅']\n",
      "00:24:48.377578 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '沈', '梦', '辰', '穿', '短', '裤', '秀', '美'... '厘', '岛', ',', '沈', '凌', ',', '周', '觅', '[SEP]']\n",
      "00:24:48.378069 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:24:48.378359 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3755, 3457, 6801, 4959, 4764, 6175, 4899, ...330, 2270, 117, 3755, 1119, 117, 1453, 6227, 102]\n",
      "00:24:48.378658 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3755, 3457, 6801, 4959, 4764, 6175... 117, 3755, 1119,  117, 1453, 6227,         102])\n",
      "00:24:48.378963 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.379890 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:24:48.380970 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.382230 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3755, 3457, 6801, 4959, 4764, 617...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011028\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 27\n",
      "00:24:48.385385 call        14     def __getitem__(self, idx):\n",
      "00:24:48.385465 line        15         if self.mode == \"test\":\n",
      "00:24:48.385503 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '女星穿搭很重要，一不小心变槽点：盘点女星的扑街丑照'\n",
      "New var:....... text_b = '邓紫棋,马思纯,吴昕,快乐大本营,穿搭,刘亦菲'\n",
      "New var:....... label = 0\n",
      "00:24:48.386162 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.386472 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.386568 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.387404 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['女', '星', '穿', '搭', '很', '重', '要', '，', '一', '不...'：', '盘', '点', '女', '星', '的', '扑', '街', '丑', '照']\n",
      "00:24:48.388336 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '星', '穿', '搭', '很', '重', '要', '，'... '点', '女', '星', '的', '扑', '街', '丑', '照', '[SEP]']\n",
      "00:24:48.388733 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:24:48.388913 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['邓', '紫', '棋', ',', '马', '思', '纯', ',', '吴', '昕...'大', '本', '营', ',', '穿', '搭', ',', '刘', '亦', '菲']\n",
      "00:24:48.389496 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '星', '穿', '搭', '很', '重', '要', '，'... '营', ',', '穿', '搭', ',', '刘', '亦', '菲', '[SEP]']\n",
      "00:24:48.390280 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "00:24:48.390617 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1957, 3215, 4959, 3022, 2523, 7028, 6206, ...5852, 117, 4959, 3022, 117, 1155, 771, 5838, 102]\n",
      "00:24:48.391396 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1957, 3215, 4959, 3022, 2523, 7028...4959, 3022,  117, 1155,         771, 5838,  102])\n",
      "00:24:48.391718 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.392246 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:24:48.393243 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.394175 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1957, 3215, 4959, 3022, 2523, 702...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010788\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 28\n",
      "00:24:48.396213 call        14     def __getitem__(self, idx):\n",
      "00:24:48.396393 line        15         if self.mode == \"test\":\n",
      "00:24:48.396434 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '刘浩强：演员、导演、编剧、策划人'\n",
      "New var:....... text_b = '演员,编剧,电影,婚礼,山乡溢彩,叮当遇险记,克隆出租,刘浩强'\n",
      "New var:....... label = 0\n",
      "00:24:48.397032 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.397131 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.397299 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.397553 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['刘', '浩', '强', '：', '演', '员', '、', '导', '演', '、', '编', '剧', '、', '策', '划', '人']\n",
      "00:24:48.398053 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刘', '浩', '强', '：', '演', '员', '、', '导', '演', '、', '编', '剧', '、', '策', '划', '人', '[SEP]']\n",
      "00:24:48.398241 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 18\n",
      "00:24:48.398609 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['演', '员', ',', '编', '剧', ',', '电', '影', ',', '婚...'记', ',', '克', '隆', '出', '租', ',', '刘', '浩', '强']\n",
      "00:24:48.399393 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刘', '浩', '强', '：', '演', '员', '、', '导'... '克', '隆', '出', '租', ',', '刘', '浩', '强', '[SEP]']\n",
      "00:24:48.399647 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 32\n",
      "00:24:48.399863 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1155, 3856, 2487, 8038, 4028, 1447, 510, 2...46, 7384, 1139, 4909, 117, 1155, 3856, 2487, 102]\n",
      "00:24:48.400081 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1155, 3856, 2487, 8038, 4028, 1447...1139, 4909,  117, 1155, 3856,        2487,  102])\n",
      "00:24:48.400364 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.401037 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:24:48.401642 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.403045 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1155, 3856, 2487, 8038, 4028, 144...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009681\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 29\n",
      "00:24:48.405935 call        14     def __getitem__(self, idx):\n",
      "00:24:48.406018 line        15         if self.mode == \"test\":\n",
      "00:24:48.406057 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '唐嫣首部穿越剧开播，《天意》配角比主角演技还好，实力圈粉！'\n",
      "New var:....... text_b = '张丹峰,秦始皇,唐嫣,天意,剧中饰演,乔振宇'\n",
      "New var:....... label = 0\n",
      "00:24:48.406662 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.406755 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.406835 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.407096 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['唐', '嫣', '首', '部', '穿', '越', '剧', '开', '播', '，...'演', '技', '还', '好', '，', '实', '力', '圈', '粉', '！']\n",
      "00:24:48.407892 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '唐', '嫣', '首', '部', '穿', '越', '剧', '开'... '还', '好', '，', '实', '力', '圈', '粉', '！', '[SEP]']\n",
      "00:24:48.408068 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:24:48.408262 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['张', '丹', '峰', ',', '秦', '始', '皇', ',', '唐', '嫣...'意', ',', '剧', '中', '饰', '演', ',', '乔', '振', '宇']\n",
      "00:24:48.408878 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '唐', '嫣', '首', '部', '穿', '越', '剧', '开'... '剧', '中', '饰', '演', ',', '乔', '振', '宇', '[SEP]']\n",
      "00:24:48.409260 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:24:48.409641 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1538, 2073, 7674, 6956, 4959, 6632, 1196, ...1196, 704, 7652, 4028, 117, 730, 2920, 2126, 102]\n",
      "00:24:48.410188 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1538, 2073, 7674, 6956, 4959, 6632...7652,        4028,  117,  730, 2920, 2126,  102])\n",
      "00:24:48.410538 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.411514 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:24:48.412239 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.413450 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1538, 2073, 7674, 6956, 4959, 663...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010104\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 30\n",
      "00:24:48.416068 call        14     def __getitem__(self, idx):\n",
      "00:24:48.416140 line        15         if self.mode == \"test\":\n",
      "00:24:48.416178 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '时尚扫街·“暮光女”克里斯汀·斯图尔特，个性短裙下车大摆Pose'\n",
      "New var:....... text_b = '克里斯汀,MTV电影奖最佳女演员奖,暮光之城,奇幻爱情电影,暮光之城：新月,东方IC,暮光之城：月食,届MTV'\n",
      "New var:....... label = 0\n",
      "00:24:48.416756 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.416850 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.416926 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.417204 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['时', '尚', '扫', '街', '·', '[UNK]', '暮', '光', '女'..., '个', '性', '短', '裙', '下', '车', '大', '摆', 'pose']\n",
      "00:24:48.417977 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '时', '尚', '扫', '街', '·', '[UNK]', '暮',...', '短', '裙', '下', '车', '大', '摆', 'pose', '[SEP]']\n",
      "00:24:48.418223 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:24:48.418413 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['克', '里', '斯', '汀', ',', 'mtv', '电', '影', '奖', ...', '光', '之', '城', '：', '月', '食', ',', '届', 'mtv']\n",
      "00:24:48.419527 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '时', '尚', '扫', '街', '·', '[UNK]', '暮',...之', '城', '：', '月', '食', ',', '届', 'mtv', '[SEP]']\n",
      "00:24:48.420002 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 51\n",
      "00:24:48.420224 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3198, 2213, 2812, 6125, 185, 100, 3272, 10...2, 1814, 8038, 3299, 7608, 117, 2237, 11529, 102]\n",
      "00:24:48.420476 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  3198,  2213,  2812,  6125,   185... 3299,  7608,   117,  2237,        11529,   102])\n",
      "00:24:48.420780 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.421840 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.422867 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.424356 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  3198,  2213,  2812,  6125,   18...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011315\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 31\n",
      "00:24:48.427414 call        14     def __getitem__(self, idx):\n",
      "00:24:48.427487 line        15         if self.mode == \"test\":\n",
      "00:24:48.427524 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '被消失五年，这是近几年最让人不寒而栗的国产佳作'\n",
      "New var:....... text_b = '李玩,中国式,烈日灼心,鸟人,时间简史,曹保平,爱因斯坦,李米的猜想,追凶者也,周迅'\n",
      "New var:....... label = 0\n",
      "00:24:48.428099 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.428267 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.428344 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.428520 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['被', '消', '失', '五', '年', '，', '这', '是', '近', '几...'人', '不', '寒', '而', '栗', '的', '国', '产', '佳', '作']\n",
      "00:24:48.429179 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '被', '消', '失', '五', '年', '，', '这', '是'... '寒', '而', '栗', '的', '国', '产', '佳', '作', '[SEP]']\n",
      "00:24:48.429365 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:24:48.429546 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['李', '玩', ',', '中', '国', '式', ',', '烈', '日', '灼...'猜', '想', ',', '追', '凶', '者', '也', ',', '周', '迅']\n",
      "00:24:48.430472 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '被', '消', '失', '五', '年', '，', '这', '是'... ',', '追', '凶', '者', '也', ',', '周', '迅', '[SEP]']\n",
      "00:24:48.430764 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 43\n",
      "00:24:48.430958 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6158, 3867, 1927, 758, 2399, 8024, 6821, 3...117, 6841, 1136, 5442, 738, 117, 1453, 6813, 102]\n",
      "00:24:48.431170 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6158, 3867, 1927,  758, 2399, 8024... 6841, 1136, 5442,  738,  117, 1453, 6813,  102])\n",
      "00:24:48.431384 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.432310 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.432999 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.434217 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6158, 3867, 1927,  758, 2399, 802... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009344\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 32\n",
      "00:24:48.436788 call        14     def __getitem__(self, idx):\n",
      "00:24:48.436856 line        15         if self.mode == \"test\":\n",
      "00:24:48.436893 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '她出演了韩国的综艺节目，她的美被人发现'\n",
      "New var:....... text_b = '塞尔维亚语,综艺节目,Angelina,圣彼得堡,俄罗斯'\n",
      "New var:....... label = 0\n",
      "00:24:48.437419 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.437509 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.437581 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.437828 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['她', '出', '演', '了', '韩', '国', '的', '综', '艺', '节', '目', '，', '她', '的', '美', '被', '人', '发', '现']\n",
      "00:24:48.438367 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '她', '出', '演', '了', '韩', '国', '的', '综'... '，', '她', '的', '美', '被', '人', '发', '现', '[SEP]']\n",
      "00:24:48.438550 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:24:48.438790 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['塞', '尔', '维', '亚', '语', ',', '综', '艺', '节', '目...na', ',', '圣', '彼', '得', '堡', ',', '俄', '罗', '斯']\n",
      "00:24:48.439443 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '她', '出', '演', '了', '韩', '国', '的', '综'... '圣', '彼', '得', '堡', ',', '俄', '罗', '斯', '[SEP]']\n",
      "00:24:48.439650 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:24:48.439879 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1961, 1139, 4028, 749, 7506, 1744, 4638, 5...760, 2516, 2533, 1836, 117, 915, 5384, 3172, 102]\n",
      "00:24:48.440119 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1961, 1139, 4028,  749, 7506, 1744... 2516, 2533, 1836,  117,  915, 5384, 3172,  102])\n",
      "00:24:48.440307 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.440917 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.441524 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.442609 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1961, 1139, 4028,  749, 7506, 174... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007795\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 33\n",
      "00:24:48.444612 call        14     def __getitem__(self, idx):\n",
      "00:24:48.444750 line        15         if self.mode == \"test\":\n",
      "00:24:48.444791 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = 'TVB艺人吴家乐拍新剧遭人排挤，好友李珊珊安慰：遇神杀神！'\n",
      "New var:....... text_b = '吴家乐,包青天再起风云,花蝴蝶,谭俊彦,姚子羚,城寨英雄,李珊珊,胡定欣'\n",
      "New var:....... label = 0\n",
      "00:24:48.445318 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.445410 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.445482 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.445662 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['tvb', '艺', '人', '吴', '家', '乐', '拍', '新', '剧', ...'珊', '珊', '安', '慰', '：', '遇', '神', '杀', '神', '！']\n",
      "00:24:48.446350 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'tvb', '艺', '人', '吴', '家', '乐', '拍', '... '安', '慰', '：', '遇', '神', '杀', '神', '！', '[SEP]']\n",
      "00:24:48.446577 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:24:48.446764 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['吴', '家', '乐', ',', '包', '青', '天', '再', '起', '风...'英', '雄', ',', '李', '珊', '珊', ',', '胡', '定', '欣']\n",
      "00:24:48.447647 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'tvb', '艺', '人', '吴', '家', '乐', '拍', '... ',', '李', '珊', '珊', ',', '胡', '定', '欣', '[SEP]']\n",
      "00:24:48.447849 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 37\n",
      "00:24:48.448044 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 9312, 5686, 782, 1426, 2157, 727, 2864, 31...17, 3330, 4396, 4396, 117, 5529, 2137, 3615, 102]\n",
      "00:24:48.448267 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 9312, 5686,  782, 1426, 2157,  727...4396,        4396,  117, 5529, 2137, 3615,  102])\n",
      "00:24:48.448485 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.449365 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.450067 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.451358 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 9312, 5686,  782, 1426, 2157,  72... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009173\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 34\n",
      "00:24:48.453814 call        14     def __getitem__(self, idx):\n",
      "00:24:48.453884 line        15         if self.mode == \"test\":\n",
      "00:24:48.453921 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '言承旭带娃俨然慈父，端小碗耐心喂饭超宠溺，网友喊话林志玲'\n",
      "New var:....... text_b = '放开我北鼻,言承旭,经纪人,真人秀,林志玲'\n",
      "New var:....... label = 0\n",
      "00:24:48.454460 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.454552 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.454629 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.454902 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['言', '承', '旭', '带', '娃', '俨', '然', '慈', '父', '，...'宠', '溺', '，', '网', '友', '喊', '话', '林', '志', '玲']\n",
      "00:24:48.455623 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '言', '承', '旭', '带', '娃', '俨', '然', '慈'... '，', '网', '友', '喊', '话', '林', '志', '玲', '[SEP]']\n",
      "00:24:48.455819 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:24:48.456003 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['放', '开', '我', '北', '鼻', ',', '言', '承', '旭', ',...'纪', '人', ',', '真', '人', '秀', ',', '林', '志', '玲']\n",
      "00:24:48.456577 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '言', '承', '旭', '带', '娃', '俨', '然', '慈'... ',', '真', '人', '秀', ',', '林', '志', '玲', '[SEP]']\n",
      "00:24:48.456774 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 22\n",
      "00:24:48.457104 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6241, 2824, 3195, 2372, 2015, 929, 4197, 2...117, 4696, 782, 4899, 117, 3360, 2562, 4386, 102]\n",
      "00:24:48.457328 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6241, 2824, 3195, 2372, 2015,  929... 782, 4899,  117,        3360, 2562, 4386,  102])\n",
      "00:24:48.457550 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.458213 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:24:48.458927 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.460181 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6241, 2824, 3195, 2372, 2015,  92...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008317\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 35\n",
      "00:24:48.462159 call        14     def __getitem__(self, idx):\n",
      "00:24:48.462225 line        15         if self.mode == \"test\":\n",
      "00:24:48.462352 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '一部反应人性的电影，母女逃难被全村人欺负，让人心焦'\n",
      "New var:....... text_b = '全村人,电影,母女'\n",
      "New var:....... label = 0\n",
      "00:24:48.462970 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.463063 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.463139 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.463324 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['一', '部', '反', '应', '人', '性', '的', '电', '影', '，...'全', '村', '人', '欺', '负', '，', '让', '人', '心', '焦']\n",
      "00:24:48.463972 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '一', '部', '反', '应', '人', '性', '的', '电'... '人', '欺', '负', '，', '让', '人', '心', '焦', '[SEP]']\n",
      "00:24:48.464198 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:24:48.464379 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['全', '村', '人', ',', '电', '影', ',', '母', '女']\n",
      "00:24:48.464809 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '一', '部', '反', '应', '人', '性', '的', '电'... '村', '人', ',', '电', '影', ',', '母', '女', '[SEP]']\n",
      "00:24:48.465002 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 10\n",
      "00:24:48.465189 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 671, 6956, 1353, 2418, 782, 2595, 4638, 45...3333, 782, 117, 4510, 2512, 117, 3678, 1957, 102]\n",
      "00:24:48.465388 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  671, 6956, 1353, 2418,  782, 2595... 117, 4510, 2512,  117, 3678, 1957,         102])\n",
      "00:24:48.465593 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.466116 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0,        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.466790 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.467694 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  671, 6956, 1353, 2418,  782, 259... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007267\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 36\n",
      "00:24:48.469456 call        14     def __getitem__(self, idx):\n",
      "00:24:48.469525 line        15         if self.mode == \"test\":\n",
      "00:24:48.469562 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '女明星的高跟鞋谁最奇特？'\n",
      "New var:....... text_b = '女明星,景甜,冉莹颖,尚雯婕,欧阳娜娜,邹市明,女人味,范冰冰,袁姗姗,驴蹄鞋,恨天高,连衣裙'\n",
      "New var:....... label = 0\n",
      "00:24:48.470075 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.470166 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.470238 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.470484 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['女', '明', '星', '的', '高', '跟', '鞋', '谁', '最', '奇', '特', '？']\n",
      "00:24:48.471018 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '明', '星', '的', '高', '跟', '鞋', '谁', '最', '奇', '特', '？', '[SEP]']\n",
      "00:24:48.471346 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 14\n",
      "00:24:48.471633 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['女', '明', '星', ',', '景', '甜', ',', '冉', '莹', '颖...'蹄', '鞋', ',', '恨', '天', '高', ',', '连', '衣', '裙']\n",
      "00:24:48.472684 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '明', '星', '的', '高', '跟', '鞋', '谁'... ',', '恨', '天', '高', ',', '连', '衣', '裙', '[SEP]']\n",
      "00:24:48.472928 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 48\n",
      "00:24:48.473125 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1957, 3209, 3215, 4638, 7770, 6656, 7490, ...17, 2616, 1921, 7770, 117, 6825, 6132, 6170, 102]\n",
      "00:24:48.473338 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1957, 3209, 3215, 4638, 7770, 6656...1921, 7770,  117, 6825, 6132,        6170,  102])\n",
      "00:24:48.473558 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.474351 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.475022 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.476240 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1957, 3209, 3215, 4638, 7770, 665... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009222\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 37\n",
      "00:24:48.478710 call        14     def __getitem__(self, idx):\n",
      "00:24:48.478780 line        15         if self.mode == \"test\":\n",
      "00:24:48.478816 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '鹿晗出行澳门被网友偶遇，身旁同行的女孩亮了，有点眼熟'\n",
      "New var:....... text_b = '鹿晗,澳门,粉丝'\n",
      "New var:....... label = 0\n",
      "00:24:48.479404 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.479495 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.479568 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.479879 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['鹿', '晗', '出', '行', '澳', '门', '被', '网', '友', '偶...'的', '女', '孩', '亮', '了', '，', '有', '点', '眼', '熟']\n",
      "00:24:48.480535 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '鹿', '晗', '出', '行', '澳', '门', '被', '网'... '孩', '亮', '了', '，', '有', '点', '眼', '熟', '[SEP]']\n",
      "00:24:48.480722 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:24:48.480904 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['鹿', '晗', ',', '澳', '门', ',', '粉', '丝']\n",
      "00:24:48.481249 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '鹿', '晗', '出', '行', '澳', '门', '被', '网'... '鹿', '晗', ',', '澳', '门', ',', '粉', '丝', '[SEP]']\n",
      "00:24:48.481439 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 9\n",
      "00:24:48.481624 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7922, 3240, 1139, 6121, 4078, 7305, 6158, ...7922, 3240, 117, 4078, 7305, 117, 5106, 692, 102]\n",
      "00:24:48.481859 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7922, 3240, 1139, 6121, 4078, 7305... 117, 4078, 7305,  117, 5106,  692,         102])\n",
      "00:24:48.482127 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.482690 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0,        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.483310 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.484276 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7922, 3240, 1139, 6121, 4078, 730... 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007343\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 38\n",
      "00:24:48.486081 call        14     def __getitem__(self, idx):\n",
      "00:24:48.486149 line        15         if self.mode == \"test\":\n",
      "00:24:48.486186 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = 'Jasper为应采儿庆祝母亲节用这招，网友：心都化了'\n",
      "New var:....... text_b = '爸爸去哪儿,社交网站,母亲节,陈小春,爸爸去哪儿5,应采儿,Jasper'\n",
      "New var:....... label = 0\n",
      "00:24:48.486680 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.486866 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.486944 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.487147 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['j', '##as', '##per', '为', '应', '采', '儿', '庆', ...'这', '招', '，', '网', '友', '：', '心', '都', '化', '了']\n",
      "00:24:48.487772 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'j', '##as', '##per', '为', '应', '采', '... '，', '网', '友', '：', '心', '都', '化', '了', '[SEP]']\n",
      "00:24:48.487963 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:24:48.488185 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['爸', '爸', '去', '哪', '儿', ',', '社', '交', '网', '站...', ',', '应', '采', '儿', ',', 'j', '##as', '##per']\n",
      "00:24:48.488960 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'j', '##as', '##per', '为', '应', '采', '...应', '采', '儿', ',', 'j', '##as', '##per', '[SEP]']\n",
      "00:24:48.489224 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 34\n",
      "00:24:48.489421 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 152, 8576, 9063, 711, 2418, 7023, 1036, 24...117, 2418, 7023, 1036, 117, 152, 8576, 9063, 102]\n",
      "00:24:48.489632 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  152, 8576, 9063,  711, 2418, 7023... 2418, 7023, 1036,  117,  152, 8576, 9063,  102])\n",
      "00:24:48.489845 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.490574 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.491230 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.492458 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  152, 8576, 9063,  711, 2418, 702...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008826\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 39\n",
      "00:24:48.494935 call        14     def __getitem__(self, idx):\n",
      "00:24:48.495004 line        15         if self.mode == \"test\":\n",
      "00:24:48.495041 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '娱乐圈中男扮女装的男星们，有的变身萌妹，有的辣眼睛！'\n",
      "New var:....... text_b = '美人痣,男星,娱乐圈,马天宇,萌妹子'\n",
      "New var:....... label = 0\n",
      "00:24:48.495522 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.495614 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.495685 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.495863 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['娱', '乐', '圈', '中', '男', '扮', '女', '装', '的', '男...'身', '萌', '妹', '，', '有', '的', '辣', '眼', '睛', '！']\n",
      "00:24:48.496514 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '中', '男', '扮', '女', '装'... '妹', '，', '有', '的', '辣', '眼', '睛', '！', '[SEP]']\n",
      "00:24:48.496668 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:24:48.496881 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['美', '人', '痣', ',', '男', '星', ',', '娱', '乐', '圈', ',', '马', '天', '宇', ',', '萌', '妹', '子']\n",
      "00:24:48.497494 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '中', '男', '扮', '女', '装'... ',', '马', '天', '宇', ',', '萌', '妹', '子', '[SEP]']\n",
      "00:24:48.497715 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:24:48.497917 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2031, 727, 1750, 704, 4511, 2815, 1957, 61...17, 7716, 1921, 2126, 117, 5846, 1987, 2094, 102]\n",
      "00:24:48.498127 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2031,  727, 1750,  704, 4511, 2815... 7716, 1921, 2126,  117, 5846, 1987, 2094,  102])\n",
      "00:24:48.498345 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.498924 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.499610 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.500668 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2031,  727, 1750,  704, 4511, 281... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007721\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 40\n",
      "00:24:48.502685 call        14     def __getitem__(self, idx):\n",
      "00:24:48.502753 line        15         if self.mode == \"test\":\n",
      "00:24:48.502790 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '高颜值王源也会选错配件，贵气风衣却搭地摊包包，效果一言难尽'\n",
      "New var:....... text_b = '牛仔服,邮差包,王源,Salvatore,大主宰,Ferragamo'\n",
      "New var:....... label = 0\n",
      "00:24:48.503274 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.503366 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.503437 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.503656 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['高', '颜', '值', '王', '源', '也', '会', '选', '错', '配...'摊', '包', '包', '，', '效', '果', '一', '言', '难', '尽']\n",
      "00:24:48.504361 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '高', '颜', '值', '王', '源', '也', '会', '选'... '包', '，', '效', '果', '一', '言', '难', '尽', '[SEP]']\n",
      "00:24:48.504549 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:24:48.504732 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['牛', '仔', '服', ',', '邮', '差', '包', ',', '王', '源...salvatore', ',', '大', '主', '宰', ',', 'ferragamo']\n",
      "00:24:48.505406 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '高', '颜', '值', '王', '源', '也', '会', '选'...', ',', '大', '主', '宰', ',', 'ferragamo', '[SEP]']\n",
      "00:24:48.505606 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:24:48.505837 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7770, 7582, 966, 4374, 3975, 738, 833, 684...117, 10078, 117, 1920, 712, 2153, 117, 9992, 102]\n",
      "00:24:48.506046 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  7770,  7582,   966,  4374,  3975...  117,  1920,   712,  2153,   117,  9992,   102])\n",
      "00:24:48.506259 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.506877 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:24:48.507583 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.508753 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  7770,  7582,   966,  4374,  397...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008376\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 41\n",
      "00:24:48.511092 call        14     def __getitem__(self, idx):\n",
      "00:24:48.511164 line        15         if self.mode == \"test\":\n",
      "00:24:48.511203 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '王菲私人小号疑似曝光，分享生活日常还晒与谢霆锋合影'\n",
      "New var:....... text_b = '分享生活日常,王菲,谢霆锋,谢霆锋合影王菲私人小号,王菲私人小号疑似曝光'\n",
      "New var:....... label = 0\n",
      "00:24:48.511712 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.511804 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.511879 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.512204 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['王', '菲', '私', '人', '小', '号', '疑', '似', '曝', '光...'日', '常', '还', '晒', '与', '谢', '霆', '锋', '合', '影']\n",
      "00:24:48.512855 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '菲', '私', '人', '小', '号', '疑', '似'... '还', '晒', '与', '谢', '霆', '锋', '合', '影', '[SEP]']\n",
      "00:24:48.513081 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:24:48.513294 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['分', '享', '生', '活', '日', '常', ',', '王', '菲', ',...'王', '菲', '私', '人', '小', '号', '疑', '似', '曝', '光']\n",
      "00:24:48.514126 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '菲', '私', '人', '小', '号', '疑', '似'... '私', '人', '小', '号', '疑', '似', '曝', '光', '[SEP]']\n",
      "00:24:48.514441 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 37\n",
      "00:24:48.514703 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4374, 5838, 4900, 782, 2207, 1384, 4542, 8...900, 782, 2207, 1384, 4542, 849, 3284, 1045, 102]\n",
      "00:24:48.514928 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4374, 5838, 4900,  782, 2207, 1384...2207, 1384, 4542,         849, 3284, 1045,  102])\n",
      "00:24:48.515167 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.515863 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.516678 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.517870 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4374, 5838, 4900,  782, 2207, 138... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009256\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 42\n",
      "00:24:48.520375 call        14     def __getitem__(self, idx):\n",
      "00:24:48.520444 line        15         if self.mode == \"test\":\n",
      "00:24:48.520481 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '很快和鹿晗结婚？关晓彤的回答太甜蜜了！'\n",
      "New var:....... text_b = '鹿晗,关晓彤,粉丝,董子健,男明星'\n",
      "New var:....... label = 0\n",
      "00:24:48.520996 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.521086 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.521157 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.521457 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['很', '快', '和', '鹿', '晗', '结', '婚', '？', '关', '晓', '彤', '的', '回', '答', '太', '甜', '蜜', '了', '！']\n",
      "00:24:48.521995 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '很', '快', '和', '鹿', '晗', '结', '婚', '？'... '的', '回', '答', '太', '甜', '蜜', '了', '！', '[SEP]']\n",
      "00:24:48.522177 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:24:48.522355 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['鹿', '晗', ',', '关', '晓', '彤', ',', '粉', '丝', ',', '董', '子', '健', ',', '男', '明', '星']\n",
      "00:24:48.522850 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '很', '快', '和', '鹿', '晗', '结', '婚', '？'... ',', '董', '子', '健', ',', '男', '明', '星', '[SEP]']\n",
      "00:24:48.523042 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:24:48.523228 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2523, 2571, 1469, 7922, 3240, 5310, 2042, ...117, 5869, 2094, 978, 117, 4511, 3209, 3215, 102]\n",
      "00:24:48.523361 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2523, 2571, 1469, 7922, 3240, 5310...2094,  978,  117, 4511,        3209, 3215,  102])\n",
      "00:24:48.523499 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.524164 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.524754 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.525709 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2523, 2571, 1469, 7922, 3240, 531... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007147\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 43\n",
      "00:24:48.527548 call        14     def __getitem__(self, idx):\n",
      "00:24:48.527614 line        15         if self.mode == \"test\":\n",
      "00:24:48.527650 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '唐嫣身穿时尚现身机场，网友：正值颜值最巅峰'\n",
      "New var:....... text_b = '唐嫣,颜值,东方IC,机场,东方ic'\n",
      "New var:....... label = 0\n",
      "00:24:48.528135 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.528240 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.528316 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.528672 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['唐', '嫣', '身', '穿', '时', '尚', '现', '身', '机', '场...'网', '友', '：', '正', '值', '颜', '值', '最', '巅', '峰']\n",
      "00:24:48.529392 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '唐', '嫣', '身', '穿', '时', '尚', '现', '身'... '：', '正', '值', '颜', '值', '最', '巅', '峰', '[SEP]']\n",
      "00:24:48.529611 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:24:48.529840 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['唐', '嫣', ',', '颜', '值', ',', '东', '方', 'ic', ',', '机', '场', ',', '东', '方', 'ic']\n",
      "00:24:48.530337 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '唐', '嫣', '身', '穿', '时', '尚', '现', '身'...ic', ',', '机', '场', ',', '东', '方', 'ic', '[SEP]']\n",
      "00:24:48.530531 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 17\n",
      "00:24:48.530721 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1538, 2073, 6716, 4959, 3198, 2213, 4385, ...8577, 117, 3322, 1767, 117, 691, 3175, 8577, 102]\n",
      "00:24:48.530923 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1538, 2073, 6716, 4959, 3198, 2213...3322, 1767,  117,         691, 3175, 8577,  102])\n",
      "00:24:48.531197 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.531739 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.532368 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.533462 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1538, 2073, 6716, 4959, 3198, 221... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007861\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 44\n",
      "00:24:48.535438 call        14     def __getitem__(self, idx):\n",
      "00:24:48.535507 line        15         if self.mode == \"test\":\n",
      "00:24:48.535609 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '这款小型SUV比宝骏510智能，难怪市场下滑也能连续6个月过万辆'\n",
      "New var:....... text_b = '极限挑战,远景X3,真质良品SUV,优等生,起跑线,吉利汽车'\n",
      "New var:....... label = 0\n",
      "00:24:48.536172 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.536263 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.536336 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.536565 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['这', '款', '小', '型', 'suv', '比', '宝', '骏', '510'...'也', '能', '连', '续', '6', '个', '月', '过', '万', '辆']\n",
      "00:24:48.537285 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '款', '小', '型', 'suv', '比', '宝', '... '连', '续', '6', '个', '月', '过', '万', '辆', '[SEP]']\n",
      "00:24:48.537528 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:24:48.537817 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['极', '限', '挑', '战', ',', '远', '景', 'x3', ',', '...'生', ',', '起', '跑', '线', ',', '吉', '利', '汽', '车']\n",
      "00:24:48.538524 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '款', '小', '型', 'suv', '比', '宝', '... '起', '跑', '线', ',', '吉', '利', '汽', '车', '[SEP]']\n",
      "00:24:48.538728 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 28\n",
      "00:24:48.538923 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6821, 3621, 2207, 1798, 8540, 3683, 2140, ...29, 6651, 5296, 117, 1395, 1164, 3749, 6756, 102]\n",
      "00:24:48.539140 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  6821,  3621,  2207,  1798,  8540... 5296,   117,  1395,  1164,  3749,  6756,   102])\n",
      "00:24:48.539361 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.540109 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.540804 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.541904 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  6821,  3621,  2207,  1798,  854...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008693\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 45\n",
      "00:24:48.544171 call        14     def __getitem__(self, idx):\n",
      "00:24:48.544247 line        15         if self.mode == \"test\":\n",
      "00:24:48.544285 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '后宫才华服饰比美，贵妃创意被皇后夺取，现代女子帮贵妃扭转乾坤'\n",
      "New var:....... text_b = '皇后,后宫,服饰'\n",
      "New var:....... label = 0\n",
      "00:24:48.544931 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.545034 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.545217 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.545572 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['后', '宫', '才', '华', '服', '饰', '比', '美', '，', '贵...'代', '女', '子', '帮', '贵', '妃', '扭', '转', '乾', '坤']\n",
      "00:24:48.546329 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '后', '宫', '才', '华', '服', '饰', '比', '美'... '子', '帮', '贵', '妃', '扭', '转', '乾', '坤', '[SEP]']\n",
      "00:24:48.546461 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:48.546687 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['皇', '后', ',', '后', '宫', ',', '服', '饰']\n",
      "00:24:48.547057 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '后', '宫', '才', '华', '服', '饰', '比', '美'... '皇', '后', ',', '后', '宫', ',', '服', '饰', '[SEP]']\n",
      "00:24:48.547254 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 9\n",
      "00:24:48.547443 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1400, 2151, 2798, 1290, 3302, 7652, 3683, ...640, 1400, 117, 1400, 2151, 117, 3302, 7652, 102]\n",
      "00:24:48.547647 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1400, 2151, 2798, 1290, 3302, 7652... 117, 1400,        2151,  117, 3302, 7652,  102])\n",
      "00:24:48.548005 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.548593 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.549181 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.550639 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1400, 2151, 2798, 1290, 3302, 765... 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008830\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 46\n",
      "00:24:48.553050 call        14     def __getitem__(self, idx):\n",
      "00:24:48.553142 line        15         if self.mode == \"test\":\n",
      "00:24:48.553181 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '杨幂旗下9位小花旦，我猜你只认识迪丽热巴，能叫出名字，算你牛'\n",
      "New var:....... text_b = '铁粉杨幂旗下,迪丽热巴,我猜,幂幂,花旦'\n",
      "New var:....... label = 0\n",
      "00:24:48.553844 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.553947 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.554192 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.554531 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['杨', '幂', '旗', '下', '9', '位', '小', '花', '旦', '，...'，', '能', '叫', '出', '名', '字', '，', '算', '你', '牛']\n",
      "00:24:48.555249 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '幂', '旗', '下', '9', '位', '小', '花'... '叫', '出', '名', '字', '，', '算', '你', '牛', '[SEP]']\n",
      "00:24:48.555601 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:48.555836 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['铁', '粉', '杨', '幂', '旗', '下', ',', '迪', '丽', '热', '巴', ',', '我', '猜', ',', '幂', '幂', ',', '花', '旦']\n",
      "00:24:48.556445 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '幂', '旗', '下', '9', '位', '小', '花'... '我', '猜', ',', '幂', '幂', ',', '花', '旦', '[SEP]']\n",
      "00:24:48.556709 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:24:48.556948 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3342, 2386, 3186, 678, 130, 855, 2207, 570...769, 4339, 117, 2386, 2386, 117, 5709, 3190, 102]\n",
      "00:24:48.557204 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3342, 2386, 3186,  678,  130,  855... 117, 2386,        2386,  117, 5709, 3190,  102])\n",
      "00:24:48.557575 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.558500 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1])\n",
      "00:24:48.559309 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.560762 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3342, 2386, 3186,  678,  130,  85...1, 1, 1, 1, 1,        1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010165\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 47\n",
      "00:24:48.563246 call        14     def __getitem__(self, idx):\n",
      "00:24:48.563317 line        15         if self.mode == \"test\":\n",
      "00:24:48.563356 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '舒淇出席某活动 网友：看到她的腿，就不在乎她的嘴！'\n",
      "New var:....... text_b = '舒淇出席,舒淇'\n",
      "New var:....... label = 0\n",
      "00:24:48.563936 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.564035 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.564238 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.564492 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['舒', '淇', '出', '席', '某', '活', '动', '网', '友', '：...'腿', '，', '就', '不', '在', '乎', '她', '的', '嘴', '！']\n",
      "00:24:48.565224 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '舒', '淇', '出', '席', '某', '活', '动', '网'... '就', '不', '在', '乎', '她', '的', '嘴', '！', '[SEP]']\n",
      "00:24:48.565688 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:24:48.565933 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['舒', '淇', '出', '席', ',', '舒', '淇']\n",
      "00:24:48.566324 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '舒', '淇', '出', '席', '某', '活', '动', '网'...EP]', '舒', '淇', '出', '席', ',', '舒', '淇', '[SEP]']\n",
      "00:24:48.566565 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 8\n",
      "00:24:48.566801 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5653, 3899, 1139, 2375, 3378, 3833, 1220, ...02, 5653, 3899, 1139, 2375, 117, 5653, 3899, 102]\n",
      "00:24:48.567050 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5653, 3899, 1139, 2375, 3378, 3833... 5653, 3899, 1139, 2375,  117, 5653, 3899,  102])\n",
      "00:24:48.567305 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.568182 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0,        0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.568893 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.569917 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5653, 3899, 1139, 2375, 3378, 383...       0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008830\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 48\n",
      "00:24:48.572114 call        14     def __getitem__(self, idx):\n",
      "00:24:48.572188 line        15         if self.mode == \"test\":\n",
      "00:24:48.572225 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '推五部将军为背景的古言情小说，第二部有点像女版的琅琊榜'\n",
      "New var:....... text_b = '将军有喜,琅琊榜,将军叼回个小娇娘,男主,女主,穿越文,大将军,将军策嫡女权谋'\n",
      "New var:....... label = 0\n",
      "00:24:48.572841 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.572935 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.573284 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.573515 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['推', '五', '部', '将', '军', '为', '背', '景', '的', '古...'部', '有', '点', '像', '女', '版', '的', '琅', '琊', '榜']\n",
      "00:24:48.574251 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '推', '五', '部', '将', '军', '为', '背', '景'... '点', '像', '女', '版', '的', '琅', '琊', '榜', '[SEP]']\n",
      "00:24:48.574487 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:24:48.574717 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['将', '军', '有', '喜', ',', '琅', '琊', '榜', ',', '将...'将', '军', ',', '将', '军', '策', '嫡', '女', '权', '谋']\n",
      "00:24:48.575635 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '推', '五', '部', '将', '军', '为', '背', '景'... ',', '将', '军', '策', '嫡', '女', '权', '谋', '[SEP]']\n",
      "00:24:48.575990 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 40\n",
      "00:24:48.576232 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2972, 758, 6956, 2199, 1092, 711, 5520, 32...7, 2199, 1092, 5032, 2072, 1957, 3326, 6450, 102]\n",
      "00:24:48.576494 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2972,  758, 6956, 2199, 1092,  711... 2199, 1092, 5032, 2072, 1957, 3326, 6450,  102])\n",
      "00:24:48.576765 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.577566 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.578578 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.580118 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2972,  758, 6956, 2199, 1092,  71... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011665\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 49\n",
      "00:24:48.583808 call        14     def __getitem__(self, idx):\n",
      "00:24:48.583880 line        15         if self.mode == \"test\":\n",
      "00:24:48.583917 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '朱亚文出身于什么样的家庭？'\n",
      "New var:....... text_b = '父辈的旗帜,父傲,艺人,朱亚文,娱乐圈,闯关东'\n",
      "New var:....... label = 0\n",
      "00:24:48.584469 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.584680 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.584758 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.584983 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['朱', '亚', '文', '出', '身', '于', '什', '么', '样', '的', '家', '庭', '？']\n",
      "00:24:48.585463 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '朱', '亚', '文', '出', '身', '于', '什', '么', '样', '的', '家', '庭', '？', '[SEP]']\n",
      "00:24:48.585692 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "00:24:48.585836 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['父', '辈', '的', '旗', '帜', ',', '父', '傲', ',', '艺...'亚', '文', ',', '娱', '乐', '圈', ',', '闯', '关', '东']\n",
      "00:24:48.586431 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '朱', '亚', '文', '出', '身', '于', '什', '么'... ',', '娱', '乐', '圈', ',', '闯', '关', '东', '[SEP]']\n",
      "00:24:48.586599 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "00:24:48.586862 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3319, 762, 3152, 1139, 6716, 754, 784, 720... 117, 2031, 727, 1750, 117, 7310, 1068, 691, 102]\n",
      "00:24:48.587089 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3319,  762, 3152, 1139, 6716,  754... 727, 1750,  117, 7310,        1068,  691,  102])\n",
      "00:24:48.587340 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.588061 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.588787 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.589889 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3319,  762, 3152, 1139, 6716,  75... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008267\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 50\n",
      "00:24:48.592105 call        14     def __getitem__(self, idx):\n",
      "00:24:48.592172 line        15         if self.mode == \"test\":\n",
      "00:24:48.592208 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '林心如曾是多少人的梦中情人，现在她当妈了，非常受宝宝喜欢'\n",
      "New var:....... text_b = '霍建华,林心如,小孩子'\n",
      "New var:....... label = 0\n",
      "00:24:48.592717 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.592808 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.592880 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.593106 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['林', '心', '如', '曾', '是', '多', '少', '人', '的', '梦...'妈', '了', '，', '非', '常', '受', '宝', '宝', '喜', '欢']\n",
      "00:24:48.593836 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '心', '如', '曾', '是', '多', '少', '人'... '，', '非', '常', '受', '宝', '宝', '喜', '欢', '[SEP]']\n",
      "00:24:48.594069 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:24:48.594406 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['霍', '建', '华', ',', '林', '心', '如', ',', '小', '孩', '子']\n",
      "00:24:48.594852 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '心', '如', '曾', '是', '多', '少', '人'... ',', '林', '心', '如', ',', '小', '孩', '子', '[SEP]']\n",
      "00:24:48.595092 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 12\n",
      "00:24:48.595327 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3360, 2552, 1963, 3295, 3221, 1914, 2208, ...17, 3360, 2552, 1963, 117, 2207, 2111, 2094, 102]\n",
      "00:24:48.595572 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3360, 2552, 1963, 3295, 3221, 1914...2552,        1963,  117, 2207, 2111, 2094,  102])\n",
      "00:24:48.595825 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.596572 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.597215 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.598315 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3360, 2552, 1963, 3295, 3221, 191... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008759\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 51\n",
      "00:24:48.600923 call        14     def __getitem__(self, idx):\n",
      "00:24:48.601013 line        15         if self.mode == \"test\":\n",
      "00:24:48.601056 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '陈乔恩携爱犬登封面 放慢脚步享受友爱生活'\n",
      "New var:....... text_b = '爱犬,陈乔恩'\n",
      "New var:....... label = 0\n",
      "00:24:48.601785 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.602308 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.602533 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.602903 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['陈', '乔', '恩', '携', '爱', '犬', '登', '封', '面', '放', '慢', '脚', '步', '享', '受', '友', '爱', '生', '活']\n",
      "00:24:48.603509 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '乔', '恩', '携', '爱', '犬', '登', '封'... '脚', '步', '享', '受', '友', '爱', '生', '活', '[SEP]']\n",
      "00:24:48.603743 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:24:48.603971 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['爱', '犬', ',', '陈', '乔', '恩']\n",
      "00:24:48.604442 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '乔', '恩', '携', '爱', '犬', '登', '封'..., '[SEP]', '爱', '犬', ',', '陈', '乔', '恩', '[SEP]']\n",
      "00:24:48.604682 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 7\n",
      "00:24:48.604913 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7357, 730, 2617, 3025, 4263, 4305, 4633, 2...3833, 102, 4263, 4305, 117, 7357, 730, 2617, 102]\n",
      "00:24:48.605154 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7357,  730, 2617, 3025, 4263, 4305...4263, 4305,  117,        7357,  730, 2617,  102])\n",
      "00:24:48.605404 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.606061 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:24:48.606833 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.607638 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7357,  730, 2617, 3025, 4263, 430...0, 0, 0, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007726\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 52\n",
      "00:24:48.608674 call        14     def __getitem__(self, idx):\n",
      "00:24:48.608742 line        15         if self.mode == \"test\":\n",
      "00:24:48.608779 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '周丽淇出席某活动，网友：她长得很有辨识度看着舒服，保养得很好'\n",
      "New var:....... text_b = '周丽淇,辨识度'\n",
      "New var:....... label = 0\n",
      "00:24:48.609344 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.609517 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.609591 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.609767 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['周', '丽', '淇', '出', '席', '某', '活', '动', '，', '网...'看', '着', '舒', '服', '，', '保', '养', '得', '很', '好']\n",
      "00:24:48.610487 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '周', '丽', '淇', '出', '席', '某', '活', '动'... '舒', '服', '，', '保', '养', '得', '很', '好', '[SEP]']\n",
      "00:24:48.610673 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:48.610855 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['周', '丽', '淇', ',', '辨', '识', '度']\n",
      "00:24:48.611182 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '周', '丽', '淇', '出', '席', '某', '活', '动'...EP]', '周', '丽', '淇', ',', '辨', '识', '度', '[SEP]']\n",
      "00:24:48.611372 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 8\n",
      "00:24:48.611621 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1453, 714, 3899, 1139, 2375, 3378, 3833, 1...102, 1453, 714, 3899, 117, 6795, 6399, 2428, 102]\n",
      "00:24:48.611822 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1453,  714, 3899, 1139, 2375, 3378... 714, 3899,  117,        6795, 6399, 2428,  102])\n",
      "00:24:48.612026 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.612558 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.613269 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.614278 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1453,  714, 3899, 1139, 2375, 337... 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007420\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 53\n",
      "00:24:48.616126 call        14     def __getitem__(self, idx):\n",
      "00:24:48.616198 line        15         if self.mode == \"test\":\n",
      "00:24:48.616235 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '维密超模后台化妆照，展示魅力天使容颜和独特的 风采！'\n",
      "New var:....... text_b = '安布罗,时装周,安布罗休,奥尔德里奇,维密'\n",
      "New var:....... label = 0\n",
      "00:24:48.616803 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.616997 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.617082 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.617270 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['维', '密', '超', '模', '后', '台', '化', '妆', '照', '，...'使', '容', '颜', '和', '独', '特', '的', '风', '采', '！']\n",
      "00:24:48.617927 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '维', '密', '超', '模', '后', '台', '化', '妆'... '颜', '和', '独', '特', '的', '风', '采', '！', '[SEP]']\n",
      "00:24:48.618117 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:24:48.618299 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['安', '布', '罗', ',', '时', '装', '周', ',', '安', '布...'休', ',', '奥', '尔', '德', '里', '奇', ',', '维', '密']\n",
      "00:24:48.618845 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '维', '密', '超', '模', '后', '台', '化', '妆'... '奥', '尔', '德', '里', '奇', ',', '维', '密', '[SEP]']\n",
      "00:24:48.619006 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 22\n",
      "00:24:48.619313 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5335, 2166, 6631, 3563, 1400, 1378, 1265, ...52, 2209, 2548, 7027, 1936, 117, 5335, 2166, 102]\n",
      "00:24:48.619532 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5335, 2166, 6631, 3563, 1400, 1378...2548, 7027, 1936,  117, 5335, 2166,         102])\n",
      "00:24:48.619816 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.620418 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:24:48.621069 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.622097 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5335, 2166, 6631, 3563, 1400, 137...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007964\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 54\n",
      "00:24:48.624121 call        14     def __getitem__(self, idx):\n",
      "00:24:48.624190 line        15         if self.mode == \"test\":\n",
      "00:24:48.624292 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '拿到多位业界大佬投资，她如何在内容和商业教育领域立下壁垒'\n",
      "New var:....... text_b = '毛大庆,吴婷,我有嘉宾,嘉宾派,嘉宾大学,科大讯飞'\n",
      "New var:....... label = 0\n",
      "00:24:48.624931 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.625028 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.625108 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.625297 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['拿', '到', '多', '位', '业', '界', '大', '佬', '投', '资...'商', '业', '教', '育', '领', '域', '立', '下', '壁', '垒']\n",
      "00:24:48.626033 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '拿', '到', '多', '位', '业', '界', '大', '佬'... '教', '育', '领', '域', '立', '下', '壁', '垒', '[SEP]']\n",
      "00:24:48.626222 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:24:48.626502 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['毛', '大', '庆', ',', '吴', '婷', ',', '我', '有', '嘉...',', '嘉', '宾', '大', '学', ',', '科', '大', '讯', '飞']\n",
      "00:24:48.627183 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '拿', '到', '多', '位', '业', '界', '大', '佬'... '宾', '大', '学', ',', '科', '大', '讯', '飞', '[SEP]']\n",
      "00:24:48.627395 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 26\n",
      "00:24:48.627593 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2897, 1168, 1914, 855, 689, 4518, 1920, 87...61, 1920, 2110, 117, 4906, 1920, 6380, 7607, 102]\n",
      "00:24:48.627809 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2897, 1168, 1914,  855,  689, 4518... 1920, 2110,  117, 4906, 1920, 6380, 7607,  102])\n",
      "00:24:48.628028 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.628784 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.629483 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.630543 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2897, 1168, 1914,  855,  689, 451...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008680\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 55\n",
      "00:24:48.632835 call        14     def __getitem__(self, idx):\n",
      "00:24:48.632905 line        15         if self.mode == \"test\":\n",
      "00:24:48.632943 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '贾斯汀比伯和薛之谦可比吗？'\n",
      "New var:....... text_b = 'Justin Bieber,Sorry,What Do You Mean ?,I ’ ll Show You,福布斯,薛之谦,贾斯汀比伯,Purpose,侃爷,FEAR'\n",
      "New var:....... label = 0\n",
      "00:24:48.633535 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.633635 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.633793 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.634101 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['贾', '斯', '汀', '比', '伯', '和', '薛', '之', '谦', '可', '比', '吗', '？']\n",
      "00:24:48.634545 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '贾', '斯', '汀', '比', '伯', '和', '薛', '之', '谦', '可', '比', '吗', '？', '[SEP]']\n",
      "00:24:48.634783 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "00:24:48.634908 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['justin', 'bi', '##e', '##ber', ',', 'sorry', '...##rp', '##ose', ',', '侃', '爷', ',', 'fe', '##ar']\n",
      "00:24:48.635971 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '贾', '斯', '汀', '比', '伯', '和', '薛', '之'...#ose', ',', '侃', '爷', ',', 'fe', '##ar', '[SEP]']\n",
      "00:24:48.636110 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 44\n",
      "00:24:48.636362 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6593, 3172, 3722, 3683, 843, 1469, 5955, 7...80, 10936, 117, 887, 4267, 117, 12605, 8458, 102]\n",
      "00:24:48.636615 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  6593,  3172,  3722,  3683,   843...  117,   887,  4267,   117, 12605,  8458,   102])\n",
      "00:24:48.636912 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.637608 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.638145 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.639490 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  6593,  3172,  3722,  3683,   84...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009441\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 56\n",
      "00:24:48.642318 call        14     def __getitem__(self, idx):\n",
      "00:24:48.642400 line        15         if self.mode == \"test\":\n",
      "00:24:48.642438 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《犬夜叉》里的玲是不是人？'\n",
      "New var:....... text_b = '犬夜叉,杀生丸,日暮戈薇,大结局,天生牙,四魂之玉'\n",
      "New var:....... label = 0\n",
      "00:24:48.643023 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.643117 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.643264 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.643442 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '犬', '夜', '叉', '》', '里', '的', '玲', '是', '不', '是', '人', '？']\n",
      "00:24:48.643885 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '犬', '夜', '叉', '》', '里', '的', '玲', '是', '不', '是', '人', '？', '[SEP]']\n",
      "00:24:48.644118 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "00:24:48.644293 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['犬', '夜', '叉', ',', '杀', '生', '丸', ',', '日', '暮...'局', ',', '天', '生', '牙', ',', '四', '魂', '之', '玉']\n",
      "00:24:48.644986 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '犬', '夜', '叉', '》', '里', '的', '玲'... '天', '生', '牙', ',', '四', '魂', '之', '玉', '[SEP]']\n",
      "00:24:48.645181 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 26\n",
      "00:24:48.645370 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 4305, 1915, 1349, 518, 7027, 4638, 43...921, 4495, 4280, 117, 1724, 7789, 722, 4373, 102]\n",
      "00:24:48.645571 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 4305, 1915, 1349,  518, 7027...4280,  117,        1724, 7789,  722, 4373,  102])\n",
      "00:24:48.645779 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.646366 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.647052 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.647963 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 4305, 1915, 1349,  518, 702... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007644\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 57\n",
      "00:24:48.649995 call        14     def __getitem__(self, idx):\n",
      "00:24:48.650071 line        15         if self.mode == \"test\":\n",
      "00:24:48.650111 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '片场偶遇刘涛新剧造型 网友：太有年代感了'\n",
      "New var:....... text_b = '刘涛,片场,新剧'\n",
      "New var:....... label = 0\n",
      "00:24:48.650625 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.650717 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.650791 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.651030 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['片', '场', '偶', '遇', '刘', '涛', '新', '剧', '造', '型', '网', '友', '：', '太', '有', '年', '代', '感', '了']\n",
      "00:24:48.651604 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '片', '场', '偶', '遇', '刘', '涛', '新', '剧'... '友', '：', '太', '有', '年', '代', '感', '了', '[SEP]']\n",
      "00:24:48.651789 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:24:48.651969 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['刘', '涛', ',', '片', '场', ',', '新', '剧']\n",
      "00:24:48.652346 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '片', '场', '偶', '遇', '刘', '涛', '新', '剧'... '刘', '涛', ',', '片', '场', ',', '新', '剧', '[SEP]']\n",
      "00:24:48.652515 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 9\n",
      "00:24:48.652701 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4275, 1767, 981, 6878, 1155, 3875, 3173, 1...155, 3875, 117, 4275, 1767, 117, 3173, 1196, 102]\n",
      "00:24:48.652896 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4275, 1767,  981, 6878, 1155, 3875... 117,        4275, 1767,  117, 3173, 1196,  102])\n",
      "00:24:48.653099 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.653658 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0, 0, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:24:48.654211 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.655359 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4275, 1767,  981, 6878, 1155, 387...0, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007032\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 58\n",
      "00:24:48.657057 call        14     def __getitem__(self, idx):\n",
      "00:24:48.657127 line        15         if self.mode == \"test\":\n",
      "00:24:48.657165 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '虽然不被大多数人知道，但她是个难得一见的好演员'\n",
      "New var:....... text_b = '万箭穿心,成泰燊,霸王别姬,李宝莉,戏曲,密阳,体验派,颜丙燕,香港大营救'\n",
      "New var:....... label = 0\n",
      "00:24:48.657721 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.657885 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.657961 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.658136 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['虽', '然', '不', '被', '大', '多', '数', '人', '知', '道...'是', '个', '难', '得', '一', '见', '的', '好', '演', '员']\n",
      "00:24:48.658746 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '虽', '然', '不', '被', '大', '多', '数', '人'... '难', '得', '一', '见', '的', '好', '演', '员', '[SEP]']\n",
      "00:24:48.658982 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:24:48.659231 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['万', '箭', '穿', '心', ',', '成', '泰', '燊', ',', '霸...',', '颜', '丙', '燕', ',', '香', '港', '大', '营', '救']\n",
      "00:24:48.660113 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '虽', '然', '不', '被', '大', '多', '数', '人'... '丙', '燕', ',', '香', '港', '大', '营', '救', '[SEP]']\n",
      "00:24:48.660386 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 38\n",
      "00:24:48.660585 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6006, 4197, 679, 6158, 1920, 1914, 3144, 7...88, 4242, 117, 7676, 3949, 1920, 5852, 3131, 102]\n",
      "00:24:48.660801 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6006, 4197,  679, 6158, 1920, 1914... 117, 7676, 3949, 1920,        5852, 3131,  102])\n",
      "00:24:48.661065 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.661797 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.662588 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.663832 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6006, 4197,  679, 6158, 1920, 191... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009317\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 59\n",
      "00:24:48.666404 call        14     def __getitem__(self, idx):\n",
      "00:24:48.666474 line        15         if self.mode == \"test\":\n",
      "00:24:48.666511 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '蒋欣生日“两美”送祝福 但与这两位却没互动'\n",
      "New var:....... text_b = '蒋欣,杨紫,欢乐颂,王子文'\n",
      "New var:....... label = 0\n",
      "00:24:48.667077 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.667168 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.667321 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.667499 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['蒋', '欣', '生', '日', '[UNK]', '两', '美', '[UNK]',...'福', '但', '与', '这', '两', '位', '却', '没', '互', '动']\n",
      "00:24:48.668062 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '蒋', '欣', '生', '日', '[UNK]', '两', '美',... '与', '这', '两', '位', '却', '没', '互', '动', '[SEP]']\n",
      "00:24:48.668250 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 22\n",
      "00:24:48.668469 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['蒋', '欣', ',', '杨', '紫', ',', '欢', '乐', '颂', ',', '王', '子', '文']\n",
      "00:24:48.668900 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '蒋', '欣', '生', '日', '[UNK]', '两', '美',... ',', '欢', '乐', '颂', ',', '王', '子', '文', '[SEP]']\n",
      "00:24:48.669091 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "00:24:48.669277 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5882, 3615, 4495, 3189, 100, 697, 5401, 10...117, 3614, 727, 7563, 117, 4374, 2094, 3152, 102]\n",
      "00:24:48.669539 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5882, 3615, 4495, 3189,  100,  697... 3614,  727, 7563,  117, 4374, 2094, 3152,  102])\n",
      "00:24:48.669744 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.670253 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.670803 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.671792 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5882, 3615, 4495, 3189,  100,  69... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007135\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 60\n",
      "00:24:48.673566 call        14     def __getitem__(self, idx):\n",
      "00:24:48.673632 line        15         if self.mode == \"test\":\n",
      "00:24:48.673669 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '演《破冰者》被称“社会我逗姐” 潘之琳：罗晋大师兄成表演明灯'\n",
      "New var:....... text_b = '破冰者,大师兄,真爱的谎言之破冰者,娘要嫁人,谭逗逗,职场是个技术活,罗晋,老房有喜,潘之琳'\n",
      "New var:....... label = 0\n",
      "00:24:48.674153 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.674354 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.674427 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.674601 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['演', '《', '破', '冰', '者', '》', '被', '称', '[UNK]'...'罗', '晋', '大', '师', '兄', '成', '表', '演', '明', '灯']\n",
      "00:24:48.675311 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '演', '《', '破', '冰', '者', '》', '被', '称'... '大', '师', '兄', '成', '表', '演', '明', '灯', '[SEP]']\n",
      "00:24:48.675498 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:24:48.675680 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['破', '冰', '者', ',', '大', '师', '兄', ',', '真', '爱...'晋', ',', '老', '房', '有', '喜', ',', '潘', '之', '琳']\n",
      "00:24:48.676668 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '演', '《', '破', '冰', '者', '》', '被', '称'... '老', '房', '有', '喜', ',', '潘', '之', '琳', '[SEP]']\n",
      "00:24:48.676969 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 47\n",
      "00:24:48.677164 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4028, 517, 4788, 1102, 5442, 518, 6158, 49...439, 2791, 3300, 1599, 117, 4050, 722, 4432, 102]\n",
      "00:24:48.677383 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4028,  517, 4788, 1102, 5442,  518...3300,        1599,  117, 4050,  722, 4432,  102])\n",
      "00:24:48.677602 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.678375 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:24:48.679386 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.680716 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4028,  517, 4788, 1102, 5442,  51...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010220\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 61\n",
      "00:24:48.683824 call        14     def __getitem__(self, idx):\n",
      "00:24:48.683898 line        15         if self.mode == \"test\":\n",
      "00:24:48.683936 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《非自然死亡》豆瓣评分9.2 国产剧比日剧到底差在哪'\n",
      "New var:....... text_b = '豆瓣,非自然死亡,石原里美'\n",
      "New var:....... label = 0\n",
      "00:24:48.684518 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.684747 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.684837 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.684956 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '非', '自', '然', '死', '亡', '》', '豆', '瓣', '评...'产', '剧', '比', '日', '剧', '到', '底', '差', '在', '哪']\n",
      "00:24:48.685637 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '非', '自', '然', '死', '亡', '》', '豆'... '比', '日', '剧', '到', '底', '差', '在', '哪', '[SEP]']\n",
      "00:24:48.685872 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:24:48.686059 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['豆', '瓣', ',', '非', '自', '然', '死', '亡', ',', '石', '原', '里', '美']\n",
      "00:24:48.686493 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '非', '自', '然', '死', '亡', '》', '豆'... '然', '死', '亡', ',', '石', '原', '里', '美', '[SEP]']\n",
      "00:24:48.686685 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "00:24:48.686940 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 7478, 5632, 4197, 3647, 767, 518, 648...197, 3647, 767, 117, 4767, 1333, 7027, 5401, 102]\n",
      "00:24:48.687143 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 7478, 5632, 4197, 3647,  767... 767,  117,        4767, 1333, 7027, 5401,  102])\n",
      "00:24:48.687351 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.688006 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.688945 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.689919 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 7478, 5632, 4197, 3647,  76... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008303\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 62\n",
      "00:24:48.692159 call        14     def __getitem__(self, idx):\n",
      "00:24:48.692231 line        15         if self.mode == \"test\":\n",
      "00:24:48.692269 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '这两个举动证明：黄圣依在教育孩子方面绝对是把好手'\n",
      "New var:....... text_b = '安迪,妈妈是超人,黄圣依'\n",
      "New var:....... label = 0\n",
      "00:24:48.692816 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.692909 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.692983 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.693205 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['这', '两', '个', '举', '动', '证', '明', '：', '黄', '圣...'孩', '子', '方', '面', '绝', '对', '是', '把', '好', '手']\n",
      "00:24:48.693834 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '两', '个', '举', '动', '证', '明', '：'... '方', '面', '绝', '对', '是', '把', '好', '手', '[SEP]']\n",
      "00:24:48.694021 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:24:48.694286 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['安', '迪', ',', '妈', '妈', '是', '超', '人', ',', '黄', '圣', '依']\n",
      "00:24:48.694731 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '两', '个', '举', '动', '证', '明', '：'... '妈', '是', '超', '人', ',', '黄', '圣', '依', '[SEP]']\n",
      "00:24:48.694931 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 13\n",
      "00:24:48.695122 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6821, 697, 702, 715, 1220, 6395, 3209, 803...1968, 3221, 6631, 782, 117, 7942, 1760, 898, 102]\n",
      "00:24:48.695362 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6821,  697,  702,  715, 1220, 6395...6631,  782,  117, 7942,        1760,  898,  102])\n",
      "00:24:48.695568 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.696102 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:48.696702 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.697607 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6821,  697,  702,  715, 1220, 639... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007377\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 63\n",
      "00:24:48.699563 call        14     def __getitem__(self, idx):\n",
      "00:24:48.699630 line        15         if self.mode == \"test\":\n",
      "00:24:48.699667 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '安吉劈木板，小鱼儿一字马，胡可为啥这么爱晒娃？这回答机智了'\n",
      "New var:....... text_b = '安吉,跆拳道,男子汉,胡可,小鱼儿'\n",
      "New var:....... label = 0\n",
      "00:24:48.700147 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:48.700238 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:48.700308 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:48.700485 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['安', '吉', '劈', '木', '板', '，', '小', '鱼', '儿', '一...'爱', '晒', '娃', '？', '这', '回', '答', '机', '智', '了']\n",
      "00:24:48.701190 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '安', '吉', '劈', '木', '板', '，', '小', '鱼'... '娃', '？', '这', '回', '答', '机', '智', '了', '[SEP]']\n",
      "00:24:48.701375 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:24:48.701594 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['安', '吉', ',', '跆', '拳', '道', ',', '男', '子', '汉', ',', '胡', '可', ',', '小', '鱼', '儿']\n",
      "00:24:48.702159 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '安', '吉', '劈', '木', '板', '，', '小', '鱼'... '汉', ',', '胡', '可', ',', '小', '鱼', '儿', '[SEP]']\n",
      "00:24:48.702352 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:24:48.702540 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2128, 1395, 1207, 3312, 3352, 8024, 2207, ...727, 117, 5529, 1377, 117, 2207, 7824, 1036, 102]\n",
      "00:24:48.702743 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2128, 1395, 1207, 3312, 3352, 8024...5529, 1377,  117, 2207, 7824, 1036,         102])\n",
      "00:24:48.702951 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:48.703538 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:24:48.704238 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:48.705279 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2128, 1395, 1207, 3312, 3352, 802...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007865\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 64\n",
      "00:24:55.015040 call        14     def __getitem__(self, idx):\n",
      "00:24:55.015347 line        15         if self.mode == \"test\":\n",
      "00:24:55.015405 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张柏芝，曾经多少人爱慕你年轻时的容颜'\n",
      "New var:....... text_b = '张柏芝'\n",
      "New var:....... label = 0\n",
      "00:24:55.016025 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.016123 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.016344 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.016694 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '柏', '芝', '，', '曾', '经', '多', '少', '人', '爱', '慕', '你', '年', '轻', '时', '的', '容', '颜']\n",
      "00:24:55.017565 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '柏', '芝', '，', '曾', '经', '多', '少'... '慕', '你', '年', '轻', '时', '的', '容', '颜', '[SEP]']\n",
      "00:24:55.017866 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 20\n",
      "00:24:55.018092 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['张', '柏', '芝']\n",
      "00:24:55.018407 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '柏', '芝', '，', '曾', '经', '多', '少'..., '的', '容', '颜', '[SEP]', '张', '柏', '芝', '[SEP]']\n",
      "00:24:55.018700 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 4\n",
      "00:24:55.019067 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 3377, 5698, 8024, 3295, 5307, 1914, ...98, 4638, 2159, 7582, 102, 2476, 3377, 5698, 102]\n",
      "00:24:55.019319 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476, 3377, 5698, 8024, 3295, 5307... 4638, 2159, 7582,  102, 2476, 3377, 5698,  102])\n",
      "00:24:55.019563 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.020487 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1])\n",
      "00:24:55.021068 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.022048 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2476, 3377, 5698, 8024, 3295, 530... 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009045\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 65\n",
      "00:24:55.024103 call        14     def __getitem__(self, idx):\n",
      "00:24:55.024175 line        15         if self.mode == \"test\":\n",
      "00:24:55.024213 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '古惑仔原班人马新作《黄金兄弟》，世界拳王周比利复出成最大惊喜'\n",
      "New var:....... text_b = '周比利,古惑仔,新宿事件,钱嘉乐,曾志伟,仓田保昭,精武英雄,洪金宝,富贵列车,夏日福星,黄金兄弟,最佳拍档之醉街拍档,煎饼侠,特警新人类,东方秃鹰'\n",
      "New var:....... label = 0\n",
      "00:24:55.024767 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.024860 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.024934 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.025290 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['古', '惑', '仔', '原', '班', '人', '马', '新', '作', '《...'周', '比', '利', '复', '出', '成', '最', '大', '惊', '喜']\n",
      "00:24:55.026160 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '古', '惑', '仔', '原', '班', '人', '马', '新'... '利', '复', '出', '成', '最', '大', '惊', '喜', '[SEP]']\n",
      "00:24:55.026379 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:55.026589 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['周', '比', '利', ',', '古', '惑', '仔', ',', '新', '宿...'特', '警', '新', '人', '类', ',', '东', '方', '秃', '鹰']\n",
      "00:24:55.028084 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '古', '惑', '仔', '原', '班', '人', '马', '新'... '新', '人', '类', ',', '东', '方', '秃', '鹰', '[SEP]']\n",
      "00:24:55.028456 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 75\n",
      "00:24:55.028912 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1367, 2663, 798, 1333, 4408, 782, 7716, 31...3173, 782, 5102, 117, 691, 3175, 4901, 7916, 102]\n",
      "00:24:55.029311 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1367, 2663,  798, 1333, 4408,  782...  782, 5102,  117,  691, 3175, 4901, 7916,  102])\n",
      "00:24:55.029688 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.030787 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.032476 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.034856 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1367, 2663,  798, 1333, 4408,  78...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013408\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 66\n",
      "00:24:55.037548 call        14     def __getitem__(self, idx):\n",
      "00:24:55.037667 line        15         if self.mode == \"test\":\n",
      "00:24:55.037718 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '王美人怀上龙胎，皇后让婕妤娘娘送贺礼，没想到早已打好算盘'\n",
      "New var:....... text_b = '帝锦,王美人,皇后,婕妤,娘娘'\n",
      "New var:....... label = 0\n",
      "00:24:55.038230 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.038324 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.038398 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.038625 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['王', '美', '人', '怀', '上', '龙', '胎', '，', '皇', '后...'，', '没', '想', '到', '早', '已', '打', '好', '算', '盘']\n",
      "00:24:55.039406 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '美', '人', '怀', '上', '龙', '胎', '，'... '想', '到', '早', '已', '打', '好', '算', '盘', '[SEP]']\n",
      "00:24:55.039869 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:24:55.040299 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['帝', '锦', ',', '王', '美', '人', ',', '皇', '后', ',', '婕', '妤', ',', '娘', '娘']\n",
      "00:24:55.040822 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '美', '人', '怀', '上', '龙', '胎', '，'... '皇', '后', ',', '婕', '妤', ',', '娘', '娘', '[SEP]']\n",
      "00:24:55.041050 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 16\n",
      "00:24:55.041311 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4374, 5401, 782, 2577, 677, 7987, 5522, 80...640, 1400, 117, 2041, 1979, 117, 2023, 2023, 102]\n",
      "00:24:55.041683 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4374, 5401,  782, 2577,  677, 7987... 1400,  117, 2041, 1979,  117, 2023, 2023,  102])\n",
      "00:24:55.042229 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.043015 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.043776 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.045504 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4374, 5401,  782, 2577,  677, 798... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009439\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 67\n",
      "00:24:55.047019 call        14     def __getitem__(self, idx):\n",
      "00:24:55.047097 line        15         if self.mode == \"test\":\n",
      "00:24:55.047138 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '一年一度的影迷狂欢：戛纳！'\n",
      "New var:....... text_b = '戛纳电影节,三张面孔,电影节,绿洲,白气球,无人知晓,过往,戛纳,李沧东'\n",
      "New var:....... label = 0\n",
      "00:24:55.047662 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.047859 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.047943 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.048144 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['一', '年', '一', '度', '的', '影', '迷', '狂', '欢', '：', '戛', '纳', '！']\n",
      "00:24:55.048608 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '一', '年', '一', '度', '的', '影', '迷', '狂', '欢', '：', '戛', '纳', '！', '[SEP]']\n",
      "00:24:55.048939 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "00:24:55.049178 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['戛', '纳', '电', '影', '节', ',', '三', '张', '面', '孔...',', '过', '往', ',', '戛', '纳', ',', '李', '沧', '东']\n",
      "00:24:55.050234 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '一', '年', '一', '度', '的', '影', '迷', '狂'... '往', ',', '戛', '纳', ',', '李', '沧', '东', '[SEP]']\n",
      "00:24:55.050676 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 37\n",
      "00:24:55.050908 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 671, 2399, 671, 2428, 4638, 2512, 6837, 43...2518, 117, 2775, 5287, 117, 3330, 3771, 691, 102]\n",
      "00:24:55.051142 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  671, 2399,  671, 2428, 4638, 2512...2775, 5287,  117,        3330, 3771,  691,  102])\n",
      "00:24:55.051418 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.052454 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:24:55.053752 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.055138 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  671, 2399,  671, 2428, 4638, 251...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009545\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 68\n",
      "00:24:55.056601 call        14     def __getitem__(self, idx):\n",
      "00:24:55.056674 line        15         if self.mode == \"test\":\n",
      "00:24:55.056712 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《温暖的弦》张翰不再承包鱼塘了，把观众给承包了！'\n",
      "New var:....... text_b = '塘主,张翰,张钧甯,霸道总裁,中国合伙人,大汉天子,黄晓明'\n",
      "New var:....... label = 0\n",
      "00:24:55.057222 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.057314 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.057387 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.057839 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '温', '暖', '的', '弦', '》', '张', '翰', '不', '再...'了', '，', '把', '观', '众', '给', '承', '包', '了', '！']\n",
      "00:24:55.058618 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '温', '暖', '的', '弦', '》', '张', '翰'... '把', '观', '众', '给', '承', '包', '了', '！', '[SEP]']\n",
      "00:24:55.058836 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:24:55.059166 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['塘', '主', ',', '张', '翰', ',', '张', '钧', '甯', ',...'人', ',', '大', '汉', '天', '子', ',', '黄', '晓', '明']\n",
      "00:24:55.059903 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '温', '暖', '的', '弦', '》', '张', '翰'... '大', '汉', '天', '子', ',', '黄', '晓', '明', '[SEP]']\n",
      "00:24:55.060440 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 30\n",
      "00:24:55.060669 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 3946, 3265, 4638, 2478, 518, 2476, 54...20, 3727, 1921, 2094, 117, 7942, 3236, 3209, 102]\n",
      "00:24:55.060916 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 3946, 3265, 4638, 2478,  518... 3727, 1921, 2094,  117, 7942, 3236, 3209,  102])\n",
      "00:24:55.061206 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.062366 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.063402 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.064713 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 3946, 3265, 4638, 2478,  51...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009697\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 69\n",
      "00:24:55.066329 call        14     def __getitem__(self, idx):\n",
      "00:24:55.066402 line        15         if self.mode == \"test\":\n",
      "00:24:55.066440 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张钧甯隔7年感叹好多白发 修杰楷：老艺人的悲哀'\n",
      "New var:....... text_b = '修杰楷,白色巨塔,TVBS,真心请按两次铃,痞子英雄,陈奕,何润东,张钧甯'\n",
      "New var:....... label = 0\n",
      "00:24:55.066934 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.067026 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.067100 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.067521 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '钧', '甯', '隔', '7', '年', '感', '叹', '好', '多...'修', '杰', '楷', '：', '老', '艺', '人', '的', '悲', '哀']\n",
      "00:24:55.068223 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '钧', '甯', '隔', '7', '年', '感', '叹'... '楷', '：', '老', '艺', '人', '的', '悲', '哀', '[SEP]']\n",
      "00:24:55.068540 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:24:55.068745 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['修', '杰', '楷', ',', '白', '色', '巨', '塔', ',', 't...'陈', '奕', ',', '何', '润', '东', ',', '张', '钧', '甯']\n",
      "00:24:55.069711 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '钧', '甯', '隔', '7', '年', '感', '叹'... ',', '何', '润', '东', ',', '张', '钧', '甯', '[SEP]']\n",
      "00:24:55.070158 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 35\n",
      "00:24:55.070399 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 7172, 4505, 7392, 128, 2399, 2697, 1... 117, 862, 3883, 691, 117, 2476, 7172, 4505, 102]\n",
      "00:24:55.070669 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  2476,  7172,  4505,  7392,   128... 3883,   691,   117,  2476,  7172,  4505,   102])\n",
      "00:24:55.071152 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.072491 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.073333 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.074884 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  2476,  7172,  4505,  7392,   12...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010289\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 70\n",
      "00:24:55.076647 call        14     def __getitem__(self, idx):\n",
      "00:24:55.076719 line        15         if self.mode == \"test\":\n",
      "00:24:55.076756 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '邱淑贞出席某活动 网友：一袭黑衣，这是又要去做大姐大吗？'\n",
      "New var:....... text_b = '活动网友,做大姐大,邱淑贞,一袭黑衣,邱淑贞出席'\n",
      "New var:....... label = 0\n",
      "00:24:55.077245 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.077445 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.077524 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.077731 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['邱', '淑', '贞', '出', '席', '某', '活', '动', '网', '友...'是', '又', '要', '去', '做', '大', '姐', '大', '吗', '？']\n",
      "00:24:55.078450 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '邱', '淑', '贞', '出', '席', '某', '活', '动'... '要', '去', '做', '大', '姐', '大', '吗', '？', '[SEP]']\n",
      "00:24:55.078688 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:24:55.078898 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['活', '动', '网', '友', ',', '做', '大', '姐', '大', ',...'一', '袭', '黑', '衣', ',', '邱', '淑', '贞', '出', '席']\n",
      "00:24:55.079673 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '邱', '淑', '贞', '出', '席', '某', '活', '动'... '黑', '衣', ',', '邱', '淑', '贞', '出', '席', '[SEP]']\n",
      "00:24:55.080211 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 25\n",
      "00:24:55.080462 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6937, 3902, 6565, 1139, 2375, 3378, 3833, ...46, 6132, 117, 6937, 3902, 6565, 1139, 2375, 102]\n",
      "00:24:55.080718 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6937, 3902, 6565, 1139, 2375, 3378... 117,        6937, 3902, 6565, 1139, 2375,  102])\n",
      "00:24:55.080971 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.081991 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:24:55.082954 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.084964 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6937, 3902, 6565, 1139, 2375, 337...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010158\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 71\n",
      "00:24:55.086839 call        14     def __getitem__(self, idx):\n",
      "00:24:55.086915 line        15         if self.mode == \"test\":\n",
      "00:24:55.086954 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《我是你妈》首映 闫妮邹元清母女现场落泪'\n",
      "New var:....... text_b = '轻喜剧,电影,闫妮,母女,陈楚生,邹元清'\n",
      "New var:....... label = 0\n",
      "00:24:55.087519 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.087613 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.087691 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.088145 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '我', '是', '你', '妈', '》', '首', '映', '闫', '妮', '邹', '元', '清', '母', '女', '现', '场', '落', '泪']\n",
      "00:24:55.088944 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '我', '是', '你', '妈', '》', '首', '映'... '元', '清', '母', '女', '现', '场', '落', '泪', '[SEP]']\n",
      "00:24:55.089483 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:24:55.089727 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['轻', '喜', '剧', ',', '电', '影', ',', '闫', '妮', ',', '母', '女', ',', '陈', '楚', '生', ',', '邹', '元', '清']\n",
      "00:24:55.090327 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '我', '是', '你', '妈', '》', '首', '映'... ',', '陈', '楚', '生', ',', '邹', '元', '清', '[SEP]']\n",
      "00:24:55.090945 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:24:55.091496 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 2769, 3221, 872, 1968, 518, 7674, 321...17, 7357, 3504, 4495, 117, 6941, 1039, 3926, 102]\n",
      "00:24:55.091733 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 2769, 3221,  872, 1968,  518...3504,        4495,  117, 6941, 1039, 3926,  102])\n",
      "00:24:55.092089 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.092729 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.093894 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.094561 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 2769, 3221,  872, 1968,  51... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009184\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 72\n",
      "00:24:55.096147 call        14     def __getitem__(self, idx):\n",
      "00:24:55.096226 line        15         if self.mode == \"test\":\n",
      "00:24:55.096263 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '钟汉良更博，爱她，请告诉她！期待程天佑早日上线'\n",
      "New var:....... text_b = '钟汉良,节日快乐,湖南卫视'\n",
      "New var:....... label = 0\n",
      "00:24:55.096758 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.096850 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.096922 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.097274 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['钟', '汉', '良', '更', '博', '，', '爱', '她', '，', '请...'！', '期', '待', '程', '天', '佑', '早', '日', '上', '线']\n",
      "00:24:55.097976 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '钟', '汉', '良', '更', '博', '，', '爱', '她'... '待', '程', '天', '佑', '早', '日', '上', '线', '[SEP]']\n",
      "00:24:55.098383 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:24:55.098697 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['钟', '汉', '良', ',', '节', '日', '快', '乐', ',', '湖', '南', '卫', '视']\n",
      "00:24:55.099504 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '钟', '汉', '良', '更', '博', '，', '爱', '她'... '日', '快', '乐', ',', '湖', '南', '卫', '视', '[SEP]']\n",
      "00:24:55.099871 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "00:24:55.100090 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7164, 3727, 5679, 3291, 1300, 8024, 4263, ...189, 2571, 727, 117, 3959, 1298, 1310, 6228, 102]\n",
      "00:24:55.100322 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7164, 3727, 5679, 3291, 1300, 8024... 727,  117, 3959, 1298,        1310, 6228,  102])\n",
      "00:24:55.100860 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.101710 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.102594 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.103793 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7164, 3727, 5679, 3291, 1300, 802... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008827\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 73\n",
      "00:24:55.105013 call        14     def __getitem__(self, idx):\n",
      "00:24:55.105097 line        15         if self.mode == \"test\":\n",
      "00:24:55.105140 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '谭松韵，俏皮丸子头演绎明朗少女，展现十足青春感'\n",
      "New var:....... text_b = '俏皮丸子头演绎明朗少女,少女,谭松韵,展现十足青春感谭松韵,丸子头'\n",
      "New var:....... label = 0\n",
      "00:24:55.105639 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.105833 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.105923 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.106136 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['谭', '松', '韵', '，', '俏', '皮', '丸', '子', '头', '演...'少', '女', '，', '展', '现', '十', '足', '青', '春', '感']\n",
      "00:24:55.106871 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '谭', '松', '韵', '，', '俏', '皮', '丸', '子'... '，', '展', '现', '十', '足', '青', '春', '感', '[SEP]']\n",
      "00:24:55.107240 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:24:55.107583 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['俏', '皮', '丸', '子', '头', '演', '绎', '明', '朗', '少...'青', '春', '感', '谭', '松', '韵', ',', '丸', '子', '头']\n",
      "00:24:55.108599 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '谭', '松', '韵', '，', '俏', '皮', '丸', '子'... '感', '谭', '松', '韵', ',', '丸', '子', '头', '[SEP]']\n",
      "00:24:55.108924 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 34\n",
      "00:24:55.109268 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6478, 3351, 7510, 8024, 918, 4649, 709, 20...697, 6478, 3351, 7510, 117, 709, 2094, 1928, 102]\n",
      "00:24:55.109838 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6478, 3351, 7510, 8024,  918, 4649... 6478, 3351, 7510,  117,  709, 2094, 1928,  102])\n",
      "00:24:55.110198 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.111186 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.112601 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.113333 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6478, 3351, 7510, 8024,  918, 464...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010003\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 74\n",
      "00:24:55.115046 call        14     def __getitem__(self, idx):\n",
      "00:24:55.115123 line        15         if self.mode == \"test\":\n",
      "00:24:55.115165 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '奚梦瑶获四太梁安琪认可，与何猷君全家游曼谷，全程跟在四太身后'\n",
      "New var:....... text_b = '超级名模,何猷启,奚梦瑶,何超盈,何猷君'\n",
      "New var:....... label = 0\n",
      "00:24:55.115655 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.115754 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.115833 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.116072 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['奚', '梦', '瑶', '获', '四', '太', '梁', '安', '琪', '认...'谷', '，', '全', '程', '跟', '在', '四', '太', '身', '后']\n",
      "00:24:55.116836 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '奚', '梦', '瑶', '获', '四', '太', '梁', '安'... '全', '程', '跟', '在', '四', '太', '身', '后', '[SEP]']\n",
      "00:24:55.117054 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:55.117480 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['超', '级', '名', '模', ',', '何', '猷', '启', ',', '奚', '梦', '瑶', ',', '何', '超', '盈', ',', '何', '猷', '君']\n",
      "00:24:55.118179 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '奚', '梦', '瑶', '获', '四', '太', '梁', '安'... ',', '何', '超', '盈', ',', '何', '猷', '君', '[SEP]']\n",
      "00:24:55.118527 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:24:55.118935 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1949, 3457, 4457, 5815, 1724, 1922, 3448, ... 117, 862, 6631, 4659, 117, 862, 4349, 1409, 102]\n",
      "00:24:55.119216 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1949, 3457, 4457, 5815, 1724, 1922...6631, 4659,         117,  862, 4349, 1409,  102])\n",
      "00:24:55.119446 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.120903 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1])\n",
      "00:24:55.121751 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.123431 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1949, 3457, 4457, 5815, 1724, 192...1, 1, 1, 1, 1,        1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009820\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 75\n",
      "00:24:55.124994 call        14     def __getitem__(self, idx):\n",
      "00:24:55.125076 line        15         if self.mode == \"test\":\n",
      "00:24:55.125115 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '大神问答，张檬整容？房祖名追周冬雨？'\n",
      "New var:....... text_b = '井色,美人心计,井柏然,房祖名,周冬雨'\n",
      "New var:....... label = 0\n",
      "00:24:55.125601 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.125808 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.125886 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.126115 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['大', '神', '问', '答', '，', '张', '檬', '整', '容', '？', '房', '祖', '名', '追', '周', '冬', '雨', '？']\n",
      "00:24:55.126802 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '大', '神', '问', '答', '，', '张', '檬', '整'... '房', '祖', '名', '追', '周', '冬', '雨', '？', '[SEP]']\n",
      "00:24:55.127132 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 20\n",
      "00:24:55.127336 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['井', '色', ',', '美', '人', '心', '计', ',', '井', '柏', '然', ',', '房', '祖', '名', ',', '周', '冬', '雨']\n",
      "00:24:55.127906 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '大', '神', '问', '答', '，', '张', '檬', '整'... ',', '房', '祖', '名', ',', '周', '冬', '雨', '[SEP]']\n",
      "00:24:55.128305 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:24:55.128554 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1920, 4868, 7309, 5031, 8024, 2476, 3597, ...17, 2791, 4862, 1399, 117, 1453, 1100, 7433, 102]\n",
      "00:24:55.128802 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1920, 4868, 7309, 5031, 8024, 2476...4862, 1399,  117,        1453, 1100, 7433,  102])\n",
      "00:24:55.129145 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.130406 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.131095 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.132329 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1920, 4868, 7309, 5031, 8024, 247... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008546\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 76\n",
      "00:24:55.133566 call        14     def __getitem__(self, idx):\n",
      "00:24:55.133634 line        15         if self.mode == \"test\":\n",
      "00:24:55.133670 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '季冬阳嫉妒发狂！展颜：要不爱我，要不放弃我！难道你都做不到'\n",
      "New var:....... text_b = '季冬阳,展颜'\n",
      "New var:....... label = 0\n",
      "00:24:55.134162 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.134253 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.134325 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.134551 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['季', '冬', '阳', '嫉', '妒', '发', '狂', '！', '展', '颜...'弃', '我', '！', '难', '道', '你', '都', '做', '不', '到']\n",
      "00:24:55.135504 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '季', '冬', '阳', '嫉', '妒', '发', '狂', '！'... '！', '难', '道', '你', '都', '做', '不', '到', '[SEP]']\n",
      "00:24:55.135943 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:24:55.136178 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['季', '冬', '阳', ',', '展', '颜']\n",
      "00:24:55.136540 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '季', '冬', '阳', '嫉', '妒', '发', '狂', '！'..., '[SEP]', '季', '冬', '阳', ',', '展', '颜', '[SEP]']\n",
      "00:24:55.136771 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 7\n",
      "00:24:55.137094 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2108, 1100, 7345, 2065, 1971, 1355, 4312, ...168, 102, 2108, 1100, 7345, 117, 2245, 7582, 102]\n",
      "00:24:55.137445 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2108, 1100, 7345, 2065, 1971, 1355...2108, 1100, 7345,  117, 2245,        7582,  102])\n",
      "00:24:55.137690 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.138565 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.139420 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.140692 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2108, 1100, 7345, 2065, 1971, 135... 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008724\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 77\n",
      "00:24:55.142321 call        14     def __getitem__(self, idx):\n",
      "00:24:55.142398 line        15         if self.mode == \"test\":\n",
      "00:24:55.142439 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '华晨宇《嘿！好样的》演奏《我的滑板鞋》'\n",
      "New var:....... text_b = '华晨宇见面,歌手,李浩菲,王荣麟,主持新生,湖南卫视'\n",
      "New var:....... label = 0\n",
      "00:24:55.142926 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.143134 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.143213 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.143621 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['华', '晨', '宇', '《', '嘿', '！', '好', '样', '的', '》', '演', '奏', '《', '我', '的', '滑', '板', '鞋', '》']\n",
      "00:24:55.144176 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '华', '晨', '宇', '《', '嘿', '！', '好', '样'... '奏', '《', '我', '的', '滑', '板', '鞋', '》', '[SEP]']\n",
      "00:24:55.144476 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:24:55.144689 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['华', '晨', '宇', '见', '面', ',', '歌', '手', ',', '李...',', '主', '持', '新', '生', ',', '湖', '南', '卫', '视']\n",
      "00:24:55.145379 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '华', '晨', '宇', '《', '嘿', '！', '好', '样'... '持', '新', '生', ',', '湖', '南', '卫', '视', '[SEP]']\n",
      "00:24:55.145673 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 27\n",
      "00:24:55.146011 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1290, 3247, 2126, 517, 1678, 8013, 1962, 3...98, 3173, 4495, 117, 3959, 1298, 1310, 6228, 102]\n",
      "00:24:55.146351 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1290, 3247, 2126,  517, 1678, 8013... 3173, 4495,  117, 3959, 1298, 1310, 6228,  102])\n",
      "00:24:55.146841 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.147880 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.148762 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.150476 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1290, 3247, 2126,  517, 1678, 801... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009684\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 78\n",
      "00:24:55.152050 call        14     def __getitem__(self, idx):\n",
      "00:24:55.152142 line        15         if self.mode == \"test\":\n",
      "00:24:55.152184 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '邓超手伤未愈仍与鹿晗聚餐，深情对望画风搞笑，二人饭量惊人'\n",
      "New var:....... text_b = '父子情深,邓超,社交网站,鹿晗,父子'\n",
      "New var:....... label = 0\n",
      "00:24:55.152742 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.152973 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.153066 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.153297 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['邓', '超', '手', '伤', '未', '愈', '仍', '与', '鹿', '晗...'风', '搞', '笑', '，', '二', '人', '饭', '量', '惊', '人']\n",
      "00:24:55.154065 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '邓', '超', '手', '伤', '未', '愈', '仍', '与'... '笑', '，', '二', '人', '饭', '量', '惊', '人', '[SEP]']\n",
      "00:24:55.154313 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:24:55.154750 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['父', '子', '情', '深', ',', '邓', '超', ',', '社', '交', '网', '站', ',', '鹿', '晗', ',', '父', '子']\n",
      "00:24:55.155490 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '邓', '超', '手', '伤', '未', '愈', '仍', '与'... '网', '站', ',', '鹿', '晗', ',', '父', '子', '[SEP]']\n",
      "00:24:55.155897 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:24:55.156328 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6924, 6631, 2797, 839, 3313, 2689, 793, 68...381, 4991, 117, 7922, 3240, 117, 4266, 2094, 102]\n",
      "00:24:55.156578 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6924, 6631, 2797,  839, 3313, 2689... 117, 7922, 3240,  117, 4266, 2094,         102])\n",
      "00:24:55.156857 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.157846 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:24:55.158781 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.160297 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6924, 6631, 2797,  839, 3313, 268...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009625\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 79\n",
      "00:24:55.161706 call        14     def __getitem__(self, idx):\n",
      "00:24:55.161778 line        15         if self.mode == \"test\":\n",
      "00:24:55.161816 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '她们都曾出演过同一个经典角色，谁是您最认可的呢？'\n",
      "New var:....... text_b = '刘涛,黄圣依,白娘子,贤妻良母,赵雅芝'\n",
      "New var:....... label = 0\n",
      "00:24:55.162332 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.162525 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.162604 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.162925 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['她', '们', '都', '曾', '出', '演', '过', '同', '一', '个...'，', '谁', '是', '您', '最', '认', '可', '的', '呢', '？']\n",
      "00:24:55.163809 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '她', '们', '都', '曾', '出', '演', '过', '同'... '是', '您', '最', '认', '可', '的', '呢', '？', '[SEP]']\n",
      "00:24:55.164038 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:24:55.164345 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['刘', '涛', ',', '黄', '圣', '依', ',', '白', '娘', '子', ',', '贤', '妻', '良', '母', ',', '赵', '雅', '芝']\n",
      "00:24:55.165053 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '她', '们', '都', '曾', '出', '演', '过', '同'... '贤', '妻', '良', '母', ',', '赵', '雅', '芝', '[SEP]']\n",
      "00:24:55.165287 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:24:55.165629 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1961, 812, 6963, 3295, 1139, 4028, 6814, 1...70, 1988, 5679, 3678, 117, 6627, 7414, 5698, 102]\n",
      "00:24:55.165901 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1961,  812, 6963, 3295, 1139, 4028... 1988, 5679, 3678,  117, 6627, 7414, 5698,  102])\n",
      "00:24:55.166150 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.167159 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.167831 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.169368 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1961,  812, 6963, 3295, 1139, 402... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009449\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 80\n",
      "00:24:55.171185 call        14     def __getitem__(self, idx):\n",
      "00:24:55.171254 line        15         if self.mode == \"test\":\n",
      "00:24:55.171291 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '高晓松为好友微博高调征婚，49岁林海也落俗，成功牵手95后'\n",
      "New var:....... text_b = '中央音乐学院,林海,张亚东,征婚,高晓松'\n",
      "New var:....... label = 0\n",
      "00:24:55.171781 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.171979 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.172055 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.172389 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['高', '晓', '松', '为', '好', '友', '微', '博', '高', '调...也', '落', '俗', '，', '成', '功', '牵', '手', '95', '后']\n",
      "00:24:55.173260 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '高', '晓', '松', '为', '好', '友', '微', '博'...'俗', '，', '成', '功', '牵', '手', '95', '后', '[SEP]']\n",
      "00:24:55.173483 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:24:55.173810 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['中', '央', '音', '乐', '学', '院', ',', '林', '海', ',', '张', '亚', '东', ',', '征', '婚', ',', '高', '晓', '松']\n",
      "00:24:55.174549 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '高', '晓', '松', '为', '好', '友', '微', '博'... '东', ',', '征', '婚', ',', '高', '晓', '松', '[SEP]']\n",
      "00:24:55.174850 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:24:55.175085 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7770, 3236, 3351, 711, 1962, 1351, 2544, 1...691, 117, 2519, 2042, 117, 7770, 3236, 3351, 102]\n",
      "00:24:55.175338 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7770, 3236, 3351,  711, 1962, 1351...2519, 2042,  117, 7770, 3236,        3351,  102])\n",
      "00:24:55.175811 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.176739 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:24:55.178059 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.179416 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7770, 3236, 3351,  711, 1962, 135...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009650\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 81\n",
      "00:24:55.180866 call        14     def __getitem__(self, idx):\n",
      "00:24:55.180944 line        15         if self.mode == \"test\":\n",
      "00:24:55.180984 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '跑男遭遇最强挑战，baby李晨郑恺所有人都哭成一团，太心疼了'\n",
      "New var:....... text_b = '兄弟团,划龙舟,武大靖,龙舟赛,东方IC'\n",
      "New var:....... label = 0\n",
      "00:24:55.181493 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.181695 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.181776 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.182114 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['跑', '男', '遭', '遇', '最', '强', '挑', '战', '，', 'b...'都', '哭', '成', '一', '团', '，', '太', '心', '疼', '了']\n",
      "00:24:55.183074 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '跑', '男', '遭', '遇', '最', '强', '挑', '战'... '成', '一', '团', '，', '太', '心', '疼', '了', '[SEP]']\n",
      "00:24:55.183308 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:24:55.183525 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['兄', '弟', '团', ',', '划', '龙', '舟', ',', '武', '大', '靖', ',', '龙', '舟', '赛', ',', '东', '方', 'ic']\n",
      "00:24:55.184318 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '跑', '男', '遭', '遇', '最', '强', '挑', '战'...',', '龙', '舟', '赛', ',', '东', '方', 'ic', '[SEP]']\n",
      "00:24:55.184546 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:24:55.184782 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6651, 4511, 6901, 6878, 3297, 2487, 2904, ...117, 7987, 5660, 6612, 117, 691, 3175, 8577, 102]\n",
      "00:24:55.185084 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6651, 4511, 6901, 6878, 3297, 2487...5660, 6612,  117,  691, 3175, 8577,         102])\n",
      "00:24:55.185623 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.187164 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:24:55.187937 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.189163 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6651, 4511, 6901, 6878, 3297, 248...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010260\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 82\n",
      "00:24:55.191154 call        14     def __getitem__(self, idx):\n",
      "00:24:55.191290 line        15         if self.mode == \"test\":\n",
      "00:24:55.191333 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '景甜又晒浴室照，却意外印证早与张继科同居！网友：运动员就是快'\n",
      "New var:....... text_b = '网友发现,娱乐圈,景甜,运动员,张继科'\n",
      "New var:....... label = 0\n",
      "00:24:55.191815 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.191907 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.191980 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.192159 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['景', '甜', '又', '晒', '浴', '室', '照', '，', '却', '意...'！', '网', '友', '：', '运', '动', '员', '就', '是', '快']\n",
      "00:24:55.192883 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '景', '甜', '又', '晒', '浴', '室', '照', '，'... '友', '：', '运', '动', '员', '就', '是', '快', '[SEP]']\n",
      "00:24:55.193121 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:55.193303 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['网', '友', '发', '现', ',', '娱', '乐', '圈', ',', '景', '甜', ',', '运', '动', '员', ',', '张', '继', '科']\n",
      "00:24:55.193903 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '景', '甜', '又', '晒', '浴', '室', '照', '，'... ',', '运', '动', '员', ',', '张', '继', '科', '[SEP]']\n",
      "00:24:55.194099 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:24:55.194289 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3250, 4494, 1348, 3235, 3861, 2147, 4212, ...17, 6817, 1220, 1447, 117, 2476, 5326, 4906, 102]\n",
      "00:24:55.194645 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3250, 4494, 1348, 3235, 3861, 2147...1220, 1447,  117,        2476, 5326, 4906,  102])\n",
      "00:24:55.194877 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.195531 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:24:55.196223 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.197259 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3250, 4494, 1348, 3235, 3861, 214...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008446\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 83\n",
      "00:24:55.199627 call        14     def __getitem__(self, idx):\n",
      "00:24:55.199696 line        15         if self.mode == \"test\":\n",
      "00:24:55.199734 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '让两个亿万富翁破产的章小蕙，到底有着怎样的金钱观？'\n",
      "New var:....... text_b = '桃色,亿万富翁,金钱观,陈曜旻,钟镇涛,章小蕙'\n",
      "New var:....... label = 0\n",
      "00:24:55.200205 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.200297 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.200369 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.200549 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['让', '两', '个', '亿', '万', '富', '翁', '破', '产', '的...'底', '有', '着', '怎', '样', '的', '金', '钱', '观', '？']\n",
      "00:24:55.201249 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '让', '两', '个', '亿', '万', '富', '翁', '破'... '着', '怎', '样', '的', '金', '钱', '观', '？', '[SEP]']\n",
      "00:24:55.201436 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:24:55.201662 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['桃', '色', ',', '亿', '万', '富', '翁', ',', '金', '钱...'曜', '旻', ',', '钟', '镇', '涛', ',', '章', '小', '蕙']\n",
      "00:24:55.202266 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '让', '两', '个', '亿', '万', '富', '翁', '破'... ',', '钟', '镇', '涛', ',', '章', '小', '蕙', '[SEP]']\n",
      "00:24:55.202461 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "00:24:55.202652 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6375, 697, 702, 783, 674, 2168, 5419, 4788...17, 7164, 7252, 3875, 117, 4995, 2207, 5936, 102]\n",
      "00:24:55.202856 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6375,  697,  702,  783,  674, 2168...7252, 3875,  117, 4995,        2207, 5936,  102])\n",
      "00:24:55.203131 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.204209 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:24:55.204953 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.206005 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6375,  697,  702,  783,  674, 216...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008562\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 84\n",
      "00:24:55.208221 call        14     def __getitem__(self, idx):\n",
      "00:24:55.208292 line        15         if self.mode == \"test\":\n",
      "00:24:55.208329 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '37岁余文乐喜当爹，用7字表达对娇妻王棠云谢意'\n",
      "New var:....... text_b = '陈建州,王棠云,黑人陈建州,舒淇,余文乐'\n",
      "New var:....... label = 0\n",
      "00:24:55.208900 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.208990 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.209065 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.209283 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['37', '岁', '余', '文', '乐', '喜', '当', '爹', '，', '...'表', '达', '对', '娇', '妻', '王', '棠', '云', '谢', '意']\n",
      "00:24:55.209888 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '37', '岁', '余', '文', '乐', '喜', '当', '爹... '对', '娇', '妻', '王', '棠', '云', '谢', '意', '[SEP]']\n",
      "00:24:55.210074 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:24:55.210256 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['陈', '建', '州', ',', '王', '棠', '云', ',', '黑', '人', '陈', '建', '州', ',', '舒', '淇', ',', '余', '文', '乐']\n",
      "00:24:55.210910 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '37', '岁', '余', '文', '乐', '喜', '当', '爹... '州', ',', '舒', '淇', ',', '余', '文', '乐', '[SEP]']\n",
      "00:24:55.211129 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:24:55.211366 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 8234, 2259, 865, 3152, 727, 1599, 2496, 42... 2336, 117, 5653, 3899, 117, 865, 3152, 727, 102]\n",
      "00:24:55.211574 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 8234, 2259,  865, 3152,  727, 1599...  117, 5653, 3899,  117,  865, 3152,  727,  102])\n",
      "00:24:55.211786 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.212423 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.213083 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.214193 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 8234, 2259,  865, 3152,  727, 159... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008078\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 85\n",
      "00:24:55.216326 call        14     def __getitem__(self, idx):\n",
      "00:24:55.216395 line        15         if self.mode == \"test\":\n",
      "00:24:55.216432 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '画中人！王丽坤戛纳古堡行 气质灵动温柔'\n",
      "New var:....... text_b = '戛纳,王丽坤,画中人'\n",
      "New var:....... label = 0\n",
      "00:24:55.216921 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.217013 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.217087 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.217352 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['画', '中', '人', '！', '王', '丽', '坤', '戛', '纳', '古', '堡', '行', '气', '质', '灵', '动', '温', '柔']\n",
      "00:24:55.218326 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '画', '中', '人', '！', '王', '丽', '坤', '戛'... '堡', '行', '气', '质', '灵', '动', '温', '柔', '[SEP]']\n",
      "00:24:55.218482 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 20\n",
      "00:24:55.218758 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['戛', '纳', ',', '王', '丽', '坤', ',', '画', '中', '人']\n",
      "00:24:55.219139 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '画', '中', '人', '！', '王', '丽', '坤', '戛'... ',', '王', '丽', '坤', ',', '画', '中', '人', '[SEP]']\n",
      "00:24:55.219268 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 11\n",
      "00:24:55.219497 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4514, 704, 782, 8013, 4374, 714, 1787, 277..., 117, 4374, 714, 1787, 117, 4514, 704, 782, 102]\n",
      "00:24:55.219932 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4514,  704,  782, 8013, 4374,  714...        714, 1787,  117, 4514,  704,  782,  102])\n",
      "00:24:55.220201 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.221009 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.221632 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.222901 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4514,  704,  782, 8013, 4374,  71...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009070\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 86\n",
      "00:24:55.225437 call        14     def __getitem__(self, idx):\n",
      "00:24:55.225516 line        15         if self.mode == \"test\":\n",
      "00:24:55.225556 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '应该保密的事情，结果刘宪华还是说漏嘴，何炅再三提醒都没用！'\n",
      "New var:....... text_b = '极限挑战,跑男,刘宪华,何炅,张杰,金玟岐,谢娜,这！就是街舞'\n",
      "New var:....... label = 0\n",
      "00:24:55.226154 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.226255 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.226469 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.226715 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['应', '该', '保', '密', '的', '事', '情', '，', '结', '果...'何', '炅', '再', '三', '提', '醒', '都', '没', '用', '！']\n",
      "00:24:55.227403 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '应', '该', '保', '密', '的', '事', '情', '，'... '再', '三', '提', '醒', '都', '没', '用', '！', '[SEP]']\n",
      "00:24:55.227726 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:24:55.227958 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['极', '限', '挑', '战', ',', '跑', '男', ',', '刘', '宪...',', '谢', '娜', ',', '这', '！', '就', '是', '街', '舞']\n",
      "00:24:55.228743 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '应', '该', '保', '密', '的', '事', '情', '，'... '娜', ',', '这', '！', '就', '是', '街', '舞', '[SEP]']\n",
      "00:24:55.228985 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 32\n",
      "00:24:55.229260 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2418, 6421, 924, 2166, 4638, 752, 2658, 80...25, 117, 6821, 8013, 2218, 3221, 6125, 5659, 102]\n",
      "00:24:55.229529 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2418, 6421,  924, 2166, 4638,  752...6821, 8013, 2218, 3221,        6125, 5659,  102])\n",
      "00:24:55.229907 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.231095 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.232586 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.234059 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2418, 6421,  924, 2166, 4638,  75... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011078\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 87\n",
      "00:24:55.236548 call        14     def __getitem__(self, idx):\n",
      "00:24:55.236620 line        15         if self.mode == \"test\":\n",
      "00:24:55.236659 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '76岁慰安妇掀开衣服露出刀疤，控诉日本恶行！这部电影值得大家看'\n",
      "New var:....... text_b = '慰安妇,花漾奶奶秀英文,鬼怪奶奶,公务员,二十二,纪录片,罗文姬'\n",
      "New var:....... label = 0\n",
      "00:24:55.237236 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.237327 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.237403 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.237694 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['76', '岁', '慰', '安', '妇', '掀', '开', '衣', '服', '...'！', '这', '部', '电', '影', '值', '得', '大', '家', '看']\n",
      "00:24:55.238440 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '76', '岁', '慰', '安', '妇', '掀', '开', '衣... '部', '电', '影', '值', '得', '大', '家', '看', '[SEP]']\n",
      "00:24:55.238637 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:55.238823 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['慰', '安', '妇', ',', '花', '漾', '奶', '奶', '秀', '英...'十', '二', ',', '纪', '录', '片', ',', '罗', '文', '姬']\n",
      "00:24:55.239613 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '76', '岁', '慰', '安', '妇', '掀', '开', '衣... ',', '纪', '录', '片', ',', '罗', '文', '姬', '[SEP]']\n",
      "00:24:55.239970 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 33\n",
      "00:24:55.240218 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 8399, 2259, 2720, 2128, 1967, 2952, 2458, ...17, 5279, 2497, 4275, 117, 5384, 3152, 2010, 102]\n",
      "00:24:55.240484 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 8399, 2259, 2720, 2128, 1967, 2952...2497, 4275,         117, 5384, 3152, 2010,  102])\n",
      "00:24:55.240722 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.241428 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.242198 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.243577 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 8399, 2259, 2720, 2128, 1967, 295... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009488\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 88\n",
      "00:24:55.246066 call        14     def __getitem__(self, idx):\n",
      "00:24:55.246137 line        15         if self.mode == \"test\":\n",
      "00:24:55.246174 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '罗晋欲报灭门之仇《破冰者》再掀剧情高潮'\n",
      "New var:....... text_b = '张晨光,都市情感,潘之琳,真爱的谎言之破冰者,张定邦,罗晋'\n",
      "New var:....... label = 0\n",
      "00:24:55.246776 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.246871 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.246948 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.247246 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['罗', '晋', '欲', '报', '灭', '门', '之', '仇', '《', '破', '冰', '者', '》', '再', '掀', '剧', '情', '高', '潮']\n",
      "00:24:55.247794 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '罗', '晋', '欲', '报', '灭', '门', '之', '仇'... '者', '》', '再', '掀', '剧', '情', '高', '潮', '[SEP]']\n",
      "00:24:55.247979 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:24:55.248159 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['张', '晨', '光', ',', '都', '市', '情', '感', ',', '潘...'破', '冰', '者', ',', '张', '定', '邦', ',', '罗', '晋']\n",
      "00:24:55.248867 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '罗', '晋', '欲', '报', '灭', '门', '之', '仇'... '者', ',', '张', '定', '邦', ',', '罗', '晋', '[SEP]']\n",
      "00:24:55.249061 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 30\n",
      "00:24:55.249250 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5384, 3232, 3617, 2845, 4127, 7305, 722, 7...442, 117, 2476, 2137, 6930, 117, 5384, 3232, 102]\n",
      "00:24:55.249517 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5384, 3232, 3617, 2845, 4127, 7305...2476, 2137, 6930,  117,        5384, 3232,  102])\n",
      "00:24:55.249731 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.250386 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:24:55.251033 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.252447 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5384, 3232, 3617, 2845, 4127, 730...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008545\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 89\n",
      "00:24:55.254651 call        14     def __getitem__(self, idx):\n",
      "00:24:55.254729 line        15         if self.mode == \"test\":\n",
      "00:24:55.254768 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '熊黛林初为人母，脚肿得无法走路，韩火火同情不已'\n",
      "New var:....... text_b = '人工受孕,郭可盈,韩火火,熊黛林,郭富城'\n",
      "New var:....... label = 0\n",
      "00:24:55.255375 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.255475 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.255679 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.256094 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['熊', '黛', '林', '初', '为', '人', '母', '，', '脚', '肿...'走', '路', '，', '韩', '火', '火', '同', '情', '不', '已']\n",
      "00:24:55.256748 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '熊', '黛', '林', '初', '为', '人', '母', '，'... '，', '韩', '火', '火', '同', '情', '不', '已', '[SEP]']\n",
      "00:24:55.257011 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:24:55.257197 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['人', '工', '受', '孕', ',', '郭', '可', '盈', ',', '韩', '火', '火', ',', '熊', '黛', '林', ',', '郭', '富', '城']\n",
      "00:24:55.257752 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '熊', '黛', '林', '初', '为', '人', '母', '，'... ',', '熊', '黛', '林', ',', '郭', '富', '城', '[SEP]']\n",
      "00:24:55.257948 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:24:55.258138 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4220, 7950, 3360, 1159, 711, 782, 3678, 80...17, 4220, 7950, 3360, 117, 6958, 2168, 1814, 102]\n",
      "00:24:55.258343 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4220, 7950, 3360, 1159,  711,  782... 4220, 7950, 3360,  117, 6958, 2168, 1814,  102])\n",
      "00:24:55.258559 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.259277 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.259891 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.260995 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4220, 7950, 3360, 1159,  711,  78... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008494\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 90\n",
      "00:24:55.263174 call        14     def __getitem__(self, idx):\n",
      "00:24:55.263243 line        15         if self.mode == \"test\":\n",
      "00:24:55.263281 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '水原希子和石原里美告诉我们一个道理，短发女人也可以性感和可爱'\n",
      "New var:....... text_b = '半身裙,奥黛丽·赫本,性感,石原里美,水原希子'\n",
      "New var:....... label = 0\n",
      "00:24:55.263831 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.263922 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.263995 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.264209 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['水', '原', '希', '子', '和', '石', '原', '里', '美', '告...'女', '人', '也', '可', '以', '性', '感', '和', '可', '爱']\n",
      "00:24:55.265015 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '水', '原', '希', '子', '和', '石', '原', '里'... '也', '可', '以', '性', '感', '和', '可', '爱', '[SEP]']\n",
      "00:24:55.265229 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:55.265436 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['半', '身', '裙', ',', '奥', '黛', '丽', '·', '赫', '本...',', '石', '原', '里', '美', ',', '水', '原', '希', '子']\n",
      "00:24:55.266051 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '水', '原', '希', '子', '和', '石', '原', '里'... '原', '里', '美', ',', '水', '原', '希', '子', '[SEP]']\n",
      "00:24:55.266253 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "00:24:55.266470 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3717, 1333, 2361, 2094, 1469, 4767, 1333, ...33, 7027, 5401, 117, 3717, 1333, 2361, 2094, 102]\n",
      "00:24:55.266744 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3717, 1333, 2361, 2094, 1469, 4767... 7027, 5401,  117, 3717, 1333, 2361, 2094,  102])\n",
      "00:24:55.266959 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.267648 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.268285 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.269404 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3717, 1333, 2361, 2094, 1469, 476...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008450\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 91\n",
      "00:24:55.271655 call        14     def __getitem__(self, idx):\n",
      "00:24:55.271730 line        15         if self.mode == \"test\":\n",
      "00:24:55.271769 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张含韵现身机场，网友：她的眼神是那么的有魅力！'\n",
      "New var:....... text_b = '张含韵现身机场,机场,张含韵'\n",
      "New var:....... label = 0\n",
      "00:24:55.272266 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.272361 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.272436 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.272617 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '含', '韵', '现', '身', '机', '场', '，', '网', '友...'眼', '神', '是', '那', '么', '的', '有', '魅', '力', '！']\n",
      "00:24:55.273220 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '含', '韵', '现', '身', '机', '场', '，'... '是', '那', '么', '的', '有', '魅', '力', '！', '[SEP]']\n",
      "00:24:55.273407 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:24:55.273587 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['张', '含', '韵', '现', '身', '机', '场', ',', '机', '场', ',', '张', '含', '韵']\n",
      "00:24:55.274130 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '含', '韵', '现', '身', '机', '场', '，'... '场', ',', '机', '场', ',', '张', '含', '韵', '[SEP]']\n",
      "00:24:55.274372 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 15\n",
      "00:24:55.274578 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 1419, 7510, 4385, 6716, 3322, 1767, ...767, 117, 3322, 1767, 117, 2476, 1419, 7510, 102]\n",
      "00:24:55.274785 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476, 1419, 7510, 4385, 6716, 3322...3322, 1767,  117,        2476, 1419, 7510,  102])\n",
      "00:24:55.274998 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.275540 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.276208 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.277191 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2476, 1419, 7510, 4385, 6716, 332... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007421\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 92\n",
      "00:24:55.279109 call        14     def __getitem__(self, idx):\n",
      "00:24:55.279180 line        15         if self.mode == \"test\":\n",
      "00:24:55.279218 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '被杨洋抱过的女星，抱郑爽最像情侣，抱陈都灵心疼，抱她像哄宝宝'\n",
      "New var:....... text_b = '郑爽,胡冰卿,李珥,陈都灵,杨洋抱'\n",
      "New var:....... label = 0\n",
      "00:24:55.279787 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.279879 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.279954 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.280137 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['被', '杨', '洋', '抱', '过', '的', '女', '星', '，', '抱...'灵', '心', '疼', '，', '抱', '她', '像', '哄', '宝', '宝']\n",
      "00:24:55.280869 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '被', '杨', '洋', '抱', '过', '的', '女', '星'... '疼', '，', '抱', '她', '像', '哄', '宝', '宝', '[SEP]']\n",
      "00:24:55.281057 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:55.281303 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['郑', '爽', ',', '胡', '冰', '卿', ',', '李', '珥', ',', '陈', '都', '灵', ',', '杨', '洋', '抱']\n",
      "00:24:55.281805 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '被', '杨', '洋', '抱', '过', '的', '女', '星'... ',', '陈', '都', '灵', ',', '杨', '洋', '抱', '[SEP]']\n",
      "00:24:55.281999 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:24:55.282217 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6158, 3342, 3817, 2849, 6814, 4638, 1957, ...17, 7357, 6963, 4130, 117, 3342, 3817, 2849, 102]\n",
      "00:24:55.282436 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6158, 3342, 3817, 2849, 6814, 4638...6963, 4130,  117, 3342, 3817,        2849,  102])\n",
      "00:24:55.282652 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.283260 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:24:55.283957 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.284952 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6158, 3342, 3817, 2849, 6814, 463...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007912\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 93\n",
      "00:24:55.287050 call        14     def __getitem__(self, idx):\n",
      "00:24:55.287120 line        15         if self.mode == \"test\":\n",
      "00:24:55.287157 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '杨幂迪丽热巴的塑料鞋这么火，到底好看在哪里？'\n",
      "New var:....... text_b = '驴蹄鞋,ELLE taiwan,塑料,歌手,塑料鞋,Jimmy Choo'\n",
      "New var:....... label = 0\n",
      "00:24:55.287691 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.287783 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.287856 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.288036 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['杨', '幂', '迪', '丽', '热', '巴', '的', '塑', '料', '鞋...'火', '，', '到', '底', '好', '看', '在', '哪', '里', '？']\n",
      "00:24:55.288627 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '幂', '迪', '丽', '热', '巴', '的', '塑'... '到', '底', '好', '看', '在', '哪', '里', '？', '[SEP]']\n",
      "00:24:55.288813 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:24:55.288994 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['驴', '蹄', '鞋', ',', 'elle', 'taiwan', ',', '塑',..., ',', '塑', '料', '鞋', ',', 'jimmy', 'cho', '##o']\n",
      "00:24:55.289771 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '幂', '迪', '丽', '热', '巴', '的', '塑'...', '料', '鞋', ',', 'jimmy', 'cho', '##o', '[SEP]']\n",
      "00:24:55.290002 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:24:55.290327 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3342, 2386, 6832, 714, 4178, 2349, 4638, 1...7, 1848, 3160, 7490, 117, 10317, 9829, 8167, 102]\n",
      "00:24:55.290792 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  3342,  2386,  6832,   714,  4178... 7490,          117, 10317,  9829,  8167,   102])\n",
      "00:24:55.291391 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.292373 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.293280 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.294655 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  3342,  2386,  6832,   714,  417... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009595\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 94\n",
      "00:24:55.296678 call        14     def __getitem__(self, idx):\n",
      "00:24:55.296816 line        15         if self.mode == \"test\":\n",
      "00:24:55.296856 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '实力为王，数据辅证，赵丽颖新剧登录新闻联播，为颖宝加油点赞'\n",
      "New var:....... text_b = '新闻联播,知否知否应是绿肥红瘦,赵丽颖,新剧,艺人,颖宝'\n",
      "New var:....... label = 0\n",
      "00:24:55.297438 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.297532 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.297612 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.297793 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['实', '力', '为', '王', '，', '数', '据', '辅', '证', '，...'联', '播', '，', '为', '颖', '宝', '加', '油', '点', '赞']\n",
      "00:24:55.298517 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '实', '力', '为', '王', '，', '数', '据', '辅'... '，', '为', '颖', '宝', '加', '油', '点', '赞', '[SEP]']\n",
      "00:24:55.298706 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:24:55.298954 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['新', '闻', '联', '播', ',', '知', '否', '知', '否', '应...'颖', ',', '新', '剧', ',', '艺', '人', ',', '颖', '宝']\n",
      "00:24:55.299646 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '实', '力', '为', '王', '，', '数', '据', '辅'... '新', '剧', ',', '艺', '人', ',', '颖', '宝', '[SEP]']\n",
      "00:24:55.299843 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 29\n",
      "00:24:55.300035 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2141, 1213, 711, 4374, 8024, 3144, 2945, 6...3173, 1196, 117, 5686, 782, 117, 7577, 2140, 102]\n",
      "00:24:55.300244 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2141, 1213,  711, 4374, 8024, 3144... 1196,  117, 5686,  782,  117, 7577, 2140,  102])\n",
      "00:24:55.300459 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.301191 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.301952 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.303112 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2141, 1213,  711, 4374, 8024, 314... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008646\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 95\n",
      "00:24:55.305353 call        14     def __getitem__(self, idx):\n",
      "00:24:55.305422 line        15         if self.mode == \"test\":\n",
      "00:24:55.305459 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '杨幂的初恋，Angelababy的初恋，而赵丽颖的初恋却是从暗恋开始的'\n",
      "New var:....... text_b = '杨幂,初恋,陈伟霆,俞灏明,赵丽颖,非常静距离'\n",
      "New var:....... label = 0\n",
      "00:24:55.305979 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.306071 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.306145 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.306326 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['杨', '幂', '的', '初', '恋', '，', 'angelababy', '的'...'初', '恋', '却', '是', '从', '暗', '恋', '开', '始', '的']\n",
      "00:24:55.307190 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '幂', '的', '初', '恋', '，', 'angelab... '却', '是', '从', '暗', '恋', '开', '始', '的', '[SEP]']\n",
      "00:24:55.307390 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:24:55.307578 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['杨', '幂', ',', '初', '恋', ',', '陈', '伟', '霆', ',...',', '赵', '丽', '颖', ',', '非', '常', '静', '距', '离']\n",
      "00:24:55.308185 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '幂', '的', '初', '恋', '，', 'angelab... '丽', '颖', ',', '非', '常', '静', '距', '离', '[SEP]']\n",
      "00:24:55.308383 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "00:24:55.308575 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3342, 2386, 4638, 1159, 2605, 8024, 10643,...14, 7577, 117, 7478, 2382, 7474, 6655, 4895, 102]\n",
      "00:24:55.308830 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  3342,  2386,  4638,  1159,  2605... 7478,  2382,  7474,  6655,         4895,   102])\n",
      "00:24:55.309025 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.309640 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:24:55.310287 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.311426 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  3342,  2386,  4638,  1159,  260...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008240\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 96\n",
      "00:24:55.313623 call        14     def __getitem__(self, idx):\n",
      "00:24:55.313693 line        15         if self.mode == \"test\":\n",
      "00:24:55.313730 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《北京女子图鉴》在北京，有2000万人为了梦想而漂泊'\n",
      "New var:....... text_b = '北京女子图鉴,北漂,雨夹雪,陈可,地下室,麻辣烫'\n",
      "New var:....... label = 0\n",
      "00:24:55.314258 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.314350 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.314423 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.314605 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '北', '京', '女', '子', '图', '鉴', '》', '在', '北...00', '万', '人', '为', '了', '梦', '想', '而', '漂', '泊']\n",
      "00:24:55.315232 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '北', '京', '女', '子', '图', '鉴', '》'... '人', '为', '了', '梦', '想', '而', '漂', '泊', '[SEP]']\n",
      "00:24:55.315421 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:24:55.315604 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['北', '京', '女', '子', '图', '鉴', ',', '北', '漂', ',...'陈', '可', ',', '地', '下', '室', ',', '麻', '辣', '烫']\n",
      "00:24:55.316292 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '北', '京', '女', '子', '图', '鉴', '》'... ',', '地', '下', '室', ',', '麻', '辣', '烫', '[SEP]']\n",
      "00:24:55.316488 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 25\n",
      "00:24:55.316679 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 1266, 776, 1957, 2094, 1745, 7063, 51...117, 1765, 678, 2147, 117, 7937, 6793, 4176, 102]\n",
      "00:24:55.316884 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 1266,  776, 1957, 2094, 1745... 678, 2147,  117, 7937, 6793,        4176,  102])\n",
      "00:24:55.317094 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.317743 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:24:55.318407 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.319400 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 1266,  776, 1957, 2094, 174...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008030\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 97\n",
      "00:24:55.321688 call        14     def __getitem__(self, idx):\n",
      "00:24:55.321760 line        15         if self.mode == \"test\":\n",
      "00:24:55.321799 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '安陵容来找甄嬛玩，看着甄嬛教训下人，一阵羡慕'\n",
      "New var:....... text_b = '安陵容'\n",
      "New var:....... label = 0\n",
      "00:24:55.322368 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.322462 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.322542 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.322775 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['安', '陵', '容', '来', '找', '甄', '嬛', '玩', '，', '看...'嬛', '教', '训', '下', '人', '，', '一', '阵', '羡', '慕']\n",
      "00:24:55.323484 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '安', '陵', '容', '来', '找', '甄', '嬛', '玩'... '训', '下', '人', '，', '一', '阵', '羡', '慕', '[SEP]']\n",
      "00:24:55.323696 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:24:55.323889 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['安', '陵', '容']\n",
      "00:24:55.324153 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '安', '陵', '容', '来', '找', '甄', '嬛', '玩'..., '阵', '羡', '慕', '[SEP]', '安', '陵', '容', '[SEP]']\n",
      "00:24:55.324354 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 4\n",
      "00:24:55.324575 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2128, 7377, 2159, 3341, 2823, 4488, 2083, ...71, 7347, 5406, 2710, 102, 2128, 7377, 2159, 102]\n",
      "00:24:55.324782 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2128, 7377, 2159, 3341, 2823, 4488...5406, 2710,  102,        2128, 7377, 2159,  102])\n",
      "00:24:55.325028 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.325587 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,        1, 1, 1, 1])\n",
      "00:24:55.326159 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.327088 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2128, 7377, 2159, 3341, 2823, 448...0, 0, 0, 0, 0, 0,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006989\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 98\n",
      "00:24:55.328705 call        14     def __getitem__(self, idx):\n",
      "00:24:55.328774 line        15         if self.mode == \"test\":\n",
      "00:24:55.328813 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张靓颖亮相某活动 网友：穿成这样，怎么上厕所？真替你担心'\n",
      "New var:....... text_b = '东方ic,张靓颖亮相,张靓颖,东方IC,厕所'\n",
      "New var:....... label = 0\n",
      "00:24:55.329355 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.329447 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.329522 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.329736 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '靓', '颖', '亮', '相', '某', '活', '动', '网', '友...'么', '上', '厕', '所', '？', '真', '替', '你', '担', '心']\n",
      "00:24:55.330429 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '靓', '颖', '亮', '相', '某', '活', '动'... '厕', '所', '？', '真', '替', '你', '担', '心', '[SEP]']\n",
      "00:24:55.330689 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:24:55.330874 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['东', '方', 'ic', ',', '张', '靓', '颖', '亮', '相', '...张', '靓', '颖', ',', '东', '方', 'ic', ',', '厕', '所']\n",
      "00:24:55.331432 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '靓', '颖', '亮', '相', '某', '活', '动'...'颖', ',', '东', '方', 'ic', ',', '厕', '所', '[SEP]']\n",
      "00:24:55.331628 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:24:55.331818 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 7472, 7577, 778, 4685, 3378, 3833, 1...7577, 117, 691, 3175, 8577, 117, 1329, 2792, 102]\n",
      "00:24:55.332025 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476, 7472, 7577,  778, 4685, 3378... 691, 3175, 8577,  117, 1329,        2792,  102])\n",
      "00:24:55.332272 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.332951 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:24:55.333633 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.334702 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2476, 7472, 7577,  778, 4685, 337...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008148\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 99\n",
      "00:24:55.336884 call        14     def __getitem__(self, idx):\n",
      "00:24:55.336954 line        15         if self.mode == \"test\":\n",
      "00:24:55.336991 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '娱乐圈又一位女明星宣布怀有二胎，她儿子的名字你们很熟悉'\n",
      "New var:....... text_b = '张嘉倪,陈小春,张佳妮,应采儿,张钧甯'\n",
      "New var:....... label = 0\n",
      "00:24:55.337549 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.337641 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.337717 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.337899 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['娱', '乐', '圈', '又', '一', '位', '女', '明', '星', '宣...'儿', '子', '的', '名', '字', '你', '们', '很', '熟', '悉']\n",
      "00:24:55.338674 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '又', '一', '位', '女', '明'... '的', '名', '字', '你', '们', '很', '熟', '悉', '[SEP]']\n",
      "00:24:55.338882 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:24:55.339073 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['张', '嘉', '倪', ',', '陈', '小', '春', ',', '张', '佳', '妮', ',', '应', '采', '儿', ',', '张', '钧', '甯']\n",
      "00:24:55.339617 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '又', '一', '位', '女', '明'... ',', '应', '采', '儿', ',', '张', '钧', '甯', '[SEP]']\n",
      "00:24:55.339834 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:24:55.340046 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2031, 727, 1750, 1348, 671, 855, 1957, 320...17, 2418, 7023, 1036, 117, 2476, 7172, 4505, 102]\n",
      "00:24:55.340328 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2031,  727, 1750, 1348,  671,  855...7023, 1036,  117, 2476, 7172, 4505,         102])\n",
      "00:24:55.340556 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.341171 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:24:55.341774 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.342816 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2031,  727, 1750, 1348,  671,  85...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007897\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 100\n",
      "00:24:55.344812 call        14     def __getitem__(self, idx):\n",
      "00:24:55.344882 line        15         if self.mode == \"test\":\n",
      "00:24:55.344987 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '新《泡沫之夏》晨梓妍二次出演 薇安剧中人设简介'\n",
      "New var:....... text_b = '明晓溪,爱奇艺,晨梓妍,泡沫之夏,张雪迎,薇安'\n",
      "New var:....... label = 0\n",
      "00:24:55.345601 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.345694 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.345774 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.345955 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['新', '《', '泡', '沫', '之', '夏', '》', '晨', '梓', '妍...'出', '演', '薇', '安', '剧', '中', '人', '设', '简', '介']\n",
      "00:24:55.346563 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '新', '《', '泡', '沫', '之', '夏', '》', '晨'... '薇', '安', '剧', '中', '人', '设', '简', '介', '[SEP]']\n",
      "00:24:55.346751 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:24:55.346933 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['明', '晓', '溪', ',', '爱', '奇', '艺', ',', '晨', '梓...'沫', '之', '夏', ',', '张', '雪', '迎', ',', '薇', '安']\n",
      "00:24:55.347599 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '新', '《', '泡', '沫', '之', '夏', '》', '晨'... '夏', ',', '张', '雪', '迎', ',', '薇', '安', '[SEP]']\n",
      "00:24:55.347795 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "00:24:55.347986 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3173, 517, 3796, 3773, 722, 1909, 518, 324...909, 117, 2476, 7434, 6816, 117, 5948, 2128, 102]\n",
      "00:24:55.348192 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3173,  517, 3796, 3773,  722, 1909...  117, 2476, 7434, 6816,  117, 5948, 2128,  102])\n",
      "00:24:55.348405 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.348986 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.349635 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.350624 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3173,  517, 3796, 3773,  722, 190... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008066\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 101\n",
      "00:24:55.352910 call        14     def __getitem__(self, idx):\n",
      "00:24:55.352981 line        15         if self.mode == \"test\":\n",
      "00:24:55.353019 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '赵丽颖和徐峥早在17年前就合作过 赵丽颖有没有整容一目了然'\n",
      "New var:....... text_b = '赵丽颖,徐峥'\n",
      "New var:....... label = 0\n",
      "00:24:55.353626 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.353718 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.353796 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.354066 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['赵', '丽', '颖', '和', '徐', '峥', '早', '在', '17', '...'颖', '有', '没', '有', '整', '容', '一', '目', '了', '然']\n",
      "00:24:55.354785 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '赵', '丽', '颖', '和', '徐', '峥', '早', '在'... '没', '有', '整', '容', '一', '目', '了', '然', '[SEP]']\n",
      "00:24:55.354983 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:24:55.355168 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['赵', '丽', '颖', ',', '徐', '峥']\n",
      "00:24:55.355490 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '赵', '丽', '颖', '和', '徐', '峥', '早', '在'..., '[SEP]', '赵', '丽', '颖', ',', '徐', '峥', '[SEP]']\n",
      "00:24:55.355684 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 7\n",
      "00:24:55.355871 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6627, 714, 7577, 1469, 2528, 2286, 3193, 1...4197, 102, 6627, 714, 7577, 117, 2528, 2286, 102]\n",
      "00:24:55.356143 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6627,  714, 7577, 1469, 2528, 2286...  102, 6627,  714, 7577,  117, 2528, 2286,  102])\n",
      "00:24:55.356542 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.357148 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0,        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.357876 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.358936 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6627,  714, 7577, 1469, 2528, 228... 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007725\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 102\n",
      "00:24:55.360667 call        14     def __getitem__(self, idx):\n",
      "00:24:55.360738 line        15         if self.mode == \"test\":\n",
      "00:24:55.360775 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '蒋欣出席首播会 网友：看着又圆了，真动起手来男的都不赢她'\n",
      "New var:....... text_b = '蒋欣,首映式'\n",
      "New var:....... label = 0\n",
      "00:24:55.361366 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.361535 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.361619 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.361800 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['蒋', '欣', '出', '席', '首', '播', '会', '网', '友', '：...'动', '起', '手', '来', '男', '的', '都', '不', '赢', '她']\n",
      "00:24:55.362490 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '蒋', '欣', '出', '席', '首', '播', '会', '网'... '手', '来', '男', '的', '都', '不', '赢', '她', '[SEP]']\n",
      "00:24:55.362652 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:24:55.362814 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['蒋', '欣', ',', '首', '映', '式']\n",
      "00:24:55.363130 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '蒋', '欣', '出', '席', '首', '播', '会', '网'..., '[SEP]', '蒋', '欣', ',', '首', '映', '式', '[SEP]']\n",
      "00:24:55.363320 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 7\n",
      "00:24:55.363504 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5882, 3615, 1139, 2375, 7674, 3064, 833, 5...961, 102, 5882, 3615, 117, 7674, 3216, 2466, 102]\n",
      "00:24:55.363796 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5882, 3615, 1139, 2375, 7674, 3064...  102, 5882, 3615,  117, 7674, 3216, 2466,  102])\n",
      "00:24:55.364030 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.364603 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0,        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.365167 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.366142 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5882, 3615, 1139, 2375, 7674, 306... 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007043\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 103\n",
      "00:24:55.367740 call        14     def __getitem__(self, idx):\n",
      "00:24:55.367807 line        15         if self.mode == \"test\":\n",
      "00:24:55.367844 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '苟芸慧出席活动，网友：就不能让厨师好好炒菜吗？'\n",
      "New var:....... text_b = '苟芸慧出席活动,苟芸慧,厨师好好炒菜'\n",
      "New var:....... label = 0\n",
      "00:24:55.368410 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.368575 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.368720 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.368898 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['苟', '芸', '慧', '出', '席', '活', '动', '，', '网', '友...'能', '让', '厨', '师', '好', '好', '炒', '菜', '吗', '？']\n",
      "00:24:55.369514 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '苟', '芸', '慧', '出', '席', '活', '动', '，'... '厨', '师', '好', '好', '炒', '菜', '吗', '？', '[SEP]']\n",
      "00:24:55.369704 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:24:55.369885 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['苟', '芸', '慧', '出', '席', '活', '动', ',', '苟', '芸', '慧', ',', '厨', '师', '好', '好', '炒', '菜']\n",
      "00:24:55.370405 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '苟', '芸', '慧', '出', '席', '活', '动', '，'... '慧', ',', '厨', '师', '好', '好', '炒', '菜', '[SEP]']\n",
      "00:24:55.370599 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:24:55.370850 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5732, 5712, 2716, 1139, 2375, 3833, 1220, ...16, 117, 1337, 2360, 1962, 1962, 4143, 5831, 102]\n",
      "00:24:55.371057 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5732, 5712, 2716, 1139, 2375, 3833...  117, 1337, 2360, 1962, 1962, 4143, 5831,  102])\n",
      "00:24:55.371269 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.371882 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.372500 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.373645 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5732, 5712, 2716, 1139, 2375, 383... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007921\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 104\n",
      "00:24:55.375799 call        14     def __getitem__(self, idx):\n",
      "00:24:55.375885 line        15         if self.mode == \"test\":\n",
      "00:24:55.375924 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张震亮相戛纳电影节红毯，任戛纳电影节唯一华人评委'\n",
      "New var:....... text_b = '戛纳,戛纳电影节,张震,电影节,凯特·布兰切特,刺客聂隐娘,游侠索罗：星球大战外传'\n",
      "New var:....... label = 0\n",
      "00:24:55.376531 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.376632 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.376828 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.377109 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '震', '亮', '相', '戛', '纳', '电', '影', '节', '红...'纳', '电', '影', '节', '唯', '一', '华', '人', '评', '委']\n",
      "00:24:55.377800 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '震', '亮', '相', '戛', '纳', '电', '影'... '影', '节', '唯', '一', '华', '人', '评', '委', '[SEP]']\n",
      "00:24:55.378313 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:24:55.378508 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['戛', '纳', ',', '戛', '纳', '电', '影', '节', ',', '张...'侠', '索', '罗', '：', '星', '球', '大', '战', '外', '传']\n",
      "00:24:55.379429 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '震', '亮', '相', '戛', '纳', '电', '影'... '罗', '：', '星', '球', '大', '战', '外', '传', '[SEP]']\n",
      "00:24:55.379628 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 42\n",
      "00:24:55.379821 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 7448, 778, 4685, 2775, 5287, 4510, 2...84, 8038, 3215, 4413, 1920, 2773, 1912, 837, 102]\n",
      "00:24:55.380035 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476, 7448,  778, 4685, 2775, 5287... 8038, 3215, 4413, 1920, 2773, 1912,  837,  102])\n",
      "00:24:55.380369 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.381143 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.381844 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.383073 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2476, 7448,  778, 4685, 2775, 528... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009745\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 105\n",
      "00:24:55.385574 call        14     def __getitem__(self, idx):\n",
      "00:24:55.385646 line        15         if self.mode == \"test\":\n",
      "00:24:55.385684 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '胡可未修图长腿照被网友吐槽，网友评论：这才是正常的腿'\n",
      "New var:....... text_b = '牛仔裤,娱乐圈,女演员,胡可'\n",
      "New var:....... label = 0\n",
      "00:24:55.386261 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.386354 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.386532 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.386728 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['胡', '可', '未', '修', '图', '长', '腿', '照', '被', '网...'评', '论', '：', '这', '才', '是', '正', '常', '的', '腿']\n",
      "00:24:55.387393 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '胡', '可', '未', '修', '图', '长', '腿', '照'... '：', '这', '才', '是', '正', '常', '的', '腿', '[SEP]']\n",
      "00:24:55.387631 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:24:55.387814 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['牛', '仔', '裤', ',', '娱', '乐', '圈', ',', '女', '演', '员', ',', '胡', '可']\n",
      "00:24:55.388294 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '胡', '可', '未', '修', '图', '长', '腿', '照'... '圈', ',', '女', '演', '员', ',', '胡', '可', '[SEP]']\n",
      "00:24:55.388486 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 15\n",
      "00:24:55.388761 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5529, 1377, 3313, 934, 1745, 7270, 5597, 4...750, 117, 1957, 4028, 1447, 117, 5529, 1377, 102]\n",
      "00:24:55.388994 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5529, 1377, 3313,  934, 1745, 7270...       1957, 4028, 1447,  117, 5529, 1377,  102])\n",
      "00:24:55.389217 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.389805 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.390494 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.391477 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5529, 1377, 3313,  934, 1745, 727... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007765\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 106\n",
      "00:24:55.393367 call        14     def __getitem__(self, idx):\n",
      "00:24:55.393436 line        15         if self.mode == \"test\":\n",
      "00:24:55.393473 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '董璇与女儿小酒窝的母亲节写真温馨曝光，网友调侃：爸爸去哪了'\n",
      "New var:....... text_b = '母亲节写真温馨曝光,小酒窝,董璇,网友调侃,女儿小酒窝'\n",
      "New var:....... label = 0\n",
      "00:24:55.394034 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.394335 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.394630 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.395364 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['董', '璇', '与', '女', '儿', '小', '酒', '窝', '的', '母...'网', '友', '调', '侃', '：', '爸', '爸', '去', '哪', '了']\n",
      "00:24:55.396561 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '董', '璇', '与', '女', '儿', '小', '酒', '窝'... '调', '侃', '：', '爸', '爸', '去', '哪', '了', '[SEP]']\n",
      "00:24:55.396944 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:24:55.397141 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['母', '亲', '节', '写', '真', '温', '馨', '曝', '光', ',...'网', '友', '调', '侃', ',', '女', '儿', '小', '酒', '窝']\n",
      "00:24:55.397819 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '董', '璇', '与', '女', '儿', '小', '酒', '窝'... '调', '侃', ',', '女', '儿', '小', '酒', '窝', '[SEP]']\n",
      "00:24:55.398021 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 28\n",
      "00:24:55.398216 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5869, 4462, 680, 1957, 1036, 2207, 6983, 4...444, 887, 117, 1957, 1036, 2207, 6983, 4973, 102]\n",
      "00:24:55.398424 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5869, 4462,  680, 1957, 1036, 2207...  887,  117, 1957, 1036, 2207, 6983, 4973,  102])\n",
      "00:24:55.398708 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.399532 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.400183 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.401279 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5869, 4462,  680, 1957, 1036, 220...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010208\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 107\n",
      "00:24:55.403606 call        14     def __getitem__(self, idx):\n",
      "00:24:55.403674 line        15         if self.mode == \"test\":\n",
      "00:24:55.403711 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '180511 杨洋先生你的笑容已违规 快来戳开今日份续命药丸！'\n",
      "New var:....... text_b = '杨洋'\n",
      "New var:....... label = 0\n",
      "00:24:55.404301 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.404393 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.404467 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.404710 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['180', '##51', '##1', '杨', '洋', '先', '生', '你', ...'戳', '开', '今', '日', '份', '续', '命', '药', '丸', '！']\n",
      "00:24:55.405403 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '180', '##51', '##1', '杨', '洋', '先', '... '今', '日', '份', '续', '命', '药', '丸', '！', '[SEP]']\n",
      "00:24:55.405642 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:24:55.405826 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['杨', '洋']\n",
      "00:24:55.406074 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '180', '##51', '##1', '杨', '洋', '先', '..., '命', '药', '丸', '！', '[SEP]', '杨', '洋', '[SEP]']\n",
      "00:24:55.406263 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 3\n",
      "00:24:55.406507 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 8420, 9216, 8148, 3342, 3817, 1044, 4495, ...330, 1462, 5790, 709, 8013, 102, 3342, 3817, 102]\n",
      "00:24:55.406730 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 8420, 9216, 8148, 3342, 3817, 1044...       5790,  709, 8013,  102, 3342, 3817,  102])\n",
      "00:24:55.407049 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.407543 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0, 0, 0, 0,        0, 0, 0, 0, 1, 1, 1])\n",
      "00:24:55.408127 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.409028 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 8420, 9216, 8148, 3342, 3817, 104...0, 0, 0,        0, 0, 0, 0, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007028\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 108\n",
      "00:24:55.410663 call        14     def __getitem__(self, idx):\n",
      "00:24:55.410731 line        15         if self.mode == \"test\":\n",
      "00:24:55.410768 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《忽而今夏》：异地恋很讨厌，可是你很可爱'\n",
      "New var:....... text_b = '苹果派,忽而今夏,双城故事,一起同过窗,异地恋,你好，旧时光'\n",
      "New var:....... label = 0\n",
      "00:24:55.411293 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.411386 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.411460 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.411640 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '忽', '而', '今', '夏', '》', '：', '异', '地', '恋', '很', '讨', '厌', '，', '可', '是', '你', '很', '可', '爱']\n",
      "00:24:55.412191 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '忽', '而', '今', '夏', '》', '：', '异'... '厌', '，', '可', '是', '你', '很', '可', '爱', '[SEP]']\n",
      "00:24:55.412441 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 22\n",
      "00:24:55.412672 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['苹', '果', '派', ',', '忽', '而', '今', '夏', ',', '双...'异', '地', '恋', ',', '你', '好', '，', '旧', '时', '光']\n",
      "00:24:55.413397 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '忽', '而', '今', '夏', '》', '：', '异'... '恋', ',', '你', '好', '，', '旧', '时', '光', '[SEP]']\n",
      "00:24:55.413592 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 31\n",
      "00:24:55.413781 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 2575, 5445, 791, 1909, 518, 8038, 246...605, 117, 872, 1962, 8024, 3191, 3198, 1045, 102]\n",
      "00:24:55.413987 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 2575, 5445,  791, 1909,  518... 872, 1962,        8024, 3191, 3198, 1045,  102])\n",
      "00:24:55.414197 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.414957 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1])\n",
      "00:24:55.415780 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.416946 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 2575, 5445,  791, 1909,  51...1, 1, 1, 1, 1,        1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008492\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 109\n",
      "00:24:55.419185 call        14     def __getitem__(self, idx):\n",
      "00:24:55.419257 line        15         if self.mode == \"test\":\n",
      "00:24:55.419295 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '韩磊与歌迷合唱《等待》全场激动起立，美声唱法太动听！'\n",
      "New var:....... text_b = '等待,韩磊,美声唱法,歌迷'\n",
      "New var:....... label = 0\n",
      "00:24:55.419840 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.419931 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.420005 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.420117 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['韩', '磊', '与', '歌', '迷', '合', '唱', '《', '等', '待...'立', '，', '美', '声', '唱', '法', '太', '动', '听', '！']\n",
      "00:24:55.421373 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '磊', '与', '歌', '迷', '合', '唱', '《'... '美', '声', '唱', '法', '太', '动', '听', '！', '[SEP]']\n",
      "00:24:55.421531 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:24:55.423644 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['等', '待', ',', '韩', '磊', ',', '美', '声', '唱', '法', ',', '歌', '迷']\n",
      "00:24:55.429942 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '磊', '与', '歌', '迷', '合', '唱', '《'... ',', '美', '声', '唱', '法', ',', '歌', '迷', '[SEP]']\n",
      "00:24:55.430209 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "00:24:55.430400 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7506, 4830, 680, 3625, 6837, 1394, 1548, 5...17, 5401, 1898, 1548, 3791, 117, 3625, 6837, 102]\n",
      "00:24:55.430623 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7506, 4830,  680, 3625, 6837, 1394...1898,        1548, 3791,  117, 3625, 6837,  102])\n",
      "00:24:55.431083 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.431904 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.432454 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.433359 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7506, 4830,  680, 3625, 6837, 139... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.016628\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 110\n",
      "00:24:55.435847 call        14     def __getitem__(self, idx):\n",
      "00:24:55.435920 line        15         if self.mode == \"test\":\n",
      "00:24:55.435958 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '刘惜君三十而立游土耳其 自由歌唱“YUE”启程'\n",
      "New var:....... text_b = '惜时Precious-S,YUE,歌手,土耳其,刘惜君'\n",
      "New var:....... label = 0\n",
      "00:24:55.436541 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.436634 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.436711 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.437051 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['刘', '惜', '君', '三', '十', '而', '立', '游', '土', '耳...歌', '唱', '[UNK]', 'yu', '##e', '[UNK]', '启', '程']\n",
      "00:24:55.437690 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刘', '惜', '君', '三', '十', '而', '立', '游'...'[UNK]', 'yu', '##e', '[UNK]', '启', '程', '[SEP]']\n",
      "00:24:55.437925 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:24:55.438154 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['惜', '时', 'pre', '##ci', '##ous', '-', 's', ','...'歌', '手', ',', '土', '耳', '其', ',', '刘', '惜', '君']\n",
      "00:24:55.438829 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刘', '惜', '君', '三', '十', '而', '立', '游'... ',', '土', '耳', '其', ',', '刘', '惜', '君', '[SEP]']\n",
      "00:24:55.439197 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 22\n",
      "00:24:55.439458 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1155, 2667, 1409, 676, 1282, 5445, 4989, 3...17, 1759, 5455, 1071, 117, 1155, 2667, 1409, 102]\n",
      "00:24:55.439728 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  1155,  2667,  1409,   676,  1282... 1071,          117,  1155,  2667,  1409,   102])\n",
      "00:24:55.439991 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.440771 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.441698 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.442970 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  1155,  2667,  1409,   676,  128... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009677\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 111\n",
      "00:24:55.445554 call        14     def __getitem__(self, idx):\n",
      "00:24:55.445623 line        15         if self.mode == \"test\":\n",
      "00:24:55.445660 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '戚薇翻包晒自用品，她竟然一直吃这些美白！顺便种了好多奇怪的草'\n",
      "New var:....... text_b = '玫瑰果,美白,美白丸,人鱼姬,戚薇'\n",
      "New var:....... label = 0\n",
      "00:24:55.446244 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.446335 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.446529 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.446749 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['戚', '薇', '翻', '包', '晒', '自', '用', '品', '，', '她...'顺', '便', '种', '了', '好', '多', '奇', '怪', '的', '草']\n",
      "00:24:55.447445 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '戚', '薇', '翻', '包', '晒', '自', '用', '品'... '种', '了', '好', '多', '奇', '怪', '的', '草', '[SEP]']\n",
      "00:24:55.447657 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:55.447979 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['玫', '瑰', '果', ',', '美', '白', ',', '美', '白', '丸', ',', '人', '鱼', '姬', ',', '戚', '薇']\n",
      "00:24:55.448659 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '戚', '薇', '翻', '包', '晒', '自', '用', '品'... '丸', ',', '人', '鱼', '姬', ',', '戚', '薇', '[SEP]']\n",
      "00:24:55.449064 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:24:55.449369 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2774, 5948, 5436, 1259, 3235, 5632, 4500, ... 709, 117, 782, 7824, 2010, 117, 2774, 5948, 102]\n",
      "00:24:55.449665 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2774, 5948, 5436, 1259, 3235, 5632... 782, 7824, 2010,  117, 2774,        5948,  102])\n",
      "00:24:55.449935 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.450899 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:24:55.451785 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.452941 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2774, 5948, 5436, 1259, 3235, 563...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010121\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 112\n",
      "00:24:55.455706 call        14     def __getitem__(self, idx):\n",
      "00:24:55.455775 line        15         if self.mode == \"test\":\n",
      "00:24:55.455812 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '贾静雯带女儿上节目惹争议，发长文回应，请不要评判2岁多的孩子'\n",
      "New var:....... text_b = '波妞,咘咘,妈妈是超人3,修杰楷,贾静雯'\n",
      "New var:....... label = 0\n",
      "00:24:55.456402 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.456494 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.456687 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.456907 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['贾', '静', '雯', '带', '女', '儿', '上', '节', '目', '惹...'不', '要', '评', '判', '2', '岁', '多', '的', '孩', '子']\n",
      "00:24:55.457705 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '贾', '静', '雯', '带', '女', '儿', '上', '节'... '评', '判', '2', '岁', '多', '的', '孩', '子', '[SEP]']\n",
      "00:24:55.457966 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:55.458196 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['波', '妞', ',', '咘', '咘', ',', '妈', '妈', '是', '超', '人', '3', ',', '修', '杰', '楷', ',', '贾', '静', '雯']\n",
      "00:24:55.458791 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '贾', '静', '雯', '带', '女', '儿', '上', '节'... ',', '修', '杰', '楷', ',', '贾', '静', '雯', '[SEP]']\n",
      "00:24:55.459144 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:24:55.459381 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6593, 7474, 7435, 2372, 1957, 1036, 677, 5...117, 934, 3345, 3514, 117, 6593, 7474, 7435, 102]\n",
      "00:24:55.459636 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6593, 7474, 7435, 2372, 1957, 1036...3345, 3514,         117, 6593, 7474, 7435,  102])\n",
      "00:24:55.459894 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.460697 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1])\n",
      "00:24:55.461724 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.462881 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6593, 7474, 7435, 2372, 1957, 103...1, 1, 1, 1, 1,        1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009995\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 113\n",
      "00:24:55.465743 call        14     def __getitem__(self, idx):\n",
      "00:24:55.465822 line        15         if self.mode == \"test\":\n",
      "00:24:55.465861 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '上映一天票房仅0.5万，投资人无语了，网友：圈钱烂片活该亏本'\n",
      "New var:....... text_b = '周星驰,战神纪,黄一飞,少林足球,幕后玩家,低压槽：欲望之城,狂暴巨兽,何文辉,喜剧电影,王双宝'\n",
      "New var:....... label = 0\n",
      "00:24:55.466533 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.466794 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.466993 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.467336 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['上', '映', '一', '天', '票', '房', '仅', '0', '.', '5...'友', '：', '圈', '钱', '烂', '片', '活', '该', '亏', '本']\n",
      "00:24:55.468119 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '上', '映', '一', '天', '票', '房', '仅', '0'... '圈', '钱', '烂', '片', '活', '该', '亏', '本', '[SEP]']\n",
      "00:24:55.468356 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:55.468586 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['周', '星', '驰', ',', '战', '神', '纪', ',', '黄', '一...'辉', ',', '喜', '剧', '电', '影', ',', '王', '双', '宝']\n",
      "00:24:55.469777 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '上', '映', '一', '天', '票', '房', '仅', '0'... '喜', '剧', '电', '影', ',', '王', '双', '宝', '[SEP]']\n",
      "00:24:55.470026 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 49\n",
      "00:24:55.470270 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 677, 3216, 671, 1921, 4873, 2791, 788, 121...99, 1196, 4510, 2512, 117, 4374, 1352, 2140, 102]\n",
      "00:24:55.470541 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  677, 3216,  671, 1921, 4873, 2791... 1196, 4510, 2512,  117, 4374, 1352, 2140,  102])\n",
      "00:24:55.470811 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.471803 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.472730 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.474539 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  677, 3216,  671, 1921, 4873, 279...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.012118\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 114\n",
      "00:24:55.477893 call        14     def __getitem__(self, idx):\n",
      "00:24:55.477964 line        15         if self.mode == \"test\":\n",
      "00:24:55.478002 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '故宫的一个地方曾经是妇孺皆知，如今能够进去的都是贵宾'\n",
      "New var:....... text_b = '故宫,漱芳斋,重华宫,故宫博物院,宣统皇帝'\n",
      "New var:....... label = 0\n",
      "00:24:55.478596 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.478690 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.478765 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.478990 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['故', '宫', '的', '一', '个', '地', '方', '曾', '经', '是...'今', '能', '够', '进', '去', '的', '都', '是', '贵', '宾']\n",
      "00:24:55.479693 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '故', '宫', '的', '一', '个', '地', '方', '曾'... '够', '进', '去', '的', '都', '是', '贵', '宾', '[SEP]']\n",
      "00:24:55.480077 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:24:55.480334 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['故', '宫', ',', '漱', '芳', '斋', ',', '重', '华', '宫...'故', '宫', '博', '物', '院', ',', '宣', '统', '皇', '帝']\n",
      "00:24:55.480881 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '故', '宫', '的', '一', '个', '地', '方', '曾'... '博', '物', '院', ',', '宣', '统', '皇', '帝', '[SEP]']\n",
      "00:24:55.481104 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 22\n",
      "00:24:55.481343 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3125, 2151, 4638, 671, 702, 1765, 3175, 32...00, 4289, 7368, 117, 2146, 5320, 4640, 2370, 102]\n",
      "00:24:55.481597 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3125, 2151, 4638,  671,  702, 1765...7368,  117, 2146, 5320, 4640,        2370,  102])\n",
      "00:24:55.481854 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.482779 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:24:55.483582 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.485110 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3125, 2151, 4638,  671,  702, 176...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009870\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 115\n",
      "00:24:55.487792 call        14     def __getitem__(self, idx):\n",
      "00:24:55.487859 line        15         if self.mode == \"test\":\n",
      "00:24:55.487897 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '母亲节娱乐圈送祝福，数丫丫的最暖心！'\n",
      "New var:....... text_b = '丫丫,娱乐圈,大张伟,江疏影,母亲节'\n",
      "New var:....... label = 0\n",
      "00:24:55.488391 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.488482 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.488554 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.488780 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['母', '亲', '节', '娱', '乐', '圈', '送', '祝', '福', '，', '数', '丫', '丫', '的', '最', '暖', '心', '！']\n",
      "00:24:55.489339 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '母', '亲', '节', '娱', '乐', '圈', '送', '祝'... '数', '丫', '丫', '的', '最', '暖', '心', '！', '[SEP]']\n",
      "00:24:55.489567 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 20\n",
      "00:24:55.489790 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['丫', '丫', ',', '娱', '乐', '圈', ',', '大', '张', '伟', ',', '江', '疏', '影', ',', '母', '亲', '节']\n",
      "00:24:55.490483 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '母', '亲', '节', '娱', '乐', '圈', '送', '祝'... ',', '江', '疏', '影', ',', '母', '亲', '节', '[SEP]']\n",
      "00:24:55.490668 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:24:55.490988 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3678, 779, 5688, 2031, 727, 1750, 6843, 48...117, 3736, 4541, 2512, 117, 3678, 779, 5688, 102]\n",
      "00:24:55.491511 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3678,  779, 5688, 2031,  727, 1750...4541, 2512,  117, 3678,         779, 5688,  102])\n",
      "00:24:55.491791 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.492678 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.493334 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.494558 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3678,  779, 5688, 2031,  727, 175... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009145\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 116\n",
      "00:24:55.496967 call        14     def __getitem__(self, idx):\n",
      "00:24:55.497037 line        15         if self.mode == \"test\":\n",
      "00:24:55.497075 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《爱情的边疆》开播 殷桃至今“不敢”看'\n",
      "New var:....... text_b = '高满堂,李乃文,毛卫宁,爱与生活,殷桃'\n",
      "New var:....... label = 0\n",
      "00:24:55.497665 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.497757 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.497830 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.498199 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '爱', '情', '的', '边', '疆', '》', '开', '播', '殷', '桃', '至', '今', '[UNK]', '不', '敢', '[UNK]', '看']\n",
      "00:24:55.498798 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '爱', '情', '的', '边', '疆', '》', '开'...', '今', '[UNK]', '不', '敢', '[UNK]', '看', '[SEP]']\n",
      "00:24:55.499032 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 20\n",
      "00:24:55.499260 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['高', '满', '堂', ',', '李', '乃', '文', ',', '毛', '卫', '宁', ',', '爱', '与', '生', '活', ',', '殷', '桃']\n",
      "00:24:55.499840 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '爱', '情', '的', '边', '疆', '》', '开'... ',', '爱', '与', '生', '活', ',', '殷', '桃', '[SEP]']\n",
      "00:24:55.500078 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:24:55.500421 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 4263, 2658, 4638, 6804, 4538, 518, 24...117, 4263, 680, 4495, 3833, 117, 3668, 3425, 102]\n",
      "00:24:55.500669 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 4263, 2658, 4638, 6804, 4538... 680, 4495, 3833,         117, 3668, 3425,  102])\n",
      "00:24:55.500921 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.501645 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.502393 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.503392 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 4263, 2658, 4638, 6804, 453... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007644\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 117\n",
      "00:24:55.504641 call        14     def __getitem__(self, idx):\n",
      "00:24:55.504709 line        15         if self.mode == \"test\":\n",
      "00:24:55.504747 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《这就是街舞》易烊千玺夺冠，成为首个00后冠军导师！'\n",
      "New var:....... text_b = '这就是街舞,00后,玺队长,易烊千玺,华丽转身,Battle'\n",
      "New var:....... label = 0\n",
      "00:24:55.505258 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.505349 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.505422 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.505679 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '这', '就', '是', '街', '舞', '》', '易', '烊', '千...为', '首', '个', '00', '后', '冠', '军', '导', '师', '！']\n",
      "00:24:55.506329 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '这', '就', '是', '街', '舞', '》', '易'...'个', '00', '后', '冠', '军', '导', '师', '！', '[SEP]']\n",
      "00:24:55.506517 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:24:55.506717 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['这', '就', '是', '街', '舞', ',', '00', '后', ',', '...'千', '玺', ',', '华', '丽', '转', '身', ',', 'battle']\n",
      "00:24:55.507406 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '这', '就', '是', '街', '舞', '》', '易'... ',', '华', '丽', '转', '身', ',', 'battle', '[SEP]']\n",
      "00:24:55.507607 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 25\n",
      "00:24:55.507866 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 6821, 2218, 3221, 6125, 5659, 518, 32...389, 117, 1290, 714, 6760, 6716, 117, 12379, 102]\n",
      "00:24:55.508082 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,   517,  6821,  2218,  3221,  6125...  714,  6760,  6716,   117,        12379,   102])\n",
      "00:24:55.508294 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.508960 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:24:55.509663 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.510559 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,   517,  6821,  2218,  3221,  612...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007948\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 118\n",
      "00:24:55.512691 call        14     def __getitem__(self, idx):\n",
      "00:24:55.512764 line        15         if self.mode == \"test\":\n",
      "00:24:55.512802 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '曾与周润发闪婚闪离，前夫破产不离不弃，为供女儿读书复出拍戏'\n",
      "New var:....... text_b = '陈荟莲,书剑恩仇录,早熟,我要成名,李万祺,陈真,周润发,宝贝计划,余安安,尔冬升'\n",
      "New var:....... label = 0\n",
      "00:24:55.513369 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.513461 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.513536 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.513720 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['曾', '与', '周', '润', '发', '闪', '婚', '闪', '离', '，...'为', '供', '女', '儿', '读', '书', '复', '出', '拍', '戏']\n",
      "00:24:55.514433 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '曾', '与', '周', '润', '发', '闪', '婚', '闪'... '女', '儿', '读', '书', '复', '出', '拍', '戏', '[SEP]']\n",
      "00:24:55.514621 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:24:55.514892 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['陈', '荟', '莲', ',', '书', '剑', '恩', '仇', '录', ',...'计', '划', ',', '余', '安', '安', ',', '尔', '冬', '升']\n",
      "00:24:55.515807 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '曾', '与', '周', '润', '发', '闪', '婚', '闪'... ',', '余', '安', '安', ',', '尔', '冬', '升', '[SEP]']\n",
      "00:24:55.516009 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 42\n",
      "00:24:55.516203 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3295, 680, 1453, 3883, 1355, 7306, 2042, 7...117, 865, 2128, 2128, 117, 2209, 1100, 1285, 102]\n",
      "00:24:55.516421 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3295,  680, 1453, 3883, 1355, 7306...2128, 2128,  117, 2209, 1100, 1285,         102])\n",
      "00:24:55.516640 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.517389 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:24:55.518136 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.519410 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3295,  680, 1453, 3883, 1355, 730...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009362\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 119\n",
      "00:24:55.522080 call        14     def __getitem__(self, idx):\n",
      "00:24:55.522148 line        15         if self.mode == \"test\":\n",
      "00:24:55.522184 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '即将上映的《媚者无疆》剧照已出！网友的回答是：这剧必追！'\n",
      "New var:....... text_b = '媚者无疆,翁美玲,小三,李一桐,长安,郭雪芙'\n",
      "New var:....... label = 0\n",
      "00:24:55.522710 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.522802 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.522874 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.523161 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['即', '将', '上', '映', '的', '《', '媚', '者', '无', '疆...'的', '回', '答', '是', '：', '这', '剧', '必', '追', '！']\n",
      "00:24:55.523873 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '即', '将', '上', '映', '的', '《', '媚', '者'... '答', '是', '：', '这', '剧', '必', '追', '！', '[SEP]']\n",
      "00:24:55.524065 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:24:55.524249 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['媚', '者', '无', '疆', ',', '翁', '美', '玲', ',', '小...'李', '一', '桐', ',', '长', '安', ',', '郭', '雪', '芙']\n",
      "00:24:55.524835 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '即', '将', '上', '映', '的', '《', '媚', '者'... '桐', ',', '长', '安', ',', '郭', '雪', '芙', '[SEP]']\n",
      "00:24:55.525029 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:24:55.525282 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1315, 2199, 677, 3216, 4638, 517, 2055, 54...432, 117, 7270, 2128, 117, 6958, 7434, 5696, 102]\n",
      "00:24:55.525534 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1315, 2199,  677, 3216, 4638,  517...7270, 2128,         117, 6958, 7434, 5696,  102])\n",
      "00:24:55.525905 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.526696 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1])\n",
      "00:24:55.527388 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.528971 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1315, 2199,  677, 3216, 4638,  51...1, 1, 1, 1, 1,        1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008931\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 120\n",
      "00:24:55.531142 call        14     def __getitem__(self, idx):\n",
      "00:24:55.531219 line        15         if self.mode == \"test\":\n",
      "00:24:55.531257 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '皇后不同意灰姑娘当贵妃，不料皇上竟然一怒之下把皇后推到'\n",
      "New var:....... text_b = '灰姑娘,皇后,明珠游龙'\n",
      "New var:....... label = 0\n",
      "00:24:55.531874 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.531970 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.532133 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.532320 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['皇', '后', '不', '同', '意', '灰', '姑', '娘', '当', '贵...'然', '一', '怒', '之', '下', '把', '皇', '后', '推', '到']\n",
      "00:24:55.533007 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '皇', '后', '不', '同', '意', '灰', '姑', '娘'... '怒', '之', '下', '把', '皇', '后', '推', '到', '[SEP]']\n",
      "00:24:55.533309 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:24:55.533515 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['灰', '姑', '娘', ',', '皇', '后', ',', '明', '珠', '游', '龙']\n",
      "00:24:55.533929 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '皇', '后', '不', '同', '意', '灰', '姑', '娘'... ',', '皇', '后', ',', '明', '珠', '游', '龙', '[SEP]']\n",
      "00:24:55.534127 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 12\n",
      "00:24:55.534318 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4640, 1400, 679, 1398, 2692, 4129, 1996, 2...17, 4640, 1400, 117, 3209, 4403, 3952, 7987, 102]\n",
      "00:24:55.534523 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4640, 1400,  679, 1398, 2692, 4129...1400,  117,        3209, 4403, 3952, 7987,  102])\n",
      "00:24:55.534734 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.535282 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.536250 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.537152 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4640, 1400,  679, 1398, 2692, 412... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008100\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 121\n",
      "00:24:55.539271 call        14     def __getitem__(self, idx):\n",
      "00:24:55.539341 line        15         if self.mode == \"test\":\n",
      "00:24:55.539379 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '刘昊然和李现阳光时尚代言图集，网友：果然，颜值高怎么穿都好看'\n",
      "New var:....... text_b = '广告,刘昊然广告现场图,颜值,刘昊然,现广告代言高清写真图'\n",
      "New var:....... label = 0\n",
      "00:24:55.539928 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.540052 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.540135 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.540409 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['刘', '昊', '然', '和', '李', '现', '阳', '光', '时', '尚...'，', '颜', '值', '高', '怎', '么', '穿', '都', '好', '看']\n",
      "00:24:55.541254 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刘', '昊', '然', '和', '李', '现', '阳', '光'... '值', '高', '怎', '么', '穿', '都', '好', '看', '[SEP]']\n",
      "00:24:55.541542 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:55.541736 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['广', '告', ',', '刘', '昊', '然', '广', '告', '现', '场...'现', '广', '告', '代', '言', '高', '清', '写', '真', '图']\n",
      "00:24:55.542443 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刘', '昊', '然', '和', '李', '现', '阳', '光'... '告', '代', '言', '高', '清', '写', '真', '图', '[SEP]']\n",
      "00:24:55.542645 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 30\n",
      "00:24:55.542905 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1155, 3207, 4197, 1469, 3330, 4385, 7345, ...40, 807, 6241, 7770, 3926, 1091, 4696, 1745, 102]\n",
      "00:24:55.543150 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1155, 3207, 4197, 1469, 3330, 4385...6241, 7770, 3926, 1091, 4696,        1745,  102])\n",
      "00:24:55.543377 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.544223 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.544927 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.546074 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1155, 3207, 4197, 1469, 3330, 438... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009113\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 122\n",
      "00:24:55.548413 call        14     def __getitem__(self, idx):\n",
      "00:24:55.548482 line        15         if self.mode == \"test\":\n",
      "00:24:55.548520 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '秦岚居然那么少女，上半身凸出了重点，网友：要我就娶她了'\n",
      "New var:....... text_b = '牛仔裤,秦岚,一帘幽梦,少女'\n",
      "New var:....... label = 0\n",
      "00:24:55.549069 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.549161 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.549234 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.549426 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['秦', '岚', '居', '然', '那', '么', '少', '女', '，', '上...'，', '网', '友', '：', '要', '我', '就', '娶', '她', '了']\n",
      "00:24:55.550101 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '秦', '岚', '居', '然', '那', '么', '少', '女'... '友', '：', '要', '我', '就', '娶', '她', '了', '[SEP]']\n",
      "00:24:55.550322 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:24:55.550561 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['牛', '仔', '裤', ',', '秦', '岚', ',', '一', '帘', '幽', '梦', ',', '少', '女']\n",
      "00:24:55.551058 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '秦', '岚', '居', '然', '那', '么', '少', '女'... ',', '一', '帘', '幽', '梦', ',', '少', '女', '[SEP]']\n",
      "00:24:55.551265 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 15\n",
      "00:24:55.551459 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4912, 2269, 2233, 4197, 6929, 720, 2208, 1...117, 671, 2366, 2406, 3457, 117, 2208, 1957, 102]\n",
      "00:24:55.551731 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4912, 2269, 2233, 4197, 6929,  720...  671, 2366, 2406, 3457,  117, 2208, 1957,  102])\n",
      "00:24:55.551944 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.552658 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.553249 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.554321 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4912, 2269, 2233, 4197, 6929,  72... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007764\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 123\n",
      "00:24:55.556211 call        14     def __getitem__(self, idx):\n",
      "00:24:55.556282 line        15         if self.mode == \"test\":\n",
      "00:24:55.556320 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张馨予超短裙造型少女感足 和妈妈手牵手现身亲密似姐妹'\n",
      "New var:....... text_b = '上海机场,范冰冰,新边城浪子,张馨予,女演员'\n",
      "New var:....... label = 0\n",
      "00:24:55.556902 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.557110 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.557513 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.557883 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '馨', '予', '超', '短', '裙', '造', '型', '少', '女...'手', '牵', '手', '现', '身', '亲', '密', '似', '姐', '妹']\n",
      "00:24:55.558881 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '馨', '予', '超', '短', '裙', '造', '型'... '手', '现', '身', '亲', '密', '似', '姐', '妹', '[SEP]']\n",
      "00:24:55.559320 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:24:55.559622 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['上', '海', '机', '场', ',', '范', '冰', '冰', ',', '新...'浪', '子', ',', '张', '馨', '予', ',', '女', '演', '员']\n",
      "00:24:55.560234 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '馨', '予', '超', '短', '裙', '造', '型'... ',', '张', '馨', '予', ',', '女', '演', '员', '[SEP]']\n",
      "00:24:55.560438 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:24:55.560632 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 7678, 750, 6631, 4764, 6170, 6863, 1...117, 2476, 7678, 750, 117, 1957, 4028, 1447, 102]\n",
      "00:24:55.560845 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476, 7678,  750, 6631, 4764, 6170...7678,  750,  117, 1957, 4028,        1447,  102])\n",
      "00:24:55.561065 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.561968 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:24:55.562614 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.563726 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2476, 7678,  750, 6631, 4764, 617...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009848\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 124\n",
      "00:24:55.566090 call        14     def __getitem__(self, idx):\n",
      "00:24:55.566161 line        15         if self.mode == \"test\":\n",
      "00:24:55.566198 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '观音送江流儿袈裟 悟后尘垢尽 迷时顺江流'\n",
      "New var:....... text_b = '江流儿,尘垢,袈裟,观音'\n",
      "New var:....... label = 0\n",
      "00:24:55.566805 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.566901 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.566978 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.567225 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['观', '音', '送', '江', '流', '儿', '袈', '裟', '悟', '后', '尘', '垢', '尽', '迷', '时', '顺', '江', '流']\n",
      "00:24:55.567760 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '观', '音', '送', '江', '流', '儿', '袈', '裟'... '尘', '垢', '尽', '迷', '时', '顺', '江', '流', '[SEP]']\n",
      "00:24:55.567943 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 20\n",
      "00:24:55.568121 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['江', '流', '儿', ',', '尘', '垢', ',', '袈', '裟', ',', '观', '音']\n",
      "00:24:55.568533 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '观', '音', '送', '江', '流', '儿', '袈', '裟'... '尘', '垢', ',', '袈', '裟', ',', '观', '音', '[SEP]']\n",
      "00:24:55.568723 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 13\n",
      "00:24:55.568907 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6225, 7509, 6843, 3736, 3837, 1036, 6149, ...212, 1803, 117, 6149, 6173, 117, 6225, 7509, 102]\n",
      "00:24:55.569102 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6225, 7509, 6843, 3736, 3837, 1036... 1803,  117, 6149, 6173,  117, 6225, 7509,  102])\n",
      "00:24:55.569302 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.569938 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.570537 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.571415 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6225, 7509, 6843, 3736, 3837, 103...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007135\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 125\n",
      "00:24:55.573256 call        14     def __getitem__(self, idx):\n",
      "00:24:55.573325 line        15         if self.mode == \"test\":\n",
      "00:24:55.573362 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '韩安冉什么时候开始整容的？'\n",
      "New var:....... text_b = '变形计,变形计之青春作伴,真人秀,韩安冉,湖南卫视,李蒽熙,双眼皮'\n",
      "New var:....... label = 0\n",
      "00:24:55.573940 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.574032 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.574105 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.574316 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['韩', '安', '冉', '什', '么', '时', '候', '开', '始', '整', '容', '的', '？']\n",
      "00:24:55.574841 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '安', '冉', '什', '么', '时', '候', '开', '始', '整', '容', '的', '？', '[SEP]']\n",
      "00:24:55.575011 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "00:24:55.575193 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['变', '形', '计', ',', '变', '形', '计', '之', '青', '春... '视', ',', '李', '[UNK]', '熙', ',', '双', '眼', '皮']\n",
      "00:24:55.575968 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '安', '冉', '什', '么', '时', '候', '开'..., '李', '[UNK]', '熙', ',', '双', '眼', '皮', '[SEP]']\n",
      "00:24:55.576162 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 34\n",
      "00:24:55.576354 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7506, 2128, 1083, 784, 720, 3198, 952, 245...117, 3330, 100, 4224, 117, 1352, 4706, 4649, 102]\n",
      "00:24:55.576562 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7506, 2128, 1083,  784,  720, 3198... 100, 4224,  117, 1352, 4706, 4649,         102])\n",
      "00:24:55.576862 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.577465 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:24:55.578116 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.579210 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7506, 2128, 1083,  784,  720, 319...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007885\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 126\n",
      "00:24:55.581173 call        14     def __getitem__(self, idx):\n",
      "00:24:55.581242 line        15         if self.mode == \"test\":\n",
      "00:24:55.581280 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '新版乌龙院即将上映，小胖郝邵文长大了，只是释小龙换成了宋小宝'\n",
      "New var:....... text_b = '乌龙院,宋小宝,新乌龙院之笑闹江湖,新乌龙院,郝邵文,释小龙,吴孟达'\n",
      "New var:....... label = 0\n",
      "00:24:55.581948 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.582043 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.582123 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.582240 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['新', '版', '乌', '龙', '院', '即', '将', '上', '映', '，...'是', '释', '小', '龙', '换', '成', '了', '宋', '小', '宝']\n",
      "00:24:55.583371 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '新', '版', '乌', '龙', '院', '即', '将', '上'... '小', '龙', '换', '成', '了', '宋', '小', '宝', '[SEP]']\n",
      "00:24:55.583891 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:24:55.584095 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['乌', '龙', '院', ',', '宋', '小', '宝', ',', '新', '乌...'邵', '文', ',', '释', '小', '龙', ',', '吴', '孟', '达']\n",
      "00:24:55.584901 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '新', '版', '乌', '龙', '院', '即', '将', '上'... ',', '释', '小', '龙', ',', '吴', '孟', '达', '[SEP]']\n",
      "00:24:55.585104 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 35\n",
      "00:24:55.585300 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3173, 4276, 723, 7987, 7368, 1315, 2199, 6...17, 7025, 2207, 7987, 117, 1426, 2106, 6809, 102]\n",
      "00:24:55.585518 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3173, 4276,  723, 7987, 7368, 1315...       2207, 7987,  117, 1426, 2106, 6809,  102])\n",
      "00:24:55.585805 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.586817 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:24:55.587546 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.588961 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3173, 4276,  723, 7987, 7368, 131... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010335\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 127\n",
      "00:24:55.591545 call        14     def __getitem__(self, idx):\n",
      "00:24:55.591621 line        15         if self.mode == \"test\":\n",
      "00:24:55.591659 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '年纪越大越有“韵味”的四大女星，你会选择谁当你的老婆？'\n",
      "New var:....... text_b = '颜丹晨,闫妮,武林外传,俞飞鸿,女演员,许晴'\n",
      "New var:....... label = 0\n",
      "00:24:55.592244 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:24:55.592338 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:24:55.592497 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:24:55.592682 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['年', '纪', '越', '大', '越', '有', '[UNK]', '韵', '味'...'会', '选', '择', '谁', '当', '你', '的', '老', '婆', '？']\n",
      "00:24:55.593401 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '年', '纪', '越', '大', '越', '有', '[UNK]',... '择', '谁', '当', '你', '的', '老', '婆', '？', '[SEP]']\n",
      "00:24:55.593692 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:24:55.593910 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['颜', '丹', '晨', ',', '闫', '妮', ',', '武', '林', '外...'俞', '飞', '鸿', ',', '女', '演', '员', ',', '许', '晴']\n",
      "00:24:55.594514 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '年', '纪', '越', '大', '越', '有', '[UNK]',... '鸿', ',', '女', '演', '员', ',', '许', '晴', '[SEP]']\n",
      "00:24:55.594717 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:24:55.594911 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2399, 5279, 6632, 1920, 6632, 3300, 100, 7...896, 117, 1957, 4028, 1447, 117, 6387, 3252, 102]\n",
      "00:24:55.595123 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2399, 5279, 6632, 1920, 6632, 3300...1957, 4028, 1447,         117, 6387, 3252,  102])\n",
      "00:24:55.595406 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:24:55.596151 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:24:55.596832 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:24:55.597948 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2399, 5279, 6632, 1920, 6632, 330...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008556\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 128\n",
      "00:25:04.281430 call        14     def __getitem__(self, idx):\n",
      "00:25:04.281623 line        15         if self.mode == \"test\":\n",
      "00:25:04.281670 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '后来的我们，全款变成了首付'\n",
      "New var:....... text_b = '电影,刘若英,北京,凤凰城,金龙湖'\n",
      "New var:....... label = 0\n",
      "00:25:04.282296 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.282397 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.282479 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.282838 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['后', '来', '的', '我', '们', '，', '全', '款', '变', '成', '了', '首', '付']\n",
      "00:25:04.283373 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '后', '来', '的', '我', '们', '，', '全', '款', '变', '成', '了', '首', '付', '[SEP]']\n",
      "00:25:04.283614 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "00:25:04.283988 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['电', '影', ',', '刘', '若', '英', ',', '北', '京', ',', '凤', '凰', '城', ',', '金', '龙', '湖']\n",
      "00:25:04.284551 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '后', '来', '的', '我', '们', '，', '全', '款'... ',', '凤', '凰', '城', ',', '金', '龙', '湖', '[SEP]']\n",
      "00:25:04.284804 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:04.285064 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1400, 3341, 4638, 2769, 812, 8024, 1059, 3...17, 1128, 1133, 1814, 117, 7032, 7987, 3959, 102]\n",
      "00:25:04.285316 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1400, 3341, 4638, 2769,  812, 8024... 1128, 1133, 1814,  117, 7032, 7987, 3959,  102])\n",
      "00:25:04.285582 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.286510 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.287178 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.288430 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1400, 3341, 4638, 2769,  812, 802...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009456\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 129\n",
      "00:25:04.290912 call        14     def __getitem__(self, idx):\n",
      "00:25:04.290992 line        15         if self.mode == \"test\":\n",
      "00:25:04.291031 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '因任贤齐而迅速蹿红，曾不逊色于林心如杨幂的张檬竟把自己坑了？'\n",
      "New var:....... text_b = '倚天屠龙记,任贤齐,张檬,金都1943,林心如,演艺事业,卫子夫,美人心计'\n",
      "New var:....... label = 0\n",
      "00:25:04.291664 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.291779 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.291870 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.292233 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['因', '任', '贤', '齐', '而', '迅', '速', '蹿', '红', '，...'的', '张', '檬', '竟', '把', '自', '己', '坑', '了', '？']\n",
      "00:25:04.293243 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '因', '任', '贤', '齐', '而', '迅', '速', '蹿'... '檬', '竟', '把', '自', '己', '坑', '了', '？', '[SEP]']\n",
      "00:25:04.293624 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:04.293908 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['倚', '天', '屠', '龙', '记', ',', '任', '贤', '齐', ',...'业', ',', '卫', '子', '夫', ',', '美', '人', '心', '计']\n",
      "00:25:04.294789 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '因', '任', '贤', '齐', '而', '迅', '速', '蹿'... '卫', '子', '夫', ',', '美', '人', '心', '计', '[SEP]']\n",
      "00:25:04.295148 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 35\n",
      "00:25:04.295417 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1728, 818, 6570, 7970, 5445, 6813, 6862, 6...310, 2094, 1923, 117, 5401, 782, 2552, 6369, 102]\n",
      "00:25:04.295691 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1728,  818, 6570, 7970, 5445, 6813...       1923,  117, 5401,  782, 2552, 6369,  102])\n",
      "00:25:04.295962 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.296938 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.298120 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.299649 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1728,  818, 6570, 7970, 5445, 681... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011941\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 130\n",
      "00:25:04.302883 call        14     def __getitem__(self, idx):\n",
      "00:25:04.302957 line        15         if self.mode == \"test\":\n",
      "00:25:04.302994 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '范冰冰这次推荐开塞露，你会抢购吗？上次她推荐后的面膜直接断货'\n",
      "New var:....... text_b = '范冰冰,面膜,开塞露,时尚界'\n",
      "New var:....... label = 0\n",
      "00:25:04.303574 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.303780 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.303869 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.304109 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['范', '冰', '冰', '这', '次', '推', '荐', '开', '塞', '露...'推', '荐', '后', '的', '面', '膜', '直', '接', '断', '货']\n",
      "00:25:04.304897 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '范', '冰', '冰', '这', '次', '推', '荐', '开'... '后', '的', '面', '膜', '直', '接', '断', '货', '[SEP]']\n",
      "00:25:04.305133 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:04.305362 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['范', '冰', '冰', ',', '面', '膜', ',', '开', '塞', '露', ',', '时', '尚', '界']\n",
      "00:25:04.305863 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '范', '冰', '冰', '这', '次', '推', '荐', '开'... ',', '开', '塞', '露', ',', '时', '尚', '界', '[SEP]']\n",
      "00:25:04.306194 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 15\n",
      "00:25:04.306439 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5745, 1102, 1102, 6821, 3613, 2972, 5773, ...17, 2458, 1853, 7463, 117, 3198, 2213, 4518, 102]\n",
      "00:25:04.306705 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5745, 1102, 1102, 6821, 3613, 2972... 2458, 1853, 7463,  117, 3198, 2213, 4518,  102])\n",
      "00:25:04.306962 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.307690 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.308517 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.309649 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5745, 1102, 1102, 6821, 3613, 297... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009114\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 131\n",
      "00:25:04.312026 call        14     def __getitem__(self, idx):\n",
      "00:25:04.312101 line        15         if self.mode == \"test\":\n",
      "00:25:04.312140 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '20年前捐卵 卵子成人 她俩相认 惊奇巧合这样多！'\n",
      "New var:....... text_b = '长大成人,捐卵,伊莉莎白,不孕症,南加大'\n",
      "New var:....... label = 0\n",
      "00:25:04.312647 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.312747 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.312830 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.313172 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['20', '年', '前', '捐', '卵', '卵', '子', '成', '人', '...'相', '认', '惊', '奇', '巧', '合', '这', '样', '多', '！']\n",
      "00:25:04.313828 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '20', '年', '前', '捐', '卵', '卵', '子', '成... '惊', '奇', '巧', '合', '这', '样', '多', '！', '[SEP]']\n",
      "00:25:04.314070 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:25:04.314303 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['长', '大', '成', '人', ',', '捐', '卵', ',', '伊', '莉', '莎', '白', ',', '不', '孕', '症', ',', '南', '加', '大']\n",
      "00:25:04.314909 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '20', '年', '前', '捐', '卵', '卵', '子', '成... ',', '不', '孕', '症', ',', '南', '加', '大', '[SEP]']\n",
      "00:25:04.315156 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:25:04.315476 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 8113, 2399, 1184, 2935, 1317, 1317, 2094, ...117, 679, 2097, 4568, 117, 1298, 1217, 1920, 102]\n",
      "00:25:04.315772 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 8113, 2399, 1184, 2935, 1317, 1317...  679, 2097, 4568,  117, 1298, 1217, 1920,  102])\n",
      "00:25:04.316062 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.316922 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.317868 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.318945 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 8113, 2399, 1184, 2935, 1317, 131... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009344\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 132\n",
      "00:25:04.321398 call        14     def __getitem__(self, idx):\n",
      "00:25:04.321470 line        15         if self.mode == \"test\":\n",
      "00:25:04.321509 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '刘德华的吃相、刘晓庆的吃相、徐峥的吃相，最后一个最丑让人恶心'\n",
      "New var:....... text_b = '赵薇,高晓松,徐峥,刘德华,刘晓庆'\n",
      "New var:....... label = 0\n",
      "00:25:04.321996 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.322091 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.322164 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.322498 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['刘', '德', '华', '的', '吃', '相', '、', '刘', '晓', '庆...'最', '后', '一', '个', '最', '丑', '让', '人', '恶', '心']\n",
      "00:25:04.323299 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刘', '德', '华', '的', '吃', '相', '、', '刘'... '一', '个', '最', '丑', '让', '人', '恶', '心', '[SEP]']\n",
      "00:25:04.323533 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:04.323770 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['赵', '薇', ',', '高', '晓', '松', ',', '徐', '峥', ',', '刘', '德', '华', ',', '刘', '晓', '庆']\n",
      "00:25:04.324414 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刘', '德', '华', '的', '吃', '相', '、', '刘'... ',', '刘', '德', '华', ',', '刘', '晓', '庆', '[SEP]']\n",
      "00:25:04.324777 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:04.325022 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1155, 2548, 1290, 4638, 1391, 4685, 510, 1...17, 1155, 2548, 1290, 117, 1155, 3236, 2412, 102]\n",
      "00:25:04.325315 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1155, 2548, 1290, 4638, 1391, 4685...2548, 1290,  117, 1155, 3236,        2412,  102])\n",
      "00:25:04.325563 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.326243 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:25:04.327058 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.328411 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1155, 2548, 1290, 4638, 1391, 468...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009585\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 133\n",
      "00:25:04.331015 call        14     def __getitem__(self, idx):\n",
      "00:25:04.331105 line        15         if self.mode == \"test\":\n",
      "00:25:04.331164 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '女儿跳跳和俏俏百天，谢娜感性发言，张杰唱的歌却让人想起金玟岐'\n",
      "New var:....... text_b = '张杰,小天使,金玟岐,谢娜,这首歌'\n",
      "New var:....... label = 0\n",
      "00:25:04.331701 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.331912 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.331999 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.332238 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['女', '儿', '跳', '跳', '和', '俏', '俏', '百', '天', '，...'的', '歌', '却', '让', '人', '想', '起', '金', '玟', '岐']\n",
      "00:25:04.333045 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '儿', '跳', '跳', '和', '俏', '俏', '百'... '却', '让', '人', '想', '起', '金', '玟', '岐', '[SEP]']\n",
      "00:25:04.333285 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:04.333524 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['张', '杰', ',', '小', '天', '使', ',', '金', '玟', '岐', ',', '谢', '娜', ',', '这', '首', '歌']\n",
      "00:25:04.334087 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '儿', '跳', '跳', '和', '俏', '俏', '百'... '岐', ',', '谢', '娜', ',', '这', '首', '歌', '[SEP]']\n",
      "00:25:04.334439 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:04.334713 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1957, 1036, 6663, 6663, 1469, 918, 918, 46...262, 117, 6468, 2025, 117, 6821, 7674, 3625, 102]\n",
      "00:25:04.334988 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1957, 1036, 6663, 6663, 1469,  918...6468, 2025,  117, 6821, 7674,        3625,  102])\n",
      "00:25:04.335252 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.335951 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:25:04.337140 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.338325 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1957, 1036, 6663, 6663, 1469,  91...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009795\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 134\n",
      "00:25:04.340839 call        14     def __getitem__(self, idx):\n",
      "00:25:04.340910 line        15         if self.mode == \"test\":\n",
      "00:25:04.340947 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《21克拉》谈钱说爱有惊喜'\n",
      "New var:....... text_b = '精打细算,十二公民,夏洛特烦恼,驴得水,21克拉,喜剧片,刘佳音,王继伟,商业片'\n",
      "New var:....... label = 0\n",
      "00:25:04.341446 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.341540 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.341614 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.341933 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '21', '克', '拉', '》', '谈', '钱', '说', '爱', '有', '惊', '喜']\n",
      "00:25:04.342427 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '21', '克', '拉', '》', '谈', '钱', '说', '爱', '有', '惊', '喜', '[SEP]']\n",
      "00:25:04.342651 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 14\n",
      "00:25:04.342896 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['精', '打', '细', '算', ',', '十', '二', '公', '民', ',...'佳', '音', ',', '王', '继', '伟', ',', '商', '业', '片']\n",
      "00:25:04.343849 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '21', '克', '拉', '》', '谈', '钱', '说... ',', '王', '继', '伟', ',', '商', '业', '片', '[SEP]']\n",
      "00:25:04.344200 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 40\n",
      "00:25:04.344513 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 8128, 1046, 2861, 518, 6448, 7178, 64... 117, 4374, 5326, 836, 117, 1555, 689, 4275, 102]\n",
      "00:25:04.344793 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 8128, 1046, 2861,  518, 6448...5326,         836,  117, 1555,  689, 4275,  102])\n",
      "00:25:04.345069 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.345948 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:25:04.346799 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.348126 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 8128, 1046, 2861,  518, 644...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010047\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 135\n",
      "00:25:04.350919 call        14     def __getitem__(self, idx):\n",
      "00:25:04.350992 line        15         if self.mode == \"test\":\n",
      "00:25:04.351030 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '乐嘉节目录制现场大发脾气，拍桌怒吼，连金星都被吓跑了'\n",
      "New var:....... text_b = '发脾气,乐嘉,金星'\n",
      "New var:....... label = 0\n",
      "00:25:04.351552 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.351768 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.351856 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.352084 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['乐', '嘉', '节', '目', '录', '制', '现', '场', '大', '发...'吼', '，', '连', '金', '星', '都', '被', '吓', '跑', '了']\n",
      "00:25:04.352820 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '乐', '嘉', '节', '目', '录', '制', '现', '场'... '连', '金', '星', '都', '被', '吓', '跑', '了', '[SEP]']\n",
      "00:25:04.353080 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:04.353356 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['发', '脾', '气', ',', '乐', '嘉', ',', '金', '星']\n",
      "00:25:04.353808 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '乐', '嘉', '节', '目', '录', '制', '现', '场'... '脾', '气', ',', '乐', '嘉', ',', '金', '星', '[SEP]']\n",
      "00:25:04.354150 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 10\n",
      "00:25:04.354401 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 727, 1649, 5688, 4680, 2497, 1169, 4385, 1...5569, 3698, 117, 727, 1649, 117, 7032, 3215, 102]\n",
      "00:25:04.354676 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  727, 1649, 5688, 4680, 2497, 1169... 117,  727, 1649,  117, 7032,        3215,  102])\n",
      "00:25:04.354940 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.355582 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.356304 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.357507 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  727, 1649, 5688, 4680, 2497, 116... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008609\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 136\n",
      "00:25:04.359557 call        14     def __getitem__(self, idx):\n",
      "00:25:04.359629 line        15         if self.mode == \"test\":\n",
      "00:25:04.359667 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '八九十年代港台10位女歌星一览，周慧敏真是又靓又会唱歌'\n",
      "New var:....... text_b = '情人知己,千千阙歌,周慧敏,邝美云,祝福,初恋情人,潇洒走一回,一人有一个梦想,加减乘除,发誓,焚心以火'\n",
      "New var:....... label = 0\n",
      "00:25:04.360184 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.360277 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.360352 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.360555 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['八', '九', '十', '年', '代', '港', '台', '10', '位', '...'慧', '敏', '真', '是', '又', '靓', '又', '会', '唱', '歌']\n",
      "00:25:04.361371 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '八', '九', '十', '年', '代', '港', '台', '10... '真', '是', '又', '靓', '又', '会', '唱', '歌', '[SEP]']\n",
      "00:25:04.361601 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:04.361829 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['情', '人', '知', '己', ',', '千', '千', '阙', '歌', ',...'乘', '除', ',', '发', '誓', ',', '焚', '心', '以', '火']\n",
      "00:25:04.362985 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '八', '九', '十', '年', '代', '港', '台', '10... ',', '发', '誓', ',', '焚', '心', '以', '火', '[SEP]']\n",
      "00:25:04.363330 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 53\n",
      "00:25:04.363575 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1061, 736, 1282, 2399, 807, 3949, 1378, 81...117, 1355, 6292, 117, 4190, 2552, 809, 4125, 102]\n",
      "00:25:04.363866 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1061,  736, 1282, 2399,  807, 3949... 1355, 6292,  117, 4190, 2552,  809, 4125,  102])\n",
      "00:25:04.364160 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.365079 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.366095 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.367664 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1061,  736, 1282, 2399,  807, 394...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011512\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 137\n",
      "00:25:04.371097 call        14     def __getitem__(self, idx):\n",
      "00:25:04.371168 line        15         if self.mode == \"test\":\n",
      "00:25:04.371207 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '大张伟自曝焦虑痛苦十几年？他的歌和段子其实一直都是自我治疗……'\n",
      "New var:....... text_b = '毒鸡汤,金凯瑞：我需要色彩,天天向上,变相怪杰,今晚80后脱口秀,花儿乐队,摇滚,抑郁症,大张伟,楚门的世界'\n",
      "New var:....... label = 0\n",
      "00:25:04.371702 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.371905 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.371983 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.372210 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['大', '张', '伟', '自', '曝', '焦', '虑', '痛', '苦', '十..., '都', '是', '自', '我', '治', '疗', '[UNK]', '[UNK]']\n",
      "00:25:04.372997 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '大', '张', '伟', '自', '曝', '焦', '虑', '痛'...', '自', '我', '治', '疗', '[UNK]', '[UNK]', '[SEP]']\n",
      "00:25:04.373231 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 33\n",
      "00:25:04.373490 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['毒', '鸡', '汤', ',', '金', '凯', '瑞', '：', '我', '需...',', '大', '张', '伟', ',', '楚', '门', '的', '世', '界']\n",
      "00:25:04.374661 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '大', '张', '伟', '自', '曝', '焦', '虑', '痛'... '张', '伟', ',', '楚', '门', '的', '世', '界', '[SEP]']\n",
      "00:25:04.375000 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 54\n",
      "00:25:04.375266 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1920, 2476, 836, 5632, 3284, 4193, 5991, 4...2476, 836, 117, 3504, 7305, 4638, 686, 4518, 102]\n",
      "00:25:04.375550 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1920, 2476,  836, 5632, 3284, 4193... 117, 3504, 7305, 4638,         686, 4518,  102])\n",
      "00:25:04.375824 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.376791 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.377844 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.380085 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1920, 2476,  836, 5632, 3284, 419... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.012682\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 138\n",
      "00:25:04.383815 call        14     def __getitem__(self, idx):\n",
      "00:25:04.383897 line        15         if self.mode == \"test\":\n",
      "00:25:04.383941 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '闫妮一身露肩裙现身电影发布会，这颜值你猜她多大了？'\n",
      "New var:....... text_b = '佟湘玉,大魔术师,三七撞上二十一,画壁,炮走红,中国电视剧,武林外传,迷妹“罗”曼史,北风那个吹,中国人民解放军空军'\n",
      "New var:....... label = 0\n",
      "00:25:04.384438 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.384545 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.384651 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.384923 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['闫', '妮', '一', '身', '露', '肩', '裙', '现', '身', '电...'这', '颜', '值', '你', '猜', '她', '多', '大', '了', '？']\n",
      "00:25:04.385629 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '闫', '妮', '一', '身', '露', '肩', '裙', '现'... '值', '你', '猜', '她', '多', '大', '了', '？', '[SEP]']\n",
      "00:25:04.386130 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:25:04.386369 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['佟', '湘', '玉', ',', '大', '魔', '术', '师', ',', '三...',', '中', '国', '人', '民', '解', '放', '军', '空', '军']\n",
      "00:25:04.387630 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '闫', '妮', '一', '身', '露', '肩', '裙', '现'... '国', '人', '民', '解', '放', '军', '空', '军', '[SEP]']\n",
      "00:25:04.387883 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 59\n",
      "00:25:04.388141 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7307, 1984, 671, 6716, 7463, 5504, 6170, 4...44, 782, 3696, 6237, 3123, 1092, 4958, 1092, 102]\n",
      "00:25:04.388542 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7307, 1984,  671, 6716, 7463, 5504...3696, 6237, 3123, 1092, 4958,        1092,  102])\n",
      "00:25:04.388827 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.389783 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.390929 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.393120 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7307, 1984,  671, 6716, 7463, 550... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013338\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 139\n",
      "00:25:04.397196 call        14     def __getitem__(self, idx):\n",
      "00:25:04.397288 line        15         if self.mode == \"test\":\n",
      "00:25:04.397333 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '这些热卖国产片竟是限制级电影？以为是合家欢，结果很尴尬！'\n",
      "New var:....... text_b = '西游伏妖篇,道士下山,南京！南京！,功夫,金陵十三钗,投名状,红海行动,限制级,R级片,西游降魔篇,英雄,国产片,港囧,三枪拍案惊奇,满城尽带黄金甲'\n",
      "New var:....... label = 0\n",
      "00:25:04.397911 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.398018 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.398104 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.398337 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['这', '些', '热', '卖', '国', '产', '片', '竟', '是', '限...'合', '家', '欢', '，', '结', '果', '很', '尴', '尬', '！']\n",
      "00:25:04.399153 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '些', '热', '卖', '国', '产', '片', '竟'... '欢', '，', '结', '果', '很', '尴', '尬', '！', '[SEP]']\n",
      "00:25:04.399517 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:25:04.399798 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['西', '游', '伏', '妖', '篇', ',', '道', '士', '下', '山...'惊', '奇', ',', '满', '城', '尽', '带', '黄', '金', '甲']\n",
      "00:25:04.401527 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '些', '热', '卖', '国', '产', '片', '竟'... ',', '满', '城', '尽', '带', '黄', '金', '甲', '[SEP]']\n",
      "00:25:04.401900 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 75\n",
      "00:25:04.402163 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6821, 763, 4178, 1297, 1744, 772, 4275, 49...7, 4007, 1814, 2226, 2372, 7942, 7032, 4508, 102]\n",
      "00:25:04.402420 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6821,  763, 4178, 1297, 1744,  772... 4007, 1814, 2226, 2372, 7942, 7032, 4508,  102])\n",
      "00:25:04.402692 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.404069 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.405417 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.408334 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6821,  763, 4178, 1297, 1744,  77...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.015882\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 140\n",
      "00:25:04.413115 call        14     def __getitem__(self, idx):\n",
      "00:25:04.413195 line        15         if self.mode == \"test\":\n",
      "00:25:04.413236 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《甄嬛传》里皇上宠爱那么多人，为何对安陵容最绝情？'\n",
      "New var:....... text_b = '安陵容,安小鸟,皇帝,后宫众人,让皇帝'\n",
      "New var:....... label = 0\n",
      "00:25:04.413830 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.413923 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.414002 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.414250 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '甄', '嬛', '传', '》', '里', '皇', '上', '宠', '爱...'为', '何', '对', '安', '陵', '容', '最', '绝', '情', '？']\n",
      "00:25:04.414976 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '甄', '嬛', '传', '》', '里', '皇', '上'... '对', '安', '陵', '容', '最', '绝', '情', '？', '[SEP]']\n",
      "00:25:04.415319 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:25:04.415558 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['安', '陵', '容', ',', '安', '小', '鸟', ',', '皇', '帝', ',', '后', '宫', '众', '人', ',', '让', '皇', '帝']\n",
      "00:25:04.416153 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '甄', '嬛', '传', '》', '里', '皇', '上'... '后', '宫', '众', '人', ',', '让', '皇', '帝', '[SEP]']\n",
      "00:25:04.416417 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:04.416662 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 4488, 2083, 837, 518, 7027, 4640, 677...1400, 2151, 830, 782, 117, 6375, 4640, 2370, 102]\n",
      "00:25:04.416923 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 4488, 2083,  837,  518, 7027... 2151,  830,  782,  117, 6375, 4640, 2370,  102])\n",
      "00:25:04.417182 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.418055 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.418789 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.419895 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 4488, 2083,  837,  518, 702... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009299\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 141\n",
      "00:25:04.422441 call        14     def __getitem__(self, idx):\n",
      "00:25:04.422512 line        15         if self.mode == \"test\":\n",
      "00:25:04.422549 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '王霏霏变街舞冠军韩宇“迷妹” 网友：期待两人合作'\n",
      "New var:....... text_b = '素手遮天,选择游戏,越跳越美丽,这就是街舞,Dancing With The Stars,不要急Fantasy,韩宇,王霏霏,浙江卫视,MAMA,The'\n",
      "New var:....... label = 0\n",
      "00:25:04.423691 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.423896 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.423981 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.424216 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['王', '霏', '霏', '变', '街', '舞', '冠', '军', '韩', '宇...K]', '网', '友', '：', '期', '待', '两', '人', '合', '作']\n",
      "00:25:04.424889 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '霏', '霏', '变', '街', '舞', '冠', '军'... '友', '：', '期', '待', '两', '人', '合', '作', '[SEP]']\n",
      "00:25:04.425139 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:04.425382 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['素', '手', '遮', '天', ',', '选', '择', '游', '戏', ',...',', '浙', '江', '卫', '视', ',', 'mama', ',', 'the']\n",
      "00:25:04.426639 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '霏', '霏', '变', '街', '舞', '冠', '军'... '江', '卫', '视', ',', 'mama', ',', 'the', '[SEP]']\n",
      "00:25:04.427011 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 50\n",
      "00:25:04.427268 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4374, 7454, 7454, 1359, 6125, 5659, 1094, ...51, 3736, 1310, 6228, 117, 13186, 117, 8174, 102]\n",
      "00:25:04.428194 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  4374,  7454,  7454,  1359,  6125... 6228,          117, 13186,   117,  8174,   102])\n",
      "00:25:04.428458 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.429504 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:25:04.430436 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.432207 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  4374,  7454,  7454,  1359,  612...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013134\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 142\n",
      "00:25:04.435607 call        14     def __getitem__(self, idx):\n",
      "00:25:04.435680 line        15         if self.mode == \"test\":\n",
      "00:25:04.435719 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '老佛爷为何不喜欢紫薇？这么简单的道理连知画都懂，紫薇却不明白'\n",
      "New var:....... text_b = '老佛爷,未婚先孕,晴儿,紫薇,甄嬛传,夏雨荷'\n",
      "New var:....... label = 0\n",
      "00:25:04.436787 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.436889 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.436965 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.437192 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['老', '佛', '爷', '为', '何', '不', '喜', '欢', '紫', '薇...'画', '都', '懂', '，', '紫', '薇', '却', '不', '明', '白']\n",
      "00:25:04.437975 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '老', '佛', '爷', '为', '何', '不', '喜', '欢'... '懂', '，', '紫', '薇', '却', '不', '明', '白', '[SEP]']\n",
      "00:25:04.438303 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:04.438534 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['老', '佛', '爷', ',', '未', '婚', '先', '孕', ',', '晴...'紫', '薇', ',', '甄', '嬛', '传', ',', '夏', '雨', '荷']\n",
      "00:25:04.439165 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '老', '佛', '爷', '为', '何', '不', '喜', '欢'... ',', '甄', '嬛', '传', ',', '夏', '雨', '荷', '[SEP]']\n",
      "00:25:04.439396 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:25:04.439660 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5439, 867, 4267, 711, 862, 679, 1599, 3614...117, 4488, 2083, 837, 117, 1909, 7433, 5792, 102]\n",
      "00:25:04.439922 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5439,  867, 4267,  711,  862,  679...       2083,  837,  117, 1909, 7433, 5792,  102])\n",
      "00:25:04.440179 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.441140 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.442015 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.443577 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5439,  867, 4267,  711,  862,  67...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010596\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 143\n",
      "00:25:04.446238 call        14     def __getitem__(self, idx):\n",
      "00:25:04.446318 line        15         if self.mode == \"test\":\n",
      "00:25:04.446360 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '刀郎一首《驼铃》，全场观众坐都不住了，最后叫停音乐与观众合唱'\n",
      "New var:....... text_b = '刀郎,驼铃'\n",
      "New var:....... label = 0\n",
      "00:25:04.446933 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.447062 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.447145 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.447395 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['刀', '郎', '一', '首', '《', '驼', '铃', '》', '，', '全...'后', '叫', '停', '音', '乐', '与', '观', '众', '合', '唱']\n",
      "00:25:04.448115 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刀', '郎', '一', '首', '《', '驼', '铃', '》'... '停', '音', '乐', '与', '观', '众', '合', '唱', '[SEP]']\n",
      "00:25:04.448501 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:04.448690 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['刀', '郎', ',', '驼', '铃']\n",
      "00:25:04.448995 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刀', '郎', '一', '首', '《', '驼', '铃', '》'..., '唱', '[SEP]', '刀', '郎', ',', '驼', '铃', '[SEP]']\n",
      "00:25:04.449161 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 6\n",
      "00:25:04.449540 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1143, 6947, 671, 7674, 517, 7729, 7190, 51...394, 1548, 102, 1143, 6947, 117, 7729, 7190, 102]\n",
      "00:25:04.449837 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1143, 6947,  671, 7674,  517, 7729... 102, 1143, 6947,  117, 7729,        7190,  102])\n",
      "00:25:04.450047 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.450916 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.451971 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.453067 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1143, 6947,  671, 7674,  517, 772... 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008684\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 144\n",
      "00:25:04.454949 call        14     def __getitem__(self, idx):\n",
      "00:25:04.455018 line        15         if self.mode == \"test\":\n",
      "00:25:04.455055 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '被《向往的生活》何老师这几个小细节给暖到了，你发现了吗？'\n",
      "New var:....... text_b = '彭彭,综艺'\n",
      "New var:....... label = 0\n",
      "00:25:04.455544 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.455708 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.455785 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.455965 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['被', '《', '向', '往', '的', '生', '活', '》', '何', '老...'暖', '到', '了', '，', '你', '发', '现', '了', '吗', '？']\n",
      "00:25:04.456712 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '被', '《', '向', '往', '的', '生', '活', '》'... '了', '，', '你', '发', '现', '了', '吗', '？', '[SEP]']\n",
      "00:25:04.456901 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:25:04.457084 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['彭', '彭', ',', '综', '艺']\n",
      "00:25:04.457380 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '被', '《', '向', '往', '的', '生', '活', '》'..., '？', '[SEP]', '彭', '彭', ',', '综', '艺', '[SEP]']\n",
      "00:25:04.457572 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 6\n",
      "00:25:04.457823 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6158, 517, 1403, 2518, 4638, 4495, 3833, 5...408, 8043, 102, 2510, 2510, 117, 5341, 5686, 102]\n",
      "00:25:04.458024 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6158,  517, 1403, 2518, 4638, 4495... 8043,  102, 2510, 2510,  117, 5341, 5686,  102])\n",
      "00:25:04.458270 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.458777 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0,        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.459342 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.460328 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6158,  517, 1403, 2518, 4638, 449... 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007140\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 145\n",
      "00:25:04.462115 call        14     def __getitem__(self, idx):\n",
      "00:25:04.462181 line        15         if self.mode == \"test\":\n",
      "00:25:04.462218 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '十二生肖传奇；小孩用龙鳞救了老爷爷一命，老爷爷突然就变成龙了'\n",
      "New var:....... text_b = '老爷爷,十二生肖传奇'\n",
      "New var:....... label = 0\n",
      "00:25:04.462713 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.462911 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.462985 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.463184 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['十', '二', '生', '肖', '传', '奇', '；', '小', '孩', '用...'老', '爷', '爷', '突', '然', '就', '变', '成', '龙', '了']\n",
      "00:25:04.463880 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '十', '二', '生', '肖', '传', '奇', '；', '小'... '爷', '突', '然', '就', '变', '成', '龙', '了', '[SEP]']\n",
      "00:25:04.464087 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:04.464272 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['老', '爷', '爷', ',', '十', '二', '生', '肖', '传', '奇']\n",
      "00:25:04.464655 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '十', '二', '生', '肖', '传', '奇', '；', '小'... '爷', ',', '十', '二', '生', '肖', '传', '奇', '[SEP]']\n",
      "00:25:04.464908 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 11\n",
      "00:25:04.465164 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1282, 753, 4495, 5494, 837, 1936, 8039, 22...4267, 117, 1282, 753, 4495, 5494, 837, 1936, 102]\n",
      "00:25:04.465367 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1282,  753, 4495, 5494,  837, 1936...       1282,  753, 4495, 5494,  837, 1936,  102])\n",
      "00:25:04.465575 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.466180 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.466725 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.467770 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1282,  753, 4495, 5494,  837, 193... 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007629\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 146\n",
      "00:25:04.469771 call        14     def __getitem__(self, idx):\n",
      "00:25:04.469837 line        15         if self.mode == \"test\":\n",
      "00:25:04.469939 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '有一种爱情叫做汪峰章子怡！'\n",
      "New var:....... text_b = '章子怡,汪峰'\n",
      "New var:....... label = 0\n",
      "00:25:04.470452 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.470543 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.470614 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.470789 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['有', '一', '种', '爱', '情', '叫', '做', '汪', '峰', '章', '子', '怡', '！']\n",
      "00:25:04.471217 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '有', '一', '种', '爱', '情', '叫', '做', '汪', '峰', '章', '子', '怡', '！', '[SEP]']\n",
      "00:25:04.471450 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "00:25:04.471624 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['章', '子', '怡', ',', '汪', '峰']\n",
      "00:25:04.471898 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '有', '一', '种', '爱', '情', '叫', '做', '汪'..., '[SEP]', '章', '子', '怡', ',', '汪', '峰', '[SEP]']\n",
      "00:25:04.472124 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 7\n",
      "00:25:04.472309 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3300, 671, 4905, 4263, 2658, 1373, 976, 37...013, 102, 4995, 2094, 2592, 117, 3742, 2292, 102]\n",
      "00:25:04.472497 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3300,  671, 4905, 4263, 2658, 1373...  102, 4995, 2094, 2592,  117, 3742, 2292,  102])\n",
      "00:25:04.472695 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.473113 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.473582 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.474563 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3300,  671, 4905, 4263, 2658, 137... 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006245\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 147\n",
      "00:25:04.476048 call        14     def __getitem__(self, idx):\n",
      "00:25:04.476120 line        15         if self.mode == \"test\":\n",
      "00:25:04.476158 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '戛纳红毯女星怪相，有人假摔走光，有人耍无赖！'\n",
      "New var:....... text_b = '戛纳,走红毯,红毯,女星,戛纳电影节'\n",
      "New var:....... label = 0\n",
      "00:25:04.476734 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.476900 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.477046 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.477219 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['戛', '纳', '红', '毯', '女', '星', '怪', '相', '，', '有...'摔', '走', '光', '，', '有', '人', '耍', '无', '赖', '！']\n",
      "00:25:04.477814 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '戛', '纳', '红', '毯', '女', '星', '怪', '相'... '光', '，', '有', '人', '耍', '无', '赖', '！', '[SEP]']\n",
      "00:25:04.478053 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:25:04.478235 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['戛', '纳', ',', '走', '红', '毯', ',', '红', '毯', ',', '女', '星', ',', '戛', '纳', '电', '影', '节']\n",
      "00:25:04.478751 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '戛', '纳', '红', '毯', '女', '星', '怪', '相'... '女', '星', ',', '戛', '纳', '电', '影', '节', '[SEP]']\n",
      "00:25:04.478944 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:25:04.479231 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2775, 5287, 5273, 3691, 1957, 3215, 2597, ...57, 3215, 117, 2775, 5287, 4510, 2512, 5688, 102]\n",
      "00:25:04.479468 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2775, 5287, 5273, 3691, 1957, 3215...        117, 2775, 5287, 4510, 2512, 5688,  102])\n",
      "00:25:04.479691 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.480297 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.480860 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.481853 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2775, 5287, 5273, 3691, 1957, 321... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008167\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 148\n",
      "00:25:04.484240 call        14     def __getitem__(self, idx):\n",
      "00:25:04.484308 line        15         if self.mode == \"test\":\n",
      "00:25:04.484345 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '那些电影镜头中经典的表白镜头 台词扎心了'\n",
      "New var:....... text_b = '爱上你,电影镜头,一辈子,电影'\n",
      "New var:....... label = 0\n",
      "00:25:04.484853 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.484946 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.485018 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.485159 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['那', '些', '电', '影', '镜', '头', '中', '经', '典', '的', '表', '白', '镜', '头', '台', '词', '扎', '心', '了']\n",
      "00:25:04.485780 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '那', '些', '电', '影', '镜', '头', '中', '经'... '白', '镜', '头', '台', '词', '扎', '心', '了', '[SEP]']\n",
      "00:25:04.485919 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:25:04.486060 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['爱', '上', '你', ',', '电', '影', '镜', '头', ',', '一', '辈', '子', ',', '电', '影']\n",
      "00:25:04.486781 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '那', '些', '电', '影', '镜', '头', '中', '经'... '头', ',', '一', '辈', '子', ',', '电', '影', '[SEP]']\n",
      "00:25:04.487078 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 16\n",
      "00:25:04.487353 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6929, 763, 4510, 2512, 7262, 1928, 704, 53...1928, 117, 671, 6777, 2094, 117, 4510, 2512, 102]\n",
      "00:25:04.487652 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6929,  763, 4510, 2512, 7262, 1928... 671, 6777, 2094,  117, 4510, 2512,         102])\n",
      "00:25:04.488201 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.489104 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.489754 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.490958 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6929,  763, 4510, 2512, 7262, 192... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008834\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 149\n",
      "00:25:04.493108 call        14     def __getitem__(self, idx):\n",
      "00:25:04.493180 line        15         if self.mode == \"test\":\n",
      "00:25:04.493218 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《跑男6》家庭背景与李晨相似的她也即将加入跑男'\n",
      "New var:....... text_b = '家庭背景,韩雪,范冰冰,跑男6,李晨,跑男'\n",
      "New var:....... label = 0\n",
      "00:25:04.494869 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.495089 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.495284 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.495539 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '跑', '男', '6', '》', '家', '庭', '背', '景', '与...'似', '的', '她', '也', '即', '将', '加', '入', '跑', '男']\n",
      "00:25:04.496175 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '跑', '男', '6', '》', '家', '庭', '背'... '她', '也', '即', '将', '加', '入', '跑', '男', '[SEP]']\n",
      "00:25:04.496413 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:04.496642 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['家', '庭', '背', '景', ',', '韩', '雪', ',', '范', '冰...',', '跑', '男', '6', ',', '李', '晨', ',', '跑', '男']\n",
      "00:25:04.497255 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '跑', '男', '6', '》', '家', '庭', '背'... '男', '6', ',', '李', '晨', ',', '跑', '男', '[SEP]']\n",
      "00:25:04.497669 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 22\n",
      "00:25:04.497917 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 6651, 4511, 127, 518, 2157, 2431, 552...4511, 127, 117, 3330, 3247, 117, 6651, 4511, 102]\n",
      "00:25:04.498171 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 6651, 4511,  127,  518, 2157...  127,  117, 3330, 3247,  117, 6651, 4511,  102])\n",
      "00:25:04.498432 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.499103 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.499875 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.501059 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 6651, 4511,  127,  518, 215... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010567\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 150\n",
      "00:25:04.503704 call        14     def __getitem__(self, idx):\n",
      "00:25:04.503773 line        15         if self.mode == \"test\":\n",
      "00:25:04.503810 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '中国歌手动不动就是世界巡演！网友：实际上真正做到的只有这2位'\n",
      "New var:....... text_b = '我是歌手,歌手,A-Lin,多伦多,黄丽玲,世界巡回演唱会'\n",
      "New var:....... label = 0\n",
      "00:25:04.504402 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.504609 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.504687 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.504908 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['中', '国', '歌', '手', '动', '不', '动', '就', '是', '世...'真', '正', '做', '到', '的', '只', '有', '这', '2', '位']\n",
      "00:25:04.505686 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '中', '国', '歌', '手', '动', '不', '动', '就'... '做', '到', '的', '只', '有', '这', '2', '位', '[SEP]']\n",
      "00:25:04.505921 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:04.506151 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['我', '是', '歌', '手', ',', '歌', '手', ',', 'a', '-...'丽', '玲', ',', '世', '界', '巡', '回', '演', '唱', '会']\n",
      "00:25:04.506880 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '中', '国', '歌', '手', '动', '不', '动', '就'... ',', '世', '界', '巡', '回', '演', '唱', '会', '[SEP]']\n",
      "00:25:04.507235 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 28\n",
      "00:25:04.507478 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 704, 1744, 3625, 2797, 1220, 679, 1220, 22...117, 686, 4518, 2337, 1726, 4028, 1548, 833, 102]\n",
      "00:25:04.507747 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  704, 1744, 3625, 2797, 1220,  679...  686, 4518, 2337, 1726, 4028, 1548,  833,  102])\n",
      "00:25:04.508010 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.508757 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.509626 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.510971 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  704, 1744, 3625, 2797, 1220,  67... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009657\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 151\n",
      "00:25:04.513389 call        14     def __getitem__(self, idx):\n",
      "00:25:04.513459 line        15         if self.mode == \"test\":\n",
      "00:25:04.513496 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '刘若英的似水年华，离不开张艾嘉、陈升、黄磊、周迅……'\n",
      "New var:....... text_b = '陈国富,今天不回家,唐山大地震,人间四月天,张艾嘉,如果·爱,黄磊,少女小渔,刘若英,似水年华,日落紫禁城,后我,如果爱,陈升,奶茶,天下无贼,无言的山丘,一个好爸爸'\n",
      "New var:....... label = 0\n",
      "00:25:04.514047 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.514142 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.514219 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.514583 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['刘', '若', '英', '的', '似', '水', '年', '华', '，', '离..., '、', '黄', '磊', '、', '周', '迅', '[UNK]', '[UNK]']\n",
      "00:25:04.515288 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刘', '若', '英', '的', '似', '水', '年', '华'...', '磊', '、', '周', '迅', '[UNK]', '[UNK]', '[SEP]']\n",
      "00:25:04.515526 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:04.515756 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['陈', '国', '富', ',', '今', '天', '不', '回', '家', ',...'言', '的', '山', '丘', ',', '一', '个', '好', '爸', '爸']\n",
      "00:25:04.517420 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刘', '若', '英', '的', '似', '水', '年', '华'... '山', '丘', ',', '一', '个', '好', '爸', '爸', '[SEP]']\n",
      "00:25:04.517785 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 84\n",
      "00:25:04.518140 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1155, 5735, 5739, 4638, 849, 3717, 2399, 1... 2255, 687, 117, 671, 702, 1962, 4268, 4268, 102]\n",
      "00:25:04.518506 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1155, 5735, 5739, 4638,  849, 3717... 117,  671,  702,        1962, 4268, 4268,  102])\n",
      "00:25:04.518805 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.520001 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.521075 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.523091 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1155, 5735, 5739, 4638,  849, 371... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013844\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 152\n",
      "00:25:04.527268 call        14     def __getitem__(self, idx):\n",
      "00:25:04.527342 line        15         if self.mode == \"test\":\n",
      "00:25:04.527498 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '袁姗姗头戴一副小眼镜现身机场，最抢眼莫过于要间上的包包'\n",
      "New var:....... text_b = '机场,包包袁姗姗头戴一副,袁姗姗,抢眼莫过于,眼镜现身机场'\n",
      "New var:....... label = 0\n",
      "00:25:04.528084 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.528177 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.528369 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.528589 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['袁', '姗', '姗', '头', '戴', '一', '副', '小', '眼', '镜...'眼', '莫', '过', '于', '要', '间', '上', '的', '包', '包']\n",
      "00:25:04.529316 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '袁', '姗', '姗', '头', '戴', '一', '副', '小'... '过', '于', '要', '间', '上', '的', '包', '包', '[SEP]']\n",
      "00:25:04.529689 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:04.529950 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['机', '场', ',', '包', '包', '袁', '姗', '姗', '头', '戴...'莫', '过', '于', ',', '眼', '镜', '现', '身', '机', '场']\n",
      "00:25:04.530717 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '袁', '姗', '姗', '头', '戴', '一', '副', '小'... '于', ',', '眼', '镜', '现', '身', '机', '场', '[SEP]']\n",
      "00:25:04.531021 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 30\n",
      "00:25:04.531275 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6145, 2000, 2000, 1928, 2785, 671, 1199, 2...54, 117, 4706, 7262, 4385, 6716, 3322, 1767, 102]\n",
      "00:25:04.531537 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6145, 2000, 2000, 1928, 2785,  671...  117, 4706, 7262, 4385, 6716, 3322, 1767,  102])\n",
      "00:25:04.531927 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.532694 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.533451 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.534786 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6145, 2000, 2000, 1928, 2785,  67...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010245\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 153\n",
      "00:25:04.537549 call        14     def __getitem__(self, idx):\n",
      "00:25:04.537622 line        15         if self.mode == \"test\":\n",
      "00:25:04.537660 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '在女人节到来之时，送给男人们一首歌，《不要让女人轻易流泪》'\n",
      "New var:....... text_b = '女人节,不要让女人轻易流泪'\n",
      "New var:....... label = 0\n",
      "00:25:04.538255 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.538349 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.538545 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.538772 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['在', '女', '人', '节', '到', '来', '之', '时', '，', '送...'不', '要', '让', '女', '人', '轻', '易', '流', '泪', '》']\n",
      "00:25:04.539541 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '在', '女', '人', '节', '到', '来', '之', '时'... '让', '女', '人', '轻', '易', '流', '泪', '》', '[SEP]']\n",
      "00:25:04.539904 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:04.540142 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['女', '人', '节', ',', '不', '要', '让', '女', '人', '轻', '易', '流', '泪']\n",
      "00:25:04.540633 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '在', '女', '人', '节', '到', '来', '之', '时'... '要', '让', '女', '人', '轻', '易', '流', '泪', '[SEP]']\n",
      "00:25:04.540877 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "00:25:04.541114 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1762, 1957, 782, 5688, 1168, 3341, 722, 31...06, 6375, 1957, 782, 6768, 3211, 3837, 3801, 102]\n",
      "00:25:04.541365 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1762, 1957,  782, 5688, 1168, 3341... 6375, 1957,  782, 6768, 3211, 3837, 3801,  102])\n",
      "00:25:04.541733 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.542614 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.543375 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.544630 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1762, 1957,  782, 5688, 1168, 334... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009685\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 154\n",
      "00:25:04.547264 call        14     def __getitem__(self, idx):\n",
      "00:25:04.547335 line        15         if self.mode == \"test\":\n",
      "00:25:04.547373 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '唐嫣、张天爱、唐艺昕、杨蓉 四人同框，无修图颜值依旧在线'\n",
      "New var:....... text_b = '唐艺昕,张天爱,杨蓉,无修图颜值依旧在线唐嫣,唐嫣'\n",
      "New var:....... label = 0\n",
      "00:25:04.547969 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.548064 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.548140 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.548651 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['唐', '嫣', '、', '张', '天', '爱', '、', '唐', '艺', '昕...'，', '无', '修', '图', '颜', '值', '依', '旧', '在', '线']\n",
      "00:25:04.549327 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '唐', '嫣', '、', '张', '天', '爱', '、', '唐'... '修', '图', '颜', '值', '依', '旧', '在', '线', '[SEP]']\n",
      "00:25:04.549546 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:04.549776 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['唐', '艺', '昕', ',', '张', '天', '爱', ',', '杨', '蓉...'值', '依', '旧', '在', '线', '唐', '嫣', ',', '唐', '嫣']\n",
      "00:25:04.550489 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '唐', '嫣', '、', '张', '天', '爱', '、', '唐'... '旧', '在', '线', '唐', '嫣', ',', '唐', '嫣', '[SEP]']\n",
      "00:25:04.550843 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 26\n",
      "00:25:04.551083 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1538, 2073, 510, 2476, 1921, 4263, 510, 15...91, 1762, 5296, 1538, 2073, 117, 1538, 2073, 102]\n",
      "00:25:04.551335 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1538, 2073,  510, 2476, 1921, 4263...       5296, 1538, 2073,  117, 1538, 2073,  102])\n",
      "00:25:04.551596 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.552411 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.553342 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.554608 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1538, 2073,  510, 2476, 1921, 426...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010204\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 155\n",
      "00:25:04.557496 call        14     def __getitem__(self, idx):\n",
      "00:25:04.557563 line        15         if self.mode == \"test\":\n",
      "00:25:04.557599 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '林心如出席某活动，网友：这个“小妖精”，霍建华一手就能掌控'\n",
      "New var:....... text_b = '掌控,霍建华,林心如,林心如出席,霍建华一手'\n",
      "New var:....... label = 0\n",
      "00:25:04.558107 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.558197 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.558388 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.558606 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['林', '心', '如', '出', '席', '某', '活', '动', '，', '网...'，', '霍', '建', '华', '一', '手', '就', '能', '掌', '控']\n",
      "00:25:04.559352 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '心', '如', '出', '席', '某', '活', '动'... '建', '华', '一', '手', '就', '能', '掌', '控', '[SEP]']\n",
      "00:25:04.559583 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:04.559809 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['掌', '控', ',', '霍', '建', '华', ',', '林', '心', '如...'心', '如', '出', '席', ',', '霍', '建', '华', '一', '手']\n",
      "00:25:04.560434 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '心', '如', '出', '席', '某', '活', '动'... '出', '席', ',', '霍', '建', '华', '一', '手', '[SEP]']\n",
      "00:25:04.560782 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:25:04.561020 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3360, 2552, 1963, 1139, 2375, 3378, 3833, ...139, 2375, 117, 7452, 2456, 1290, 671, 2797, 102]\n",
      "00:25:04.561271 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3360, 2552, 1963, 1139, 2375, 3378... 117,        7452, 2456, 1290,  671, 2797,  102])\n",
      "00:25:04.561525 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.562326 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:25:04.563332 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.564440 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3360, 2552, 1963, 1139, 2375, 337...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008776\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 156\n",
      "00:25:04.566302 call        14     def __getitem__(self, idx):\n",
      "00:25:04.566370 line        15         if self.mode == \"test\":\n",
      "00:25:04.566408 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '作为当年的央视一姐，倪萍为什么要选择离开央视？'\n",
      "New var:....... text_b = '综艺大观,王文澜,央视,倪萍,董卿,金话筒奖'\n",
      "New var:....... label = 0\n",
      "00:25:04.566900 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.566992 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.567065 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.567255 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['作', '为', '当', '年', '的', '央', '视', '一', '姐', '，...'什', '么', '要', '选', '择', '离', '开', '央', '视', '？']\n",
      "00:25:04.567861 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '作', '为', '当', '年', '的', '央', '视', '一'... '要', '选', '择', '离', '开', '央', '视', '？', '[SEP]']\n",
      "00:25:04.568143 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:04.568352 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['综', '艺', '大', '观', ',', '王', '文', '澜', ',', '央...'倪', '萍', ',', '董', '卿', ',', '金', '话', '筒', '奖']\n",
      "00:25:04.568949 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '作', '为', '当', '年', '的', '央', '视', '一'... ',', '董', '卿', ',', '金', '话', '筒', '奖', '[SEP]']\n",
      "00:25:04.569147 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:25:04.569339 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 868, 711, 2496, 2399, 4638, 1925, 6228, 67...17, 5869, 1321, 117, 7032, 6413, 5030, 1946, 102]\n",
      "00:25:04.569545 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  868,  711, 2496, 2399, 4638, 1925... 5869, 1321,  117, 7032, 6413, 5030, 1946,  102])\n",
      "00:25:04.569755 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.570403 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.570987 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.572051 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  868,  711, 2496, 2399, 4638, 192... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007877\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 157\n",
      "00:25:04.574207 call        14     def __getitem__(self, idx):\n",
      "00:25:04.574273 line        15         if self.mode == \"test\":\n",
      "00:25:04.574310 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '雷军，马化腾，李彦宏等互联网大佬背后都有贤内助，却羡慕刘强东'\n",
      "New var:....... text_b = '雷军,李彦宏,刘强东,京东,马化腾'\n",
      "New var:....... label = 0\n",
      "00:25:04.574806 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.574897 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.574969 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.575146 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['雷', '军', '，', '马', '化', '腾', '，', '李', '彦', '宏...'贤', '内', '助', '，', '却', '羡', '慕', '刘', '强', '东']\n",
      "00:25:04.575932 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '雷', '军', '，', '马', '化', '腾', '，', '李'... '助', '，', '却', '羡', '慕', '刘', '强', '东', '[SEP]']\n",
      "00:25:04.576119 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:04.576301 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['雷', '军', ',', '李', '彦', '宏', ',', '刘', '强', '东', ',', '京', '东', ',', '马', '化', '腾']\n",
      "00:25:04.576800 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '雷', '军', '，', '马', '化', '腾', '，', '李'... '东', ',', '京', '东', ',', '马', '化', '腾', '[SEP]']\n",
      "00:25:04.576994 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:04.577182 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7440, 1092, 8024, 7716, 1265, 5596, 8024, ..., 691, 117, 776, 691, 117, 7716, 1265, 5596, 102]\n",
      "00:25:04.577387 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7440, 1092, 8024, 7716, 1265, 5596... 776,  691,  117, 7716, 1265,        5596,  102])\n",
      "00:25:04.577660 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.578305 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:25:04.578950 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.580148 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7440, 1092, 8024, 7716, 1265, 559...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008182\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 158\n",
      "00:25:04.582419 call        14     def __getitem__(self, idx):\n",
      "00:25:04.582490 line        15         if self.mode == \"test\":\n",
      "00:25:04.582527 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '李沁出席活动，网友：穿单肩裙子就是美！'\n",
      "New var:....... text_b = '李沁,单肩'\n",
      "New var:....... label = 0\n",
      "00:25:04.583050 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.583143 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.583216 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.583398 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['李', '沁', '出', '席', '活', '动', '，', '网', '友', '：', '穿', '单', '肩', '裙', '子', '就', '是', '美', '！']\n",
      "00:25:04.583932 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '沁', '出', '席', '活', '动', '，', '网'... '单', '肩', '裙', '子', '就', '是', '美', '！', '[SEP]']\n",
      "00:25:04.584116 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:25:04.584295 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['李', '沁', ',', '单', '肩']\n",
      "00:25:04.584678 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '沁', '出', '席', '活', '动', '，', '网'..., '！', '[SEP]', '李', '沁', ',', '单', '肩', '[SEP]']\n",
      "00:25:04.584890 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 6\n",
      "00:25:04.585083 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3330, 3751, 1139, 2375, 3833, 1220, 8024, ...401, 8013, 102, 3330, 3751, 117, 1296, 5504, 102]\n",
      "00:25:04.585281 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3330, 3751, 1139, 2375, 3833, 1220... 102, 3330, 3751,  117,        1296, 5504,  102])\n",
      "00:25:04.585486 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.585944 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,        1, 1, 1])\n",
      "00:25:04.586398 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.587313 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3330, 3751, 1139, 2375, 3833, 122...0, 0, 0, 0, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006150\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 159\n",
      "00:25:04.588597 call        14     def __getitem__(self, idx):\n",
      "00:25:04.588665 line        15         if self.mode == \"test\":\n",
      "00:25:04.588701 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '没有IP没有狗血，这是青春原本的样子—《热血狂篮》燃情来袭（1517期）'\n",
      "New var:....... text_b = '街头篮球,粉考拉,旋风少女,热血,橙红年代,篮球,佟梦实,热血狂篮'\n",
      "New var:....... label = 0\n",
      "00:25:04.589914 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.590080 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.590158 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.590334 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['没', '有', 'ip', '没', '有', '狗', '血', '，', '这', '... '燃', '情', '来', '袭', '（', '151', '##7', '期', '）']\n",
      "00:25:04.591704 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '没', '有', 'ip', '没', '有', '狗', '血', '，..., '来', '袭', '（', '151', '##7', '期', '）', '[SEP]']\n",
      "00:25:04.591902 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 35\n",
      "00:25:04.592088 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['街', '头', '篮', '球', ',', '粉', '考', '拉', ',', '旋...'球', ',', '佟', '梦', '实', ',', '热', '血', '狂', '篮']\n",
      "00:25:04.592930 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '没', '有', 'ip', '没', '有', '狗', '血', '，... '佟', '梦', '实', ',', '热', '血', '狂', '篮', '[SEP]']\n",
      "00:25:04.593129 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 34\n",
      "00:25:04.593322 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3766, 3300, 8215, 3766, 3300, 4318, 6117, ...71, 3457, 2141, 117, 4178, 6117, 4312, 5074, 102]\n",
      "00:25:04.593535 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3766, 3300, 8215, 3766, 3300, 4318... 3457, 2141,  117, 4178, 6117, 4312, 5074,  102])\n",
      "00:25:04.593751 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.594578 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.595172 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.596452 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3766, 3300, 8215, 3766, 3300, 431... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010930\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 160\n",
      "00:25:04.599569 call        14     def __getitem__(self, idx):\n",
      "00:25:04.599646 line        15         if self.mode == \"test\":\n",
      "00:25:04.599684 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '杨钰莹也有动心的时候，可惜对象不是毛宁'\n",
      "New var:....... text_b = '毛宁,杨钰莹'\n",
      "New var:....... label = 0\n",
      "00:25:04.600295 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.600464 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.600610 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.600785 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['杨', '钰', '莹', '也', '有', '动', '心', '的', '时', '候', '，', '可', '惜', '对', '象', '不', '是', '毛', '宁']\n",
      "00:25:04.601332 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '钰', '莹', '也', '有', '动', '心', '的'... '可', '惜', '对', '象', '不', '是', '毛', '宁', '[SEP]']\n",
      "00:25:04.601514 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:25:04.601695 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['毛', '宁', ',', '杨', '钰', '莹']\n",
      "00:25:04.602002 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '钰', '莹', '也', '有', '动', '心', '的'..., '[SEP]', '毛', '宁', ',', '杨', '钰', '莹', '[SEP]']\n",
      "00:25:04.602190 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 7\n",
      "00:25:04.602372 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3342, 7177, 5816, 738, 3300, 1220, 2552, 4...123, 102, 3688, 2123, 117, 3342, 7177, 5816, 102]\n",
      "00:25:04.602629 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3342, 7177, 5816,  738, 3300, 1220...3688, 2123,  117,        3342, 7177, 5816,  102])\n",
      "00:25:04.602832 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.603343 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:25:04.603792 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.604607 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3342, 7177, 5816,  738, 3300, 122...0, 0, 0, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006523\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 161\n",
      "00:25:04.606116 call        14     def __getitem__(self, idx):\n",
      "00:25:04.606184 line        15         if self.mode == \"test\":\n",
      "00:25:04.606220 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '跑男里的他是最不可能抽烟的，然而他的所作所为你意想不到！'\n",
      "New var:....... text_b = '邓超,鹿晗,关晓彤,王祖蓝,跑男'\n",
      "New var:....... label = 0\n",
      "00:25:04.606727 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.606818 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.606890 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.607068 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['跑', '男', '里', '的', '他', '是', '最', '不', '可', '能...'所', '作', '所', '为', '你', '意', '想', '不', '到', '！']\n",
      "00:25:04.607753 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '跑', '男', '里', '的', '他', '是', '最', '不'... '所', '为', '你', '意', '想', '不', '到', '！', '[SEP]']\n",
      "00:25:04.608033 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:25:04.608249 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['邓', '超', ',', '鹿', '晗', ',', '关', '晓', '彤', ',', '王', '祖', '蓝', ',', '跑', '男']\n",
      "00:25:04.608749 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '跑', '男', '里', '的', '他', '是', '最', '不'... '彤', ',', '王', '祖', '蓝', ',', '跑', '男', '[SEP]']\n",
      "00:25:04.608949 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 17\n",
      "00:25:04.609141 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6651, 4511, 7027, 4638, 800, 3221, 3297, 6...502, 117, 4374, 4862, 5905, 117, 6651, 4511, 102]\n",
      "00:25:04.609349 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6651, 4511, 7027, 4638,  800, 3221...  117, 4374, 4862, 5905,  117, 6651, 4511,  102])\n",
      "00:25:04.609561 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.610258 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.610843 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.611899 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6651, 4511, 7027, 4638,  800, 322... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007803\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 162\n",
      "00:25:04.613950 call        14     def __getitem__(self, idx):\n",
      "00:25:04.614020 line        15         if self.mode == \"test\":\n",
      "00:25:04.614058 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '女孩被继母打成“熊猫眼”，这样的人根本不配成为父母'\n",
      "New var:....... text_b = '祖国的花朵,熊猫眼,红衣女子,齐齐哈尔,父母爱情'\n",
      "New var:....... label = 0\n",
      "00:25:04.614574 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.614667 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.614741 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.614932 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['女', '孩', '被', '继', '母', '打', '成', '[UNK]', '熊'...'的', '人', '根', '本', '不', '配', '成', '为', '父', '母']\n",
      "00:25:04.615666 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '孩', '被', '继', '母', '打', '成', '[U... '根', '本', '不', '配', '成', '为', '父', '母', '[SEP]']\n",
      "00:25:04.615876 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:25:04.616067 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['祖', '国', '的', '花', '朵', ',', '熊', '猫', '眼', ',...',', '齐', '齐', '哈', '尔', ',', '父', '母', '爱', '情']\n",
      "00:25:04.616632 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '孩', '被', '继', '母', '打', '成', '[U... '齐', '哈', '尔', ',', '父', '母', '爱', '情', '[SEP]']\n",
      "00:25:04.616870 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 25\n",
      "00:25:04.617070 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1957, 2111, 6158, 5326, 3678, 2802, 2768, ...70, 1506, 2209, 117, 4266, 3678, 4263, 2658, 102]\n",
      "00:25:04.617314 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1957, 2111, 6158, 5326, 3678, 2802...2209,  117, 4266,        3678, 4263, 2658,  102])\n",
      "00:25:04.617507 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.618170 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:25:04.618845 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.620108 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1957, 2111, 6158, 5326, 3678, 280...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008326\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 163\n",
      "00:25:04.622307 call        14     def __getitem__(self, idx):\n",
      "00:25:04.622376 line        15         if self.mode == \"test\":\n",
      "00:25:04.622413 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '李易峰杨洋等鲜肉的妈妈长什么样？'\n",
      "New var:....... text_b = '刘诗诗,温哥华,陈伟霆,李易峰,吴亦凡'\n",
      "New var:....... label = 0\n",
      "00:25:04.622974 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.623065 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.623139 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.623318 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['李', '易', '峰', '杨', '洋', '等', '鲜', '肉', '的', '妈', '妈', '长', '什', '么', '样', '？']\n",
      "00:25:04.623802 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '易', '峰', '杨', '洋', '等', '鲜', '肉', '的', '妈', '妈', '长', '什', '么', '样', '？', '[SEP]']\n",
      "00:25:04.623984 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 18\n",
      "00:25:04.624159 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['刘', '诗', '诗', ',', '温', '哥', '华', ',', '陈', '伟', '霆', ',', '李', '易', '峰', ',', '吴', '亦', '凡']\n",
      "00:25:04.624687 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '易', '峰', '杨', '洋', '等', '鲜', '肉'... ',', '李', '易', '峰', ',', '吴', '亦', '凡', '[SEP]']\n",
      "00:25:04.624941 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:04.625131 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3330, 3211, 2292, 3342, 3817, 5023, 7831, ...117, 3330, 3211, 2292, 117, 1426, 771, 1127, 102]\n",
      "00:25:04.625330 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3330, 3211, 2292, 3342, 3817, 5023...3211, 2292,  117, 1426,  771,        1127,  102])\n",
      "00:25:04.625534 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.626106 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.626685 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.627641 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3330, 3211, 2292, 3342, 3817, 502... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007071\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 164\n",
      "00:25:04.629405 call        14     def __getitem__(self, idx):\n",
      "00:25:04.629472 line        15         if self.mode == \"test\":\n",
      "00:25:04.629509 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '女星化妆有多拼？baby全身擦粉，唐嫣腿上被抹三斤油！'\n",
      "New var:....... text_b = '唐嫣,倪妮,造型师,女明星,范冰冰'\n",
      "New var:....... label = 0\n",
      "00:25:04.630003 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.630166 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.630240 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.630415 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['女', '星', '化', '妆', '有', '多', '拼', '？', 'baby',...'唐', '嫣', '腿', '上', '被', '抹', '三', '斤', '油', '！']\n",
      "00:25:04.631059 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '星', '化', '妆', '有', '多', '拼', '？'... '腿', '上', '被', '抹', '三', '斤', '油', '！', '[SEP]']\n",
      "00:25:04.631263 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:04.631447 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['唐', '嫣', ',', '倪', '妮', ',', '造', '型', '师', ',', '女', '明', '星', ',', '范', '冰', '冰']\n",
      "00:25:04.631946 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '星', '化', '妆', '有', '多', '拼', '？'... ',', '女', '明', '星', ',', '范', '冰', '冰', '[SEP]']\n",
      "00:25:04.632139 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:04.632392 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1957, 3215, 1265, 1966, 3300, 1914, 2894, ...17, 1957, 3209, 3215, 117, 5745, 1102, 1102, 102]\n",
      "00:25:04.632597 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1957, 3215, 1265, 1966, 3300, 1914... 1957, 3209, 3215,  117, 5745, 1102, 1102,  102])\n",
      "00:25:04.632804 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.633409 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.634023 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.635154 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1957, 3215, 1265, 1966, 3300, 191... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007726\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 165\n",
      "00:25:04.637160 call        14     def __getitem__(self, idx):\n",
      "00:25:04.637227 line        15         if self.mode == \"test\":\n",
      "00:25:04.637329 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '香港网红模特李宓儿现身《新锁清秋》剧组'\n",
      "New var:....... text_b = '模特,戏服,新锁清秋,香港,李宓儿'\n",
      "New var:....... label = 0\n",
      "00:25:04.637838 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.637929 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.638000 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.638175 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['香', '港', '网', '红', '模', '特', '李', '宓', '儿', '现', '身', '《', '新', '锁', '清', '秋', '》', '剧', '组']\n",
      "00:25:04.638712 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '香', '港', '网', '红', '模', '特', '李', '宓'... '《', '新', '锁', '清', '秋', '》', '剧', '组', '[SEP]']\n",
      "00:25:04.638893 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:25:04.639071 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['模', '特', ',', '戏', '服', ',', '新', '锁', '清', '秋', ',', '香', '港', ',', '李', '宓', '儿']\n",
      "00:25:04.639565 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '香', '港', '网', '红', '模', '特', '李', '宓'... '秋', ',', '香', '港', ',', '李', '宓', '儿', '[SEP]']\n",
      "00:25:04.639820 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:04.640007 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7676, 3949, 5381, 5273, 3563, 4294, 3330, ...904, 117, 7676, 3949, 117, 3330, 2132, 1036, 102]\n",
      "00:25:04.640204 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7676, 3949, 5381, 5273, 3563, 4294...7676, 3949,  117, 3330,        2132, 1036,  102])\n",
      "00:25:04.640407 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.640957 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.641512 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.642524 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7676, 3949, 5381, 5273, 3563, 429... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007094\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 166\n",
      "00:25:04.644284 call        14     def __getitem__(self, idx):\n",
      "00:25:04.644353 line        15         if self.mode == \"test\":\n",
      "00:25:04.644391 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '请友对；鸳鸯对对湖中戏！'\n",
      "New var:....... text_b = '中戏,女儿情'\n",
      "New var:....... label = 0\n",
      "00:25:04.644892 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.645056 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.645131 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.645306 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['请', '友', '对', '；', '鸳', '鸯', '对', '对', '湖', '中', '戏', '！']\n",
      "00:25:04.645719 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '请', '友', '对', '；', '鸳', '鸯', '对', '对', '湖', '中', '戏', '！', '[SEP]']\n",
      "00:25:04.645903 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 14\n",
      "00:25:04.646077 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['中', '戏', ',', '女', '儿', '情']\n",
      "00:25:04.646381 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '请', '友', '对', '；', '鸳', '鸯', '对', '对'..., '[SEP]', '中', '戏', ',', '女', '儿', '情', '[SEP]']\n",
      "00:25:04.646566 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 7\n",
      "00:25:04.646747 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6435, 1351, 2190, 8039, 7892, 7891, 2190, ...8013, 102, 704, 2767, 117, 1957, 1036, 2658, 102]\n",
      "00:25:04.646877 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6435, 1351, 2190, 8039, 7892, 7891...  102,  704, 2767,  117, 1957, 1036, 2658,  102])\n",
      "00:25:04.647089 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.647561 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.648063 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.648882 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6435, 1351, 2190, 8039, 7892, 789... 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006102\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 167\n",
      "00:25:04.650413 call        14     def __getitem__(self, idx):\n",
      "00:25:04.650480 line        15         if self.mode == \"test\":\n",
      "00:25:04.650517 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《温暖的弦》南弦对一心做的这几件暖心事，让温暖感叹回不到从前'\n",
      "New var:....... text_b = '占南弦,一心,温暖的弦,商业间谍,南弦'\n",
      "New var:....... label = 0\n",
      "00:25:04.651032 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.651124 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.651196 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.651375 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '温', '暖', '的', '弦', '》', '南', '弦', '对', '一...'让', '温', '暖', '感', '叹', '回', '不', '到', '从', '前']\n",
      "00:25:04.652102 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '温', '暖', '的', '弦', '》', '南', '弦'... '暖', '感', '叹', '回', '不', '到', '从', '前', '[SEP]']\n",
      "00:25:04.652362 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:04.652548 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['占', '南', '弦', ',', '一', '心', ',', '温', '暖', '的', '弦', ',', '商', '业', '间', '谍', ',', '南', '弦']\n",
      "00:25:04.653085 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '温', '暖', '的', '弦', '》', '南', '弦'... ',', '商', '业', '间', '谍', ',', '南', '弦', '[SEP]']\n",
      "00:25:04.653279 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:04.653468 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 3946, 3265, 4638, 2478, 518, 1298, 24...117, 1555, 689, 7313, 6452, 117, 1298, 2478, 102]\n",
      "00:25:04.653674 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 3946, 3265, 4638, 2478,  518... 689, 7313, 6452,         117, 1298, 2478,  102])\n",
      "00:25:04.653883 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.654560 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:25:04.655169 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.656301 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 3946, 3265, 4638, 2478,  51...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008169\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 168\n",
      "00:25:04.658617 call        14     def __getitem__(self, idx):\n",
      "00:25:04.658689 line        15         if self.mode == \"test\":\n",
      "00:25:04.658726 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '演过无数文艺片的36岁孙艺珍，凭什么就是经常请吃饭的漂亮姐姐？'\n",
      "New var:....... text_b = '海角七号,橡皮擦,郑雨盛,假若爱有天意,现在很想见你,失忆症,孙艺珍,韩国电影'\n",
      "New var:....... label = 0\n",
      "00:25:04.659329 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.659423 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.659503 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.659751 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['演', '过', '无', '数', '文', '艺', '片', '的', '36', '...'常', '请', '吃', '饭', '的', '漂', '亮', '姐', '姐', '？']\n",
      "00:25:04.660498 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '演', '过', '无', '数', '文', '艺', '片', '的'... '吃', '饭', '的', '漂', '亮', '姐', '姐', '？', '[SEP]']\n",
      "00:25:04.660689 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:04.660873 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['海', '角', '七', '号', ',', '橡', '皮', '擦', ',', '郑...'症', ',', '孙', '艺', '珍', ',', '韩', '国', '电', '影']\n",
      "00:25:04.661751 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '演', '过', '无', '数', '文', '艺', '片', '的'... '孙', '艺', '珍', ',', '韩', '国', '电', '影', '[SEP]']\n",
      "00:25:04.662015 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 40\n",
      "00:25:04.662211 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4028, 6814, 3187, 3144, 3152, 5686, 4275, ...01, 5686, 4397, 117, 7506, 1744, 4510, 2512, 102]\n",
      "00:25:04.662429 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4028, 6814, 3187, 3144, 3152, 5686... 5686, 4397,  117, 7506, 1744, 4510, 2512,  102])\n",
      "00:25:04.662647 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.663423 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.664279 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.665595 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4028, 6814, 3187, 3144, 3152, 568... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009434\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 169\n",
      "00:25:04.668078 call        14     def __getitem__(self, idx):\n",
      "00:25:04.668148 line        15         if self.mode == \"test\":\n",
      "00:25:04.668185 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '180509 回顾刘昊然艺考青涩模样 男人味与日俱增'\n",
      "New var:....... text_b = '妹妹头,中戏,刘昊然,底迪'\n",
      "New var:....... label = 0\n",
      "00:25:04.668713 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.668805 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.668878 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.669153 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['180', '##50', '##9', '回', '顾', '刘', '昊', '然', ...'涩', '模', '样', '男', '人', '味', '与', '日', '俱', '增']\n",
      "00:25:04.669775 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '180', '##50', '##9', '回', '顾', '刘', '... '样', '男', '人', '味', '与', '日', '俱', '增', '[SEP]']\n",
      "00:25:04.669970 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:25:04.670152 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['妹', '妹', '头', ',', '中', '戏', ',', '刘', '昊', '然', ',', '底', '迪']\n",
      "00:25:04.670585 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '180', '##50', '##9', '回', '顾', '刘', '... '戏', ',', '刘', '昊', '然', ',', '底', '迪', '[SEP]']\n",
      "00:25:04.670778 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "00:25:04.670964 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 8420, 8474, 8160, 1726, 7560, 1155, 3207, ...767, 117, 1155, 3207, 4197, 117, 2419, 6832, 102]\n",
      "00:25:04.671166 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 8420, 8474, 8160, 1726, 7560, 1155...1155, 3207, 4197,  117, 2419, 6832,         102])\n",
      "00:25:04.671440 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.672737 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.673577 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.674666 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 8420, 8474, 8160, 1726, 7560, 115... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008369\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 170\n",
      "00:25:04.676478 call        14     def __getitem__(self, idx):\n",
      "00:25:04.676547 line        15         if self.mode == \"test\":\n",
      "00:25:04.676585 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '黄磊人设彻底崩塌，原来好老公好爸爸都是装出来的！'\n",
      "New var:....... text_b = '一代男神人设,孙莉,黄磊,老公好爸爸,边走边唱,深夜食堂,崩塌,似水年华'\n",
      "New var:....... label = 0\n",
      "00:25:04.677161 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.677350 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.677433 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.677618 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['黄', '磊', '人', '设', '彻', '底', '崩', '塌', '，', '原...'好', '爸', '爸', '都', '是', '装', '出', '来', '的', '！']\n",
      "00:25:04.678253 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '黄', '磊', '人', '设', '彻', '底', '崩', '塌'... '爸', '都', '是', '装', '出', '来', '的', '！', '[SEP]']\n",
      "00:25:04.678443 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:04.678625 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['一', '代', '男', '神', '人', '设', ',', '孙', '莉', ',...'食', '堂', ',', '崩', '塌', ',', '似', '水', '年', '华']\n",
      "00:25:04.679451 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '黄', '磊', '人', '设', '彻', '底', '崩', '塌'... ',', '崩', '塌', ',', '似', '水', '年', '华', '[SEP]']\n",
      "00:25:04.679713 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 37\n",
      "00:25:04.679908 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7942, 4830, 782, 6392, 2515, 2419, 2309, 1...117, 2309, 1847, 117, 849, 3717, 2399, 1290, 102]\n",
      "00:25:04.680119 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7942, 4830,  782, 6392, 2515, 2419...1847,  117,  849, 3717,        2399, 1290,  102])\n",
      "00:25:04.680333 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.681056 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.681796 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.683065 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7942, 4830,  782, 6392, 2515, 241... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009775\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 171\n",
      "00:25:04.686283 call        14     def __getitem__(self, idx):\n",
      "00:25:04.686348 line        15         if self.mode == \"test\":\n",
      "00:25:04.686385 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '今天无娱乐，只有祝福，看各大明星纷纷为汶川加油！'\n",
      "New var:....... text_b = '黄晓明,刘诗诗,大地震,张歆艺,TFBOYS'\n",
      "New var:....... label = 0\n",
      "00:25:04.687007 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.687100 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.687180 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.687326 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['今', '天', '无', '娱', '乐', '，', '只', '有', '祝', '福...'明', '星', '纷', '纷', '为', '汶', '川', '加', '油', '！']\n",
      "00:25:04.694145 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '今', '天', '无', '娱', '乐', '，', '只', '有'... '纷', '纷', '为', '汶', '川', '加', '油', '！', '[SEP]']\n",
      "00:25:04.694400 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:04.694591 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['黄', '晓', '明', ',', '刘', '诗', '诗', ',', '大', '地...', ',', '张', '歆', '艺', ',', 'tf', '##boy', '##s']\n",
      "00:25:04.695372 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '今', '天', '无', '娱', '乐', '，', '只', '有'...张', '歆', '艺', ',', 'tf', '##boy', '##s', '[SEP]']\n",
      "00:25:04.695611 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:04.696116 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 791, 1921, 3187, 2031, 727, 8024, 1372, 33..., 2476, 3622, 5686, 117, 12719, 11287, 8118, 102]\n",
      "00:25:04.696309 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,   791,  1921,  3187,  2031,   727...        5686,   117, 12719, 11287,  8118,   102])\n",
      "00:25:04.702691 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.703882 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.704324 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.705026 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,   791,  1921,  3187,  2031,   72... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.021195\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 172\n",
      "00:25:04.707513 call        14     def __getitem__(self, idx):\n",
      "00:25:04.707587 line        15         if self.mode == \"test\":\n",
      "00:25:04.707626 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '杨紫与阚清子机场秀，两种完全不同的风格'\n",
      "New var:....... text_b = '阚清子现身机场,杨紫现身机场,阚清子现身,东方IC,杨紫'\n",
      "New var:....... label = 0\n",
      "00:25:04.708213 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.708307 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.708502 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.708837 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['杨', '紫', '与', '阚', '清', '子', '机', '场', '秀', '，', '两', '种', '完', '全', '不', '同', '的', '风', '格']\n",
      "00:25:04.709430 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '紫', '与', '阚', '清', '子', '机', '场'... '种', '完', '全', '不', '同', '的', '风', '格', '[SEP]']\n",
      "00:25:04.709661 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:25:04.709887 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['阚', '清', '子', '现', '身', '机', '场', ',', '杨', '紫...子', '现', '身', ',', '东', '方', 'ic', ',', '杨', '紫']\n",
      "00:25:04.710608 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '紫', '与', '阚', '清', '子', '机', '场'...'身', ',', '东', '方', 'ic', ',', '杨', '紫', '[SEP]']\n",
      "00:25:04.710821 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 28\n",
      "00:25:04.711098 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3342, 5166, 680, 7337, 3926, 2094, 3322, 1...6716, 117, 691, 3175, 8577, 117, 3342, 5166, 102]\n",
      "00:25:04.711350 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3342, 5166,  680, 7337, 3926, 2094... 691, 3175, 8577,  117, 3342, 5166,         102])\n",
      "00:25:04.711608 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.712388 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:25:04.713069 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.714547 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3342, 5166,  680, 7337, 3926, 209...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009581\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 173\n",
      "00:25:04.717130 call        14     def __getitem__(self, idx):\n",
      "00:25:04.717207 line        15         if self.mode == \"test\":\n",
      "00:25:04.717247 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '这是林青霞的第一部古装电影，当时的她才20岁！'\n",
      "New var:....... text_b = '林青霞,古镜幽魂,宋存寿,镜花水月,灯蛾扑火'\n",
      "New var:....... label = 0\n",
      "00:25:04.717842 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.717938 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.718137 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.718467 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['这', '是', '林', '青', '霞', '的', '第', '一', '部', '古...影', '，', '当', '时', '的', '她', '才', '20', '岁', '！']\n",
      "00:25:04.719126 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '是', '林', '青', '霞', '的', '第', '一'...'当', '时', '的', '她', '才', '20', '岁', '！', '[SEP]']\n",
      "00:25:04.719471 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:25:04.719701 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['林', '青', '霞', ',', '古', '镜', '幽', '魂', ',', '宋...',', '镜', '花', '水', '月', ',', '灯', '蛾', '扑', '火']\n",
      "00:25:04.720336 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '是', '林', '青', '霞', '的', '第', '一'... '花', '水', '月', ',', '灯', '蛾', '扑', '火', '[SEP]']\n",
      "00:25:04.720576 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:25:04.720810 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6821, 3221, 3360, 7471, 7459, 4638, 5018, ...09, 3717, 3299, 117, 4128, 6042, 2800, 4125, 102]\n",
      "00:25:04.721060 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6821, 3221, 3360, 7471, 7459, 4638... 3717, 3299,  117, 4128, 6042, 2800, 4125,  102])\n",
      "00:25:04.721317 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.722104 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.722809 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.724268 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6821, 3221, 3360, 7471, 7459, 463... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009641\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 174\n",
      "00:25:04.726805 call        14     def __getitem__(self, idx):\n",
      "00:25:04.726877 line        15         if self.mode == \"test\":\n",
      "00:25:04.726914 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '石原里美新恋情疑曝光 事务所表示艺人私生活不参与'\n",
      "New var:....... text_b = '事务所,粉丝,石原里美,艺人,山下智久,朝九晚五'\n",
      "New var:....... label = 0\n",
      "00:25:04.727497 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.727591 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.727785 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.728006 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['石', '原', '里', '美', '新', '恋', '情', '疑', '曝', '光...'表', '示', '艺', '人', '私', '生', '活', '不', '参', '与']\n",
      "00:25:04.728668 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '石', '原', '里', '美', '新', '恋', '情', '疑'... '艺', '人', '私', '生', '活', '不', '参', '与', '[SEP]']\n",
      "00:25:04.729012 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:04.729241 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['事', '务', '所', ',', '粉', '丝', ',', '石', '原', '里...',', '山', '下', '智', '久', ',', '朝', '九', '晚', '五']\n",
      "00:25:04.729906 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '石', '原', '里', '美', '新', '恋', '情', '疑'... '下', '智', '久', ',', '朝', '九', '晚', '五', '[SEP]']\n",
      "00:25:04.730174 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 25\n",
      "00:25:04.730456 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4767, 1333, 7027, 5401, 3173, 2605, 2658, ..., 678, 3255, 719, 117, 3308, 736, 3241, 758, 102]\n",
      "00:25:04.730748 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4767, 1333, 7027, 5401, 3173, 2605... 719,  117, 3308,  736, 3241,         758,  102])\n",
      "00:25:04.731017 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.731835 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:25:04.732679 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.734018 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4767, 1333, 7027, 5401, 3173, 260...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009951\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 175\n",
      "00:25:04.736793 call        14     def __getitem__(self, idx):\n",
      "00:25:04.736870 line        15         if self.mode == \"test\":\n",
      "00:25:04.736909 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '林志玲深V连体裤，脸白赛过周围人，网友：女神赶紧结婚吧'\n",
      "New var:....... text_b = '连体裤,综艺节目,粉丝,事业线,林志玲'\n",
      "New var:....... label = 0\n",
      "00:25:04.737499 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.737594 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.737801 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.738034 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['林', '志', '玲', '深', 'v', '连', '体', '裤', '，', '脸...'网', '友', '：', '女', '神', '赶', '紧', '结', '婚', '吧']\n",
      "00:25:04.738770 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '志', '玲', '深', 'v', '连', '体', '裤'... '：', '女', '神', '赶', '紧', '结', '婚', '吧', '[SEP]']\n",
      "00:25:04.739118 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:04.739350 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['连', '体', '裤', ',', '综', '艺', '节', '目', ',', '粉', '丝', ',', '事', '业', '线', ',', '林', '志', '玲']\n",
      "00:25:04.739934 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '志', '玲', '深', 'v', '连', '体', '裤'... ',', '事', '业', '线', ',', '林', '志', '玲', '[SEP]']\n",
      "00:25:04.740178 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:04.740416 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3360, 2562, 4386, 3918, 164, 6825, 860, 61... 117, 752, 689, 5296, 117, 3360, 2562, 4386, 102]\n",
      "00:25:04.740680 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3360, 2562, 4386, 3918,  164, 6825... 689, 5296,  117, 3360, 2562, 4386,         102])\n",
      "00:25:04.741051 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.741854 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:25:04.742544 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.743773 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3360, 2562, 4386, 3918,  164, 682...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009923\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 176\n",
      "00:25:04.746771 call        14     def __getitem__(self, idx):\n",
      "00:25:04.746864 line        15         if self.mode == \"test\":\n",
      "00:25:04.746905 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '晚安曲丨我的心，是红桃'\n",
      "New var:....... text_b = '晚安曲,Shape of My Heart,Sting,片尾曲,这个杀手不太冷,玛蒂达'\n",
      "New var:....... label = 0\n",
      "00:25:04.747575 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.747955 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.748301 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.748998 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['晚', '安', '曲', '丨', '我', '的', '心', '，', '是', '红', '桃']\n",
      "00:25:04.749506 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '晚', '安', '曲', '丨', '我', '的', '心', '，', '是', '红', '桃', '[SEP]']\n",
      "00:25:04.750126 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 13\n",
      "00:25:04.750814 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['晚', '安', '曲', ',', 'sh', '##ape', 'of', 'my', ...'个', '杀', '手', '不', '太', '冷', ',', '玛', '蒂', '达']\n",
      "00:25:04.751689 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '晚', '安', '曲', '丨', '我', '的', '心', '，'... '手', '不', '太', '冷', ',', '玛', '蒂', '达', '[SEP]']\n",
      "00:25:04.751944 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 29\n",
      "00:25:04.752298 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3241, 2128, 3289, 701, 2769, 4638, 2552, 8...797, 679, 1922, 1107, 117, 4377, 5881, 6809, 102]\n",
      "00:25:04.752564 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  3241,  2128,  3289,   701,  2769... 1107,   117,  4377,  5881,         6809,   102])\n",
      "00:25:04.752830 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.753480 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.754128 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.755288 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  3241,  2128,  3289,   701,  276... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010034\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 177\n",
      "00:25:04.756833 call        14     def __getitem__(self, idx):\n",
      "00:25:04.756906 line        15         if self.mode == \"test\":\n",
      "00:25:04.756944 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '人民网再次痛批小鲜肉：从艺不久片酬过亿！谈判官建军大业等上榜'\n",
      "New var:....... text_b = '林海雪原,建军大业,伪现实主义,小鲜肉,人民网,谈判官'\n",
      "New var:....... label = 0\n",
      "00:25:04.757525 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.757701 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.757850 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.758027 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['人', '民', '网', '再', '次', '痛', '批', '小', '鲜', '肉...'谈', '判', '官', '建', '军', '大', '业', '等', '上', '榜']\n",
      "00:25:04.758760 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '人', '民', '网', '再', '次', '痛', '批', '小'... '官', '建', '军', '大', '业', '等', '上', '榜', '[SEP]']\n",
      "00:25:04.758948 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:04.759131 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['林', '海', '雪', '原', ',', '建', '军', '大', '业', ',...'鲜', '肉', ',', '人', '民', '网', ',', '谈', '判', '官']\n",
      "00:25:04.759799 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '人', '民', '网', '再', '次', '痛', '批', '小'... ',', '人', '民', '网', ',', '谈', '判', '官', '[SEP]']\n",
      "00:25:04.760061 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 28\n",
      "00:25:04.760255 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 782, 3696, 5381, 1086, 3613, 4578, 2821, 2...117, 782, 3696, 5381, 117, 6448, 1161, 2135, 102]\n",
      "00:25:04.760435 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  782, 3696, 5381, 1086, 3613, 4578...  782, 3696, 5381,  117, 6448, 1161, 2135,  102])\n",
      "00:25:04.760629 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.761297 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.761988 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.763193 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  782, 3696, 5381, 1086, 3613, 457... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008858\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 178\n",
      "00:25:04.765725 call        14     def __getitem__(self, idx):\n",
      "00:25:04.765798 line        15         if self.mode == \"test\":\n",
      "00:25:04.765835 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '明星势力榜第18期，陈立农实力夺冠，实力证明自己潜力'\n",
      "New var:....... text_b = '蔡徐坤,NINE PERCENT,陈立农,女孩,偶像练习生'\n",
      "New var:....... label = 0\n",
      "00:25:04.766442 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.766535 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.766681 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.766856 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['明', '星', '势', '力', '榜', '第', '18', '期', '，', '...'冠', '，', '实', '力', '证', '明', '自', '己', '潜', '力']\n",
      "00:25:04.767521 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '明', '星', '势', '力', '榜', '第', '18', '期... '实', '力', '证', '明', '自', '己', '潜', '力', '[SEP]']\n",
      "00:25:04.767658 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:25:04.768077 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['蔡', '徐', '坤', ',', 'ni', '##ne', 'pe', '##rc',...'农', ',', '女', '孩', ',', '偶', '像', '练', '习', '生']\n",
      "00:25:04.769040 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '明', '星', '势', '力', '榜', '第', '18', '期... '女', '孩', ',', '偶', '像', '练', '习', '生', '[SEP]']\n",
      "00:25:04.769635 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:25:04.770063 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3209, 3215, 1232, 1213, 3528, 5018, 8123, ...1957, 2111, 117, 981, 1008, 5298, 739, 4495, 102]\n",
      "00:25:04.770385 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  3209,  3215,  1232,  1213,  3528...  117,   981,  1008,  5298,   739,  4495,   102])\n",
      "00:25:04.770767 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.771393 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:25:04.772102 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.773386 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  3209,  3215,  1232,  1213,  352...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010077\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 179\n",
      "00:25:04.775844 call        14     def __getitem__(self, idx):\n",
      "00:25:04.775923 line        15         if self.mode == \"test\":\n",
      "00:25:04.775963 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '蒋欣生日 老搭档孙俪 刘涛没表示 网友：谁说祝福非得发博啊'\n",
      "New var:....... text_b = '刘涛,老搭档,孙俪,金牌娱乐,蒋欣'\n",
      "New var:....... label = 0\n",
      "00:25:04.776549 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.776643 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.776796 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.776999 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['蒋', '欣', '生', '日', '老', '搭', '档', '孙', '俪', '刘...'：', '谁', '说', '祝', '福', '非', '得', '发', '博', '啊']\n",
      "00:25:04.777689 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '蒋', '欣', '生', '日', '老', '搭', '档', '孙'... '说', '祝', '福', '非', '得', '发', '博', '啊', '[SEP]']\n",
      "00:25:04.777947 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:04.778132 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['刘', '涛', ',', '老', '搭', '档', ',', '孙', '俪', ',', '金', '牌', '娱', '乐', ',', '蒋', '欣']\n",
      "00:25:04.778628 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '蒋', '欣', '生', '日', '老', '搭', '档', '孙'... ',', '金', '牌', '娱', '乐', ',', '蒋', '欣', '[SEP]']\n",
      "00:25:04.778821 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:04.779011 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5882, 3615, 4495, 3189, 5439, 3022, 3440, ...117, 7032, 4277, 2031, 727, 117, 5882, 3615, 102]\n",
      "00:25:04.779214 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5882, 3615, 4495, 3189, 5439, 3022... 7032, 4277, 2031,  727,  117, 5882, 3615,  102])\n",
      "00:25:04.779421 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.780104 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.780609 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.781668 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5882, 3615, 4495, 3189, 5439, 302... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008728\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 180\n",
      "00:25:04.784615 call        14     def __getitem__(self, idx):\n",
      "00:25:04.784706 line        15         if self.mode == \"test\":\n",
      "00:25:04.784750 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '跟张杰同门师兄弟 30多岁才靠在综艺节目扮丑一举走红'\n",
      "New var:....... text_b = '我型我秀,张杰,刘维,贾玲,薛之谦,感觉自己萌萌哒,火星情报局'\n",
      "New var:....... label = 0\n",
      "00:25:04.785411 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.785654 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.785826 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.786090 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['跟', '张', '杰', '同', '门', '师', '兄', '弟', '30', '...'综', '艺', '节', '目', '扮', '丑', '一', '举', '走', '红']\n",
      "00:25:04.786741 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '跟', '张', '杰', '同', '门', '师', '兄', '弟'... '节', '目', '扮', '丑', '一', '举', '走', '红', '[SEP]']\n",
      "00:25:04.786936 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:04.787120 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['我', '型', '我', '秀', ',', '张', '杰', ',', '刘', '维...'己', '萌', '萌', '哒', ',', '火', '星', '情', '报', '局']\n",
      "00:25:04.787860 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '跟', '张', '杰', '同', '门', '师', '兄', '弟'... '萌', '哒', ',', '火', '星', '情', '报', '局', '[SEP]']\n",
      "00:25:04.788124 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 32\n",
      "00:25:04.788317 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6656, 2476, 3345, 1398, 7305, 2360, 1040, ...46, 1515, 117, 4125, 3215, 2658, 2845, 2229, 102]\n",
      "00:25:04.788528 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6656, 2476, 3345, 1398, 7305, 2360... 1515,  117, 4125, 3215, 2658, 2845, 2229,  102])\n",
      "00:25:04.788744 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.789395 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.790035 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.791151 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6656, 2476, 3345, 1398, 7305, 236...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008774\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 181\n",
      "00:25:04.793418 call        14     def __getitem__(self, idx):\n",
      "00:25:04.793487 line        15         if self.mode == \"test\":\n",
      "00:25:04.793526 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '赵本山女儿赵一涵性感私照曝光，“球姐”果然名不虚传！'\n",
      "New var:....... text_b = '烈焰红唇,赵本山,赵一涵'\n",
      "New var:....... label = 0\n",
      "00:25:04.794048 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.794141 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.794215 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.794394 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['赵', '本', '山', '女', '儿', '赵', '一', '涵', '性', '感... '姐', '[UNK]', '果', '然', '名', '不', '虚', '传', '！']\n",
      "00:25:04.795047 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '赵', '本', '山', '女', '儿', '赵', '一', '涵'...NK]', '果', '然', '名', '不', '虚', '传', '！', '[SEP]']\n",
      "00:25:04.795236 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:04.795450 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['烈', '焰', '红', '唇', ',', '赵', '本', '山', ',', '赵', '一', '涵']\n",
      "00:25:04.795928 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '赵', '本', '山', '女', '儿', '赵', '一', '涵'... ',', '赵', '本', '山', ',', '赵', '一', '涵', '[SEP]']\n",
      "00:25:04.796121 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 13\n",
      "00:25:04.796310 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6627, 3315, 2255, 1957, 1036, 6627, 671, 3...117, 6627, 3315, 2255, 117, 6627, 671, 3891, 102]\n",
      "00:25:04.796510 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6627, 3315, 2255, 1957, 1036, 6627...3315, 2255,         117, 6627,  671, 3891,  102])\n",
      "00:25:04.796718 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.797365 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.798076 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.799132 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6627, 3315, 2255, 1957, 1036, 662... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007580\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 182\n",
      "00:25:04.801035 call        14     def __getitem__(self, idx):\n",
      "00:25:04.801107 line        15         if self.mode == \"test\":\n",
      "00:25:04.801145 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '29岁阚清子懒散挥手兴致不高 遇热情粉丝求签名秒变亲切脸'\n",
      "New var:....... text_b = '阚清子,粉丝,东方IC,东方ic,女人味'\n",
      "New var:....... label = 0\n",
      "00:25:04.801769 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.801864 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.801941 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.802130 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['29', '岁', '阚', '清', '子', '懒', '散', '挥', '手', '...'粉', '丝', '求', '签', '名', '秒', '变', '亲', '切', '脸']\n",
      "00:25:04.802813 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '29', '岁', '阚', '清', '子', '懒', '散', '挥... '求', '签', '名', '秒', '变', '亲', '切', '脸', '[SEP]']\n",
      "00:25:04.803004 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:04.803254 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['阚', '清', '子', ',', '粉', '丝', ',', '东', '方', 'ic', ',', '东', '方', 'ic', ',', '女', '人', '味']\n",
      "00:25:04.803784 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '29', '岁', '阚', '清', '子', '懒', '散', '挥...',', '东', '方', 'ic', ',', '女', '人', '味', '[SEP]']\n",
      "00:25:04.803979 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:25:04.804170 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 8162, 2259, 7337, 3926, 2094, 2750, 3141, ... 117, 691, 3175, 8577, 117, 1957, 782, 1456, 102]\n",
      "00:25:04.804377 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 8162, 2259, 7337, 3926, 2094, 2750...  691, 3175, 8577,  117, 1957,  782, 1456,  102])\n",
      "00:25:04.804590 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.805177 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.805897 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.806919 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 8162, 2259, 7337, 3926, 2094, 275... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007905\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 183\n",
      "00:25:04.808974 call        14     def __getitem__(self, idx):\n",
      "00:25:04.809047 line        15         if self.mode == \"test\":\n",
      "00:25:04.809087 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '豆瓣9.5，你哭着、笑着、吃着路边摊，对我说：生活不用想太多'\n",
      "New var:....... text_b = '秘密森林,街头美食斗士,Signal,军屯锅盔,深夜食堂,鬼怪,美食家'\n",
      "New var:....... label = 0\n",
      "00:25:04.809674 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.809769 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.809850 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.810155 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['豆', '瓣', '9', '.', '5', '，', '你', '哭', '着', '、...'我', '说', '：', '生', '活', '不', '用', '想', '太', '多']\n",
      "00:25:04.810877 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '豆', '瓣', '9', '.', '5', '，', '你', '哭'... '：', '生', '活', '不', '用', '想', '太', '多', '[SEP]']\n",
      "00:25:04.811067 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:04.811252 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['秘', '密', '森', '林', ',', '街', '头', '美', '食', '斗...'夜', '食', '堂', ',', '鬼', '怪', ',', '美', '食', '家']\n",
      "00:25:04.812020 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '豆', '瓣', '9', '.', '5', '，', '你', '哭'... '堂', ',', '鬼', '怪', ',', '美', '食', '家', '[SEP]']\n",
      "00:25:04.812221 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 32\n",
      "00:25:04.812522 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6486, 4480, 130, 119, 126, 8024, 872, 1526...828, 117, 7787, 2597, 117, 5401, 7608, 2157, 102]\n",
      "00:25:04.812781 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  6486,  4480,   130,   119,   126... 2597,   117,         5401,  7608,  2157,   102])\n",
      "00:25:04.813015 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.813759 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.814763 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.815950 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  6486,  4480,   130,   119,   12... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009326\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 184\n",
      "00:25:04.818329 call        14     def __getitem__(self, idx):\n",
      "00:25:04.818397 line        15         if self.mode == \"test\":\n",
      "00:25:04.818435 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《极限挑战》张艺兴依旧缺席，连男人帮都凑不齐，全靠素人硬撑？'\n",
      "New var:....... text_b = '男人帮,张艺兴,极限挑战,综艺,极限智囊团'\n",
      "New var:....... label = 0\n",
      "00:25:04.818988 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.819081 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.819155 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.819378 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '极', '限', '挑', '战', '》', '张', '艺', '兴', '依...'不', '齐', '，', '全', '靠', '素', '人', '硬', '撑', '？']\n",
      "00:25:04.820242 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '极', '限', '挑', '战', '》', '张', '艺'... '，', '全', '靠', '素', '人', '硬', '撑', '？', '[SEP]']\n",
      "00:25:04.820445 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:04.820600 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['男', '人', '帮', ',', '张', '艺', '兴', ',', '极', '限...'战', ',', '综', '艺', ',', '极', '限', '智', '囊', '团']\n",
      "00:25:04.821151 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '极', '限', '挑', '战', '》', '张', '艺'... '综', '艺', ',', '极', '限', '智', '囊', '团', '[SEP]']\n",
      "00:25:04.821350 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 22\n",
      "00:25:04.821607 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 3353, 7361, 2904, 2773, 518, 2476, 56...41, 5686, 117, 3353, 7361, 3255, 1718, 1730, 102]\n",
      "00:25:04.821845 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 3353, 7361, 2904, 2773,  518... 117,        3353, 7361, 3255, 1718, 1730,  102])\n",
      "00:25:04.822066 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.822746 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:25:04.823422 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.824712 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 3353, 7361, 2904, 2773,  51...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008614\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 185\n",
      "00:25:04.826971 call        14     def __getitem__(self, idx):\n",
      "00:25:04.827040 line        15         if self.mode == \"test\":\n",
      "00:25:04.827078 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '北方武师逐个挑战南方武师'\n",
      "New var:....... text_b = '武师'\n",
      "New var:....... label = 0\n",
      "00:25:04.827599 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.827690 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.827760 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.827939 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['北', '方', '武', '师', '逐', '个', '挑', '战', '南', '方', '武', '师']\n",
      "00:25:04.828347 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '北', '方', '武', '师', '逐', '个', '挑', '战', '南', '方', '武', '师', '[SEP]']\n",
      "00:25:04.828527 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 14\n",
      "00:25:04.828702 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['武', '师']\n",
      "00:25:04.828933 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '北', '方', '武', '师', '逐', '个', '挑', '战', '南', '方', '武', '师', '[SEP]', '武', '师', '[SEP]']\n",
      "00:25:04.829207 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 3\n",
      "00:25:04.829416 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1266, 3175, 3636, 2360, 6852, 702, 2904, 2773, 1298, 3175, 3636, 2360, 102, 3636, 2360, 102]\n",
      "00:25:04.829613 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1266, 3175, 3636, 2360, 6852,  702...3175, 3636,        2360,  102, 3636, 2360,  102])\n",
      "00:25:04.829810 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.830271 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "00:25:04.830728 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.831457 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1266, 3175, 3636, 2360, 6852,  70... 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.005671\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 186\n",
      "00:25:04.832669 call        14     def __getitem__(self, idx):\n",
      "00:25:04.832737 line        15         if self.mode == \"test\":\n",
      "00:25:04.832772 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '那些惊艳了时光的琼瑶女郎，谁的颜值更胜一筹？老照片见证玄机'\n",
      "New var:....... text_b = '聚散两依依,白发魔女,武林外史,花姑子,林青霞,蒋勤勤,夏紫薇,绝代双骄,半生缘,陈德容,情深深雨濛濛,梅花烙,琼瑶,还珠格格,梅花三弄'\n",
      "New var:....... label = 0\n",
      "00:25:04.833292 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.833385 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.833458 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.833705 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['那', '些', '惊', '艳', '了', '时', '光', '的', '琼', '瑶...'一', '筹', '？', '老', '照', '片', '见', '证', '玄', '机']\n",
      "00:25:04.834415 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '那', '些', '惊', '艳', '了', '时', '光', '的'... '？', '老', '照', '片', '见', '证', '玄', '机', '[SEP]']\n",
      "00:25:04.834603 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:04.834785 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['聚', '散', '两', '依', '依', ',', '白', '发', '魔', '女...',', '还', '珠', '格', '格', ',', '梅', '花', '三', '弄']\n",
      "00:25:04.836149 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '那', '些', '惊', '艳', '了', '时', '光', '的'... '珠', '格', '格', ',', '梅', '花', '三', '弄', '[SEP]']\n",
      "00:25:04.836417 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 69\n",
      "00:25:04.836620 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6929, 763, 2661, 5683, 749, 3198, 1045, 46...403, 3419, 3419, 117, 3449, 5709, 676, 2462, 102]\n",
      "00:25:04.836850 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6929,  763, 2661, 5683,  749, 3198...3419,  117, 3449,        5709,  676, 2462,  102])\n",
      "00:25:04.837078 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.838097 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:25:04.839116 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.840920 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6929,  763, 2661, 5683,  749, 319...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011682\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 187\n",
      "00:25:04.844378 call        14     def __getitem__(self, idx):\n",
      "00:25:04.844448 line        15         if self.mode == \"test\":\n",
      "00:25:04.844484 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '为何这部剧刚开播，口碑就完胜热播的《温暖的弦》'\n",
      "New var:....... text_b = '温暖的弦,电影,黄景瑜,宋茜,张钧甯'\n",
      "New var:....... label = 0\n",
      "00:25:04.844981 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.845167 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.845244 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.845421 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['为', '何', '这', '部', '剧', '刚', '开', '播', '，', '口...'胜', '热', '播', '的', '《', '温', '暖', '的', '弦', '》']\n",
      "00:25:04.845989 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '为', '何', '这', '部', '剧', '刚', '开', '播'... '播', '的', '《', '温', '暖', '的', '弦', '》', '[SEP]']\n",
      "00:25:04.846155 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:04.846337 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['温', '暖', '的', '弦', ',', '电', '影', ',', '黄', '景', '瑜', ',', '宋', '茜', ',', '张', '钧', '甯']\n",
      "00:25:04.846852 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '为', '何', '这', '部', '剧', '刚', '开', '播'... '瑜', ',', '宋', '茜', ',', '张', '钧', '甯', '[SEP]']\n",
      "00:25:04.847045 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:25:04.847316 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 711, 862, 6821, 6956, 1196, 1157, 2458, 30...447, 117, 2129, 5752, 117, 2476, 7172, 4505, 102]\n",
      "00:25:04.847577 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  711,  862, 6821, 6956, 1196, 1157...  117, 2129, 5752,  117, 2476, 7172, 4505,  102])\n",
      "00:25:04.847727 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.848164 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.848654 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.849876 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  711,  862, 6821, 6956, 1196, 115... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007478\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 188\n",
      "00:25:04.851884 call        14     def __getitem__(self, idx):\n",
      "00:25:04.852019 line        15         if self.mode == \"test\":\n",
      "00:25:04.852059 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张雨绮着绿群现身活动秀完美胸围 网友：谁能承接这一抹绿'\n",
      "New var:....... text_b = '绿群现身活动,现身活动秀完美胸围,胸围,张雨绮'\n",
      "New var:....... label = 0\n",
      "00:25:04.852582 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.852674 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.852746 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.852922 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '雨', '绮', '着', '绿', '群', '现', '身', '活', '动...'友', '：', '谁', '能', '承', '接', '这', '一', '抹', '绿']\n",
      "00:25:04.853579 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '雨', '绮', '着', '绿', '群', '现', '身'... '谁', '能', '承', '接', '这', '一', '抹', '绿', '[SEP]']\n",
      "00:25:04.853765 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:04.853943 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['绿', '群', '现', '身', '活', '动', ',', '现', '身', '活...'美', '胸', '围', ',', '胸', '围', ',', '张', '雨', '绮']\n",
      "00:25:04.854601 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '雨', '绮', '着', '绿', '群', '现', '身'... '围', ',', '胸', '围', ',', '张', '雨', '绮', '[SEP]']\n",
      "00:25:04.854806 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "00:25:04.854997 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 7433, 5331, 4708, 5344, 5408, 4385, ...741, 117, 5541, 1741, 117, 2476, 7433, 5331, 102]\n",
      "00:25:04.855200 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476, 7433, 5331, 4708, 5344, 5408...5541, 1741,  117,        2476, 7433, 5331,  102])\n",
      "00:25:04.855410 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.856265 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:25:04.856866 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.857854 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2476, 7433, 5331, 4708, 5344, 540...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008391\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 189\n",
      "00:25:04.860305 call        14     def __getitem__(self, idx):\n",
      "00:25:04.860372 line        15         if self.mode == \"test\":\n",
      "00:25:04.860409 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '韩雪参加某节目 网友：不敢黑 不敢黑'\n",
      "New var:....... text_b = '韩雪,节目网友,黑韩雪'\n",
      "New var:....... label = 0\n",
      "00:25:04.860931 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.861023 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.861095 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.861275 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['韩', '雪', '参', '加', '某', '节', '目', '网', '友', '：', '不', '敢', '黑', '不', '敢', '黑']\n",
      "00:25:04.861834 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '雪', '参', '加', '某', '节', '目', '网', '友', '：', '不', '敢', '黑', '不', '敢', '黑', '[SEP]']\n",
      "00:25:04.862016 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 18\n",
      "00:25:04.862192 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['韩', '雪', ',', '节', '目', '网', '友', ',', '黑', '韩', '雪']\n",
      "00:25:04.862579 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '雪', '参', '加', '某', '节', '目', '网'... '节', '目', '网', '友', ',', '黑', '韩', '雪', '[SEP]']\n",
      "00:25:04.862765 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 12\n",
      "00:25:04.862947 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7506, 7434, 1346, 1217, 3378, 5688, 4680, ...88, 4680, 5381, 1351, 117, 7946, 7506, 7434, 102]\n",
      "00:25:04.863141 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7506, 7434, 1346, 1217, 3378, 5688...5381,        1351,  117, 7946, 7506, 7434,  102])\n",
      "00:25:04.863340 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.864092 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:25:04.864592 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.865512 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7506, 7434, 1346, 1217, 3378, 568...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006903\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 190\n",
      "00:25:04.867237 call        14     def __getitem__(self, idx):\n",
      "00:25:04.867306 line        15         if self.mode == \"test\":\n",
      "00:25:04.867344 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '60岁李幼斌全家照，爆红后与糟糠之妻离婚，现任小13岁还是三婚'\n",
      "New var:....... text_b = '史兰芽,李幼斌,死证,张瑞琪,红日,闯关东,江山,亮剑,大器晚成'\n",
      "New var:....... label = 0\n",
      "00:25:04.867862 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.867956 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.868030 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.868210 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['60', '岁', '李', '幼', '斌', '全', '家', '照', '，', '...，', '现', '任', '小', '13', '岁', '还', '是', '三', '婚']\n",
      "00:25:04.868934 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '60', '岁', '李', '幼', '斌', '全', '家', '照...'任', '小', '13', '岁', '还', '是', '三', '婚', '[SEP]']\n",
      "00:25:04.869217 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:04.869431 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['史', '兰', '芽', ',', '李', '幼', '斌', ',', '死', '证...'江', '山', ',', '亮', '剑', ',', '大', '器', '晚', '成']\n",
      "00:25:04.870201 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '60', '岁', '李', '幼', '斌', '全', '家', '照... ',', '亮', '剑', ',', '大', '器', '晚', '成', '[SEP]']\n",
      "00:25:04.870403 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 33\n",
      "00:25:04.870597 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 8183, 2259, 3330, 2405, 3154, 1059, 2157, ...117, 778, 1187, 117, 1920, 1690, 3241, 2768, 102]\n",
      "00:25:04.870812 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 8183, 2259, 3330, 2405, 3154, 1059...1187,  117, 1920,        1690, 3241, 2768,  102])\n",
      "00:25:04.871028 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.871808 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.872521 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.873690 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 8183, 2259, 3330, 2405, 3154, 105... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008796\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 191\n",
      "00:25:04.876082 call        14     def __getitem__(self, idx):\n",
      "00:25:04.876190 line        15         if self.mode == \"test\":\n",
      "00:25:04.876250 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '偶遇郑爽逛商场，本人超级温柔还给了签名笑容满分'\n",
      "New var:....... text_b = '女艺人,郑爽'\n",
      "New var:....... label = 0\n",
      "00:25:04.877038 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:04.877306 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:04.877476 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:04.877847 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['偶', '遇', '郑', '爽', '逛', '商', '场', '，', '本', '人...'柔', '还', '给', '了', '签', '名', '笑', '容', '满', '分']\n",
      "00:25:04.878473 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '偶', '遇', '郑', '爽', '逛', '商', '场', '，'... '给', '了', '签', '名', '笑', '容', '满', '分', '[SEP]']\n",
      "00:25:04.878699 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:04.878902 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['女', '艺', '人', ',', '郑', '爽']\n",
      "00:25:04.879224 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '偶', '遇', '郑', '爽', '逛', '商', '场', '，'..., '[SEP]', '女', '艺', '人', ',', '郑', '爽', '[SEP]']\n",
      "00:25:04.879512 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 7\n",
      "00:25:04.879731 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 981, 6878, 6948, 4272, 6859, 1555, 1767, 8...1146, 102, 1957, 5686, 782, 117, 6948, 4272, 102]\n",
      "00:25:04.879943 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  981, 6878, 6948, 4272, 6859, 1555...  102, 1957, 5686,  782,  117, 6948, 4272,  102])\n",
      "00:25:04.880157 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:04.880649 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0, 0, 0,        0, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:04.881251 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:04.882207 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  981, 6878, 6948, 4272, 6859, 155...0, 0,        0, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007736\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 192\n",
      "00:25:14.084932 call        14     def __getitem__(self, idx):\n",
      "00:25:14.085171 line        15         if self.mode == \"test\":\n",
      "00:25:14.085241 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '范景翔首唱《城市慢生活》公益晚会“歌单揭秘”'\n",
      "New var:....... text_b = 'ME,快乐天堂,林逸欣,录音棚,范景翔,制造浪漫,在你身旁,恋爱先生,张韶涵'\n",
      "New var:....... label = 0\n",
      "00:25:14.085920 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.086039 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.086137 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.086625 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['范', '景', '翔', '首', '唱', '《', '城', '市', '慢', '生..., '晚', '会', '[UNK]', '歌', '单', '揭', '秘', '[UNK]']\n",
      "00:25:14.087328 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '范', '景', '翔', '首', '唱', '《', '城', '市'...', '[UNK]', '歌', '单', '揭', '秘', '[UNK]', '[SEP]']\n",
      "00:25:14.087682 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:25:14.087936 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['me', ',', '快', '乐', '天', '堂', ',', '林', '逸', '...'旁', ',', '恋', '爱', '先', '生', ',', '张', '韶', '涵']\n",
      "00:25:14.088921 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '范', '景', '翔', '首', '唱', '《', '城', '市'... '恋', '爱', '先', '生', ',', '张', '韶', '涵', '[SEP]']\n",
      "00:25:14.089177 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 38\n",
      "00:25:14.089436 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5745, 3250, 5425, 7674, 1548, 517, 1814, 2...05, 4263, 1044, 4495, 117, 2476, 7511, 3891, 102]\n",
      "00:25:14.089716 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5745, 3250, 5425, 7674, 1548,  517...1044, 4495,  117, 2476, 7511,        3891,  102])\n",
      "00:25:14.090122 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.091058 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.092060 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.093675 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5745, 3250, 5425, 7674, 1548,  51... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011876\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 193\n",
      "00:25:14.096829 call        14     def __getitem__(self, idx):\n",
      "00:25:14.096908 line        15         if self.mode == \"test\":\n",
      "00:25:14.096962 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '范冰冰惊艳亮相戛纳 黑色小礼裙优雅白裙尽显东方之美'\n",
      "New var:....... text_b = '范冰冰戛纳电影节范冰冰戛纳电影节,范冰冰,电影,Givenchy2018春夏高级定制,戛纳电影节'\n",
      "New var:....... label = 0\n",
      "00:25:14.097662 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.097771 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.097855 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.098147 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['范', '冰', '冰', '惊', '艳', '亮', '相', '戛', '纳', '黑...'优', '雅', '白', '裙', '尽', '显', '东', '方', '之', '美']\n",
      "00:25:14.098878 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '范', '冰', '冰', '惊', '艳', '亮', '相', '戛'... '白', '裙', '尽', '显', '东', '方', '之', '美', '[SEP]']\n",
      "00:25:14.099234 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:14.099475 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['范', '冰', '冰', '戛', '纳', '电', '影', '节', '范', '冰...'高', '级', '定', '制', ',', '戛', '纳', '电', '影', '节']\n",
      "00:25:14.101121 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '范', '冰', '冰', '惊', '艳', '亮', '相', '戛'... '定', '制', ',', '戛', '纳', '电', '影', '节', '[SEP]']\n",
      "00:25:14.101508 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 40\n",
      "00:25:14.101776 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5745, 1102, 1102, 2661, 5683, 778, 4685, 2...37, 1169, 117, 2775, 5287, 4510, 2512, 5688, 102]\n",
      "00:25:14.102082 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  5745,  1102,  1102,  2661,  5683...        2775,  5287,  4510,  2512,  5688,   102])\n",
      "00:25:14.102374 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.103378 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.104537 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.106052 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  5745,  1102,  1102,  2661,  568... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.012496\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 194\n",
      "00:25:14.109360 call        14     def __getitem__(self, idx):\n",
      "00:25:14.109439 line        15         if self.mode == \"test\":\n",
      "00:25:14.109480 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '谭维维在老公陈亦飞生病阶段，仍去看闺蜜张靓颖演唱会'\n",
      "New var:....... text_b = '超级女声,演唱会,老公陈亦飞,周笔畅,张靓颖,谭维维'\n",
      "New var:....... label = 0\n",
      "00:25:14.110074 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.110287 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.110373 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.110596 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['谭', '维', '维', '在', '老', '公', '陈', '亦', '飞', '生...'去', '看', '闺', '蜜', '张', '靓', '颖', '演', '唱', '会']\n",
      "00:25:14.111317 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '谭', '维', '维', '在', '老', '公', '陈', '亦'... '闺', '蜜', '张', '靓', '颖', '演', '唱', '会', '[SEP]']\n",
      "00:25:14.111559 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:25:14.111819 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['超', '级', '女', '声', ',', '演', '唱', '会', ',', '老...'笔', '畅', ',', '张', '靓', '颖', ',', '谭', '维', '维']\n",
      "00:25:14.112591 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '谭', '维', '维', '在', '老', '公', '陈', '亦'... ',', '张', '靓', '颖', ',', '谭', '维', '维', '[SEP]']\n",
      "00:25:14.112963 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 27\n",
      "00:25:14.113221 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6478, 5335, 5335, 1762, 5439, 1062, 7357, ...17, 2476, 7472, 7577, 117, 6478, 5335, 5335, 102]\n",
      "00:25:14.113486 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6478, 5335, 5335, 1762, 5439, 1062...7472,        7577,  117, 6478, 5335, 5335,  102])\n",
      "00:25:14.113752 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.114683 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:25:14.115523 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.117083 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6478, 5335, 5335, 1762, 5439, 106...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010592\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 195\n",
      "00:25:14.119984 call        14     def __getitem__(self, idx):\n",
      "00:25:14.120062 line        15         if self.mode == \"test\":\n",
      "00:25:14.120103 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '凡鱼传媒携手爱奇艺献礼母亲节《我是你妈》将上映'\n",
      "New var:....... text_b = '邹元清,李新月,爱奇艺,摔跤吧!爸爸,我是你妈,神秘巨星,母女,首映礼'\n",
      "New var:....... label = 0\n",
      "00:25:14.120681 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.120913 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.121014 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.121244 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['凡', '鱼', '传', '媒', '携', '手', '爱', '奇', '艺', '献...'节', '《', '我', '是', '你', '妈', '》', '将', '上', '映']\n",
      "00:25:14.121947 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '凡', '鱼', '传', '媒', '携', '手', '爱', '奇'... '我', '是', '你', '妈', '》', '将', '上', '映', '[SEP]']\n",
      "00:25:14.122194 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:14.122435 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['邹', '元', '清', ',', '李', '新', '月', ',', '爱', '奇...'秘', '巨', '星', ',', '母', '女', ',', '首', '映', '礼']\n",
      "00:25:14.123342 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '凡', '鱼', '传', '媒', '携', '手', '爱', '奇'... '星', ',', '母', '女', ',', '首', '映', '礼', '[SEP]']\n",
      "00:25:14.123704 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 36\n",
      "00:25:14.123972 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1127, 7824, 837, 2054, 3025, 2797, 4263, 1...215, 117, 3678, 1957, 117, 7674, 3216, 4851, 102]\n",
      "00:25:14.124273 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1127, 7824,  837, 2054, 3025, 2797...3678, 1957,  117, 7674, 3216, 4851,         102])\n",
      "00:25:14.124550 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.125538 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.126559 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.128370 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1127, 7824,  837, 2054, 3025, 279... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.012066\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 196\n",
      "00:25:14.132089 call        14     def __getitem__(self, idx):\n",
      "00:25:14.132178 line        15         if self.mode == \"test\":\n",
      "00:25:14.132224 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '当年叱咤亚洲的中日韩三男星，如今只有刘德华没有变成油腻男！'\n",
      "New var:....... text_b = '无限之住人,柳承龙,七年之夜,刘德华,张东健,五亿探长雷洛传,木村拓哉'\n",
      "New var:....... label = 0\n",
      "00:25:14.132869 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.133113 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.133220 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.133468 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['当', '年', '叱', '咤', '亚', '洲', '的', '中', '日', '韩...'德', '华', '没', '有', '变', '成', '油', '腻', '男', '！']\n",
      "00:25:14.134295 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '当', '年', '叱', '咤', '亚', '洲', '的', '中'... '没', '有', '变', '成', '油', '腻', '男', '！', '[SEP]']\n",
      "00:25:14.134561 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:14.134832 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['无', '限', '之', '住', '人', ',', '柳', '承', '龙', ',...'探', '长', '雷', '洛', '传', ',', '木', '村', '拓', '哉']\n",
      "00:25:14.135757 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '当', '年', '叱', '咤', '亚', '洲', '的', '中'... '雷', '洛', '传', ',', '木', '村', '拓', '哉', '[SEP]']\n",
      "00:25:14.136103 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 36\n",
      "00:25:14.136377 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2496, 2399, 1379, 1483, 762, 3828, 4638, 7...440, 3821, 837, 117, 3312, 3333, 2868, 1507, 102]\n",
      "00:25:14.136656 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2496, 2399, 1379, 1483,  762, 3828...        837,  117, 3312, 3333, 2868, 1507,  102])\n",
      "00:25:14.136955 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.137874 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.138947 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.140396 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2496, 2399, 1379, 1483,  762, 382... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011753\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 197\n",
      "00:25:14.143872 call        14     def __getitem__(self, idx):\n",
      "00:25:14.143949 line        15         if self.mode == \"test\":\n",
      "00:25:14.143989 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '42岁李小冉素颜庆生献吻老公，蛋糕暴露冻龄秘密'\n",
      "New var:....... text_b = '素颜,冻龄,徐佳宁,撒狗粮,李小冉'\n",
      "New var:....... label = 0\n",
      "00:25:14.144528 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.144744 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.144826 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.145061 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['42', '岁', '李', '小', '冉', '素', '颜', '庆', '生', '...'公', '，', '蛋', '糕', '暴', '露', '冻', '龄', '秘', '密']\n",
      "00:25:14.145733 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '42', '岁', '李', '小', '冉', '素', '颜', '庆... '蛋', '糕', '暴', '露', '冻', '龄', '秘', '密', '[SEP]']\n",
      "00:25:14.145970 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:25:14.146198 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['素', '颜', ',', '冻', '龄', ',', '徐', '佳', '宁', ',', '撒', '狗', '粮', ',', '李', '小', '冉']\n",
      "00:25:14.146804 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '42', '岁', '李', '小', '冉', '素', '颜', '庆... ',', '撒', '狗', '粮', ',', '李', '小', '冉', '[SEP]']\n",
      "00:25:14.147122 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:14.147346 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 8239, 2259, 3330, 2207, 1083, 5162, 7582, ...17, 3054, 4318, 5117, 117, 3330, 2207, 1083, 102]\n",
      "00:25:14.147597 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 8239, 2259, 3330, 2207, 1083, 5162...4318,        5117,  117, 3330, 2207, 1083,  102])\n",
      "00:25:14.147911 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.148683 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.149470 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.150665 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 8239, 2259, 3330, 2207, 1083, 516... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009126\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 198\n",
      "00:25:14.153028 call        14     def __getitem__(self, idx):\n",
      "00:25:14.153103 line        15         if self.mode == \"test\":\n",
      "00:25:14.153142 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = 'Red Velvet出席韩国2018梦想演唱会！网友：那位该减肥了！'\n",
      "New var:....... text_b = 'Irene,姜涩琪,Velvet,女子组合,金艺琳'\n",
      "New var:....... label = 0\n",
      "00:25:14.153690 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.153788 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.153866 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.154208 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['red', 've', '##lv', '##et', '出', '席', '韩', '国'...'网', '友', '：', '那', '位', '该', '减', '肥', '了', '！']\n",
      "00:25:14.155035 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'red', 've', '##lv', '##et', '出', '席',... '：', '那', '位', '该', '减', '肥', '了', '！', '[SEP]']\n",
      "00:25:14.155297 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:25:14.155541 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['ir', '##ene', ',', '姜', '涩', '琪', ',', 've', '...et', ',', '女', '子', '组', '合', ',', '金', '艺', '琳']\n",
      "00:25:14.156184 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'red', 've', '##lv', '##et', '出', '席',... '女', '子', '组', '合', ',', '金', '艺', '琳', '[SEP]']\n",
      "00:25:14.156531 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:14.156774 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 9276, 12810, 9084, 8418, 1139, 2375, 7506,...57, 2094, 5299, 1394, 117, 7032, 5686, 4432, 102]\n",
      "00:25:14.157057 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  9276, 12810,  9084,  8418,  1139... 5299,  1394,   117,  7032,  5686,  4432,   102])\n",
      "00:25:14.157347 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.158128 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.159115 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.160431 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  9276, 12810,  9084,  8418,  113... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010479\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 199\n",
      "00:25:14.163537 call        14     def __getitem__(self, idx):\n",
      "00:25:14.163613 line        15         if self.mode == \"test\":\n",
      "00:25:14.163654 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '拿下七校证书，中戏北电上戏争抢，关晓彤同学里还有这样的学霸！'\n",
      "New var:....... text_b = '可塑性,北京电影学院,朴赞郁,王馨悦,合格证'\n",
      "New var:....... label = 0\n",
      "00:25:14.164185 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.164387 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.164492 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.164731 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['拿', '下', '七', '校', '证', '书', '，', '中', '戏', '北...'学', '里', '还', '有', '这', '样', '的', '学', '霸', '！']\n",
      "00:25:14.165557 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '拿', '下', '七', '校', '证', '书', '，', '中'... '还', '有', '这', '样', '的', '学', '霸', '！', '[SEP]']\n",
      "00:25:14.165807 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:14.166050 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['可', '塑', '性', ',', '北', '京', '电', '影', '学', '院...'赞', '郁', ',', '王', '馨', '悦', ',', '合', '格', '证']\n",
      "00:25:14.166723 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '拿', '下', '七', '校', '证', '书', '，', '中'... ',', '王', '馨', '悦', ',', '合', '格', '证', '[SEP]']\n",
      "00:25:14.167083 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:25:14.167344 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2897, 678, 673, 3413, 6395, 741, 8024, 704...17, 4374, 7678, 2643, 117, 1394, 3419, 6395, 102]\n",
      "00:25:14.167616 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2897,  678,  673, 3413, 6395,  741...       7678, 2643,  117, 1394, 3419, 6395,  102])\n",
      "00:25:14.167891 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.168689 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.169770 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.171185 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2897,  678,  673, 3413, 6395,  74...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010503\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 200\n",
      "00:25:14.174072 call        14     def __getitem__(self, idx):\n",
      "00:25:14.174151 line        15         if self.mode == \"test\":\n",
      "00:25:14.174192 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '翻拍《泡沫之夏》女主角大S变小杨幂'\n",
      "New var:....... text_b = '爱奇艺,泡沫之夏,张雪迎,何润东,秦俊杰,神雕侠侣,明晓溪'\n",
      "New var:....... label = 0\n",
      "00:25:14.174699 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.174799 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.174993 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.175219 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['翻', '拍', '《', '泡', '沫', '之', '夏', '》', '女', '主', '角', '大', 's', '变', '小', '杨', '幂']\n",
      "00:25:14.175802 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '翻', '拍', '《', '泡', '沫', '之', '夏', '》'... '主', '角', '大', 's', '变', '小', '杨', '幂', '[SEP]']\n",
      "00:25:14.176047 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 19\n",
      "00:25:14.176306 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['爱', '奇', '艺', ',', '泡', '沫', '之', '夏', ',', '张...'杰', ',', '神', '雕', '侠', '侣', ',', '明', '晓', '溪']\n",
      "00:25:14.177111 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '翻', '拍', '《', '泡', '沫', '之', '夏', '》'... '神', '雕', '侠', '侣', ',', '明', '晓', '溪', '[SEP]']\n",
      "00:25:14.177466 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 30\n",
      "00:25:14.177753 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5436, 2864, 517, 3796, 3773, 722, 1909, 51...4868, 7425, 899, 901, 117, 3209, 3236, 3983, 102]\n",
      "00:25:14.178093 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5436, 2864,  517, 3796, 3773,  722... 899,  901,  117, 3209, 3236, 3983,         102])\n",
      "00:25:14.178395 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.179248 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:25:14.180107 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.181536 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5436, 2864,  517, 3796, 3773,  72...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009951\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 201\n",
      "00:25:14.184054 call        14     def __getitem__(self, idx):\n",
      "00:25:14.184128 line        15         if self.mode == \"test\":\n",
      "00:25:14.184168 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '白夜追凶第二季还拍吗？潘粤明还有什么作品要播出？'\n",
      "New var:....... text_b = '潘粤明,白夜追凶'\n",
      "New var:....... label = 0\n",
      "00:25:14.184703 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.184801 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.184878 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.185210 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['白', '夜', '追', '凶', '第', '二', '季', '还', '拍', '吗...'还', '有', '什', '么', '作', '品', '要', '播', '出', '？']\n",
      "00:25:14.185917 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '白', '夜', '追', '凶', '第', '二', '季', '还'... '什', '么', '作', '品', '要', '播', '出', '？', '[SEP]']\n",
      "00:25:14.186171 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:14.186412 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['潘', '粤', '明', ',', '白', '夜', '追', '凶']\n",
      "00:25:14.186831 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '白', '夜', '追', '凶', '第', '二', '季', '还'... '潘', '粤', '明', ',', '白', '夜', '追', '凶', '[SEP]']\n",
      "00:25:14.187080 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 9\n",
      "00:25:14.187443 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4635, 1915, 6841, 1136, 5018, 753, 2108, 6...50, 5113, 3209, 117, 4635, 1915, 6841, 1136, 102]\n",
      "00:25:14.187708 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4635, 1915, 6841, 1136, 5018,  753... 5113, 3209,  117, 4635, 1915, 6841, 1136,  102])\n",
      "00:25:14.187986 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.188623 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0,        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.189396 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.190534 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4635, 1915, 6841, 1136, 5018,  75...    0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008690\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 202\n",
      "00:25:14.192779 call        14     def __getitem__(self, idx):\n",
      "00:25:14.192855 line        15         if self.mode == \"test\":\n",
      "00:25:14.192896 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '杨子为追黄圣依，砸钱拍摄此剧，邀请众多明星做配角！'\n",
      "New var:....... text_b = '杨子,王思懿,黄圣依,妈妈是超人,杨恭如,二公主,天仙配'\n",
      "New var:....... label = 0\n",
      "00:25:14.193418 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.193517 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.193596 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.193833 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['杨', '子', '为', '追', '黄', '圣', '依', '，', '砸', '钱...'邀', '请', '众', '多', '明', '星', '做', '配', '角', '！']\n",
      "00:25:14.194551 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '子', '为', '追', '黄', '圣', '依', '，'... '众', '多', '明', '星', '做', '配', '角', '！', '[SEP]']\n",
      "00:25:14.194787 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:25:14.195111 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['杨', '子', ',', '王', '思', '懿', ',', '黄', '圣', '依...'恭', '如', ',', '二', '公', '主', ',', '天', '仙', '配']\n",
      "00:25:14.195905 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '子', '为', '追', '黄', '圣', '依', '，'... ',', '二', '公', '主', ',', '天', '仙', '配', '[SEP]']\n",
      "00:25:14.196179 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 29\n",
      "00:25:14.196430 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3342, 2094, 711, 6841, 7942, 1760, 898, 80..., 117, 753, 1062, 712, 117, 1921, 803, 6981, 102]\n",
      "00:25:14.196716 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3342, 2094,  711, 6841, 7942, 1760...  753, 1062,  712,  117, 1921,  803, 6981,  102])\n",
      "00:25:14.197030 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.198230 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.199060 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.200623 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3342, 2094,  711, 6841, 7942, 176...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010841\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 203\n",
      "00:25:14.203656 call        14     def __getitem__(self, idx):\n",
      "00:25:14.203734 line        15         if self.mode == \"test\":\n",
      "00:25:14.203775 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《雷神》洛基的设定是诡计之神还是火神？'\n",
      "New var:....... text_b = '奥丁,变相怪杰2,芬里尔,雷神,巴尔德尔,雷神3,洛基,槲寄生'\n",
      "New var:....... label = 0\n",
      "00:25:14.204367 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.204467 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.204546 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.204795 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '雷', '神', '》', '洛', '基', '的', '设', '定', '是', '诡', '计', '之', '神', '还', '是', '火', '神', '？']\n",
      "00:25:14.205428 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '雷', '神', '》', '洛', '基', '的', '设'... '计', '之', '神', '还', '是', '火', '神', '？', '[SEP]']\n",
      "00:25:14.205764 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:25:14.206004 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['奥', '丁', ',', '变', '相', '怪', '杰', '2', ',', '芬...'雷', '神', '3', ',', '洛', '基', ',', '槲', '寄', '生']\n",
      "00:25:14.206915 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '雷', '神', '》', '洛', '基', '的', '设'... '3', ',', '洛', '基', ',', '槲', '寄', '生', '[SEP]']\n",
      "00:25:14.207191 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 32\n",
      "00:25:14.207477 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 7440, 4868, 518, 3821, 1825, 4638, 63...124, 117, 3821, 1825, 117, 3550, 2164, 4495, 102]\n",
      "00:25:14.207767 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 7440, 4868,  518, 3821, 1825...3821, 1825,         117, 3550, 2164, 4495,  102])\n",
      "00:25:14.208161 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.208952 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1])\n",
      "00:25:14.209871 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.211494 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 7440, 4868,  518, 3821, 182...1, 1, 1, 1, 1,        1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011649\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 204\n",
      "00:25:14.215359 call        14     def __getitem__(self, idx):\n",
      "00:25:14.215474 line        15         if self.mode == \"test\":\n",
      "00:25:14.215528 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '胡瓜女儿好惨！节目上自曝：刚生完女儿2个月婆婆就提荒唐要求'\n",
      "New var:....... text_b = '小祯,台湾,通告艺人,胡瓜,李进良'\n",
      "New var:....... label = 0\n",
      "00:25:14.216208 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.216365 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.216495 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.216860 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['胡', '瓜', '女', '儿', '好', '惨', '！', '节', '目', '上...'个', '月', '婆', '婆', '就', '提', '荒', '唐', '要', '求']\n",
      "00:25:14.217983 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '胡', '瓜', '女', '儿', '好', '惨', '！', '节'... '婆', '婆', '就', '提', '荒', '唐', '要', '求', '[SEP]']\n",
      "00:25:14.218477 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:14.218835 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['小', '祯', ',', '台', '湾', ',', '通', '告', '艺', '人', ',', '胡', '瓜', ',', '李', '进', '良']\n",
      "00:25:14.219547 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '胡', '瓜', '女', '儿', '好', '惨', '！', '节'... '人', ',', '胡', '瓜', ',', '李', '进', '良', '[SEP]']\n",
      "00:25:14.219868 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:14.220182 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5529, 4478, 1957, 1036, 1962, 2673, 8013, ...782, 117, 5529, 4478, 117, 3330, 6822, 5679, 102]\n",
      "00:25:14.220501 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5529, 4478, 1957, 1036, 1962, 2673...5529, 4478,  117, 3330, 6822, 5679,         102])\n",
      "00:25:14.220963 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.221924 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:25:14.222700 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.224047 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5529, 4478, 1957, 1036, 1962, 267...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011888\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 205\n",
      "00:25:14.227299 call        14     def __getitem__(self, idx):\n",
      "00:25:14.227413 line        15         if self.mode == \"test\":\n",
      "00:25:14.227460 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '意不意外？又有逃犯听张学友演唱会落网 厉害了我的哥'\n",
      "New var:....... text_b = '在逃犯,演唱会,张学友,南昌,逃犯'\n",
      "New var:....... label = 0\n",
      "00:25:14.228102 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.228259 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.228353 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.228636 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['意', '不', '意', '外', '？', '又', '有', '逃', '犯', '听...'唱', '会', '落', '网', '厉', '害', '了', '我', '的', '哥']\n",
      "00:25:14.229380 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '意', '不', '意', '外', '？', '又', '有', '逃'... '落', '网', '厉', '害', '了', '我', '的', '哥', '[SEP]']\n",
      "00:25:14.229763 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:14.230021 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['在', '逃', '犯', ',', '演', '唱', '会', ',', '张', '学', '友', ',', '南', '昌', ',', '逃', '犯']\n",
      "00:25:14.230621 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '意', '不', '意', '外', '？', '又', '有', '逃'... '学', '友', ',', '南', '昌', ',', '逃', '犯', '[SEP]']\n",
      "00:25:14.230886 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:14.231160 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2692, 679, 2692, 1912, 8043, 1348, 3300, 6...110, 1351, 117, 1298, 3208, 117, 6845, 4306, 102]\n",
      "00:25:14.231453 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2692,  679, 2692, 1912, 8043, 1348... 1351,  117, 1298, 3208,  117, 6845, 4306,  102])\n",
      "00:25:14.231728 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.232533 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.233247 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.234365 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2692,  679, 2692, 1912, 8043, 134... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009376\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 206\n",
      "00:25:14.236704 call        14     def __getitem__(self, idx):\n",
      "00:25:14.236780 line        15         if self.mode == \"test\":\n",
      "00:25:14.236821 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '如果王宝强前妻马蓉想进军演艺圈，会得到大家的祝福吗？'\n",
      "New var:....... text_b = '演艺圈,王宝强,少林寺,前妻,马蓉'\n",
      "New var:....... label = 0\n",
      "00:25:14.237418 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.237515 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.237595 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.237925 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['如', '果', '王', '宝', '强', '前', '妻', '马', '蓉', '想...'会', '得', '到', '大', '家', '的', '祝', '福', '吗', '？']\n",
      "00:25:14.238696 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '如', '果', '王', '宝', '强', '前', '妻', '马'... '到', '大', '家', '的', '祝', '福', '吗', '？', '[SEP]']\n",
      "00:25:14.238960 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:14.239205 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['演', '艺', '圈', ',', '王', '宝', '强', ',', '少', '林', '寺', ',', '前', '妻', ',', '马', '蓉']\n",
      "00:25:14.239784 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '如', '果', '王', '宝', '强', '前', '妻', '马'... '林', '寺', ',', '前', '妻', ',', '马', '蓉', '[SEP]']\n",
      "00:25:14.240129 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:14.240405 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1963, 3362, 4374, 2140, 2487, 1184, 1988, ...360, 2191, 117, 1184, 1988, 117, 7716, 5900, 102]\n",
      "00:25:14.240678 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1963, 3362, 4374, 2140, 2487, 1184... 2191,  117, 1184, 1988,  117, 7716, 5900,  102])\n",
      "00:25:14.240946 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.241887 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.242700 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.244032 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1963, 3362, 4374, 2140, 2487, 118... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009792\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 207\n",
      "00:25:14.246543 call        14     def __getitem__(self, idx):\n",
      "00:25:14.246627 line        15         if self.mode == \"test\":\n",
      "00:25:14.246668 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '郭德纲正在教训岳云鹏，小岳岳却走神搭讪姑娘！台下笑炸了'\n",
      "New var:....... text_b = '郭德纲,岳岳,姑娘,岳云鹏'\n",
      "New var:....... label = 0\n",
      "00:25:14.247198 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.247295 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.247483 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.247710 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['郭', '德', '纲', '正', '在', '教', '训', '岳', '云', '鹏...'搭', '讪', '姑', '娘', '！', '台', '下', '笑', '炸', '了']\n",
      "00:25:14.248509 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '郭', '德', '纲', '正', '在', '教', '训', '岳'... '姑', '娘', '！', '台', '下', '笑', '炸', '了', '[SEP]']\n",
      "00:25:14.248641 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:14.248761 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['郭', '德', '纲', ',', '岳', '岳', ',', '姑', '娘', ',', '岳', '云', '鹏']\n",
      "00:25:14.249145 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '郭', '德', '纲', '正', '在', '教', '训', '岳'... '岳', ',', '姑', '娘', ',', '岳', '云', '鹏', '[SEP]']\n",
      "00:25:14.249277 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "00:25:14.249405 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6958, 2548, 5286, 3633, 1762, 3136, 6378, ...2277, 117, 1996, 2023, 117, 2277, 756, 7905, 102]\n",
      "00:25:14.249778 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6958, 2548, 5286, 3633, 1762, 3136...       1996, 2023,  117, 2277,  756, 7905,  102])\n",
      "00:25:14.249950 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.250522 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.251353 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.252683 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6958, 2548, 5286, 3633, 1762, 313... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008331\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 208\n",
      "00:25:14.254905 call        14     def __getitem__(self, idx):\n",
      "00:25:14.254981 line        15         if self.mode == \"test\":\n",
      "00:25:14.255020 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《创造101》，最应该pick的是这几款发型吧'\n",
      "New var:....... text_b = '强东玥,创造101,发型,胡彦斌,撑腰,偶像实习生,卷发棒,BOB头,天使之路'\n",
      "New var:....... label = 0\n",
      "00:25:14.255607 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.255705 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.255785 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.255981 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '创', '造', '101', '》', '，', '最', '应', '该', ..., '##ck', '的', '是', '这', '几', '款', '发', '型', '吧']\n",
      "00:25:14.256573 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '创', '造', '101', '》', '，', '最', '... '的', '是', '这', '几', '款', '发', '型', '吧', '[SEP]']\n",
      "00:25:14.256809 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:25:14.257133 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['强', '东', '玥', ',', '创', '造', '101', ',', '发', ...', '棒', ',', 'bob', '头', ',', '天', '使', '之', '路']\n",
      "00:25:14.258010 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '创', '造', '101', '》', '，', '最', '...,', 'bob', '头', ',', '天', '使', '之', '路', '[SEP]']\n",
      "00:25:14.258219 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 36\n",
      "00:25:14.258422 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 1158, 6863, 8359, 518, 8024, 3297, 24...117, 10542, 1928, 117, 1921, 886, 722, 6662, 102]\n",
      "00:25:14.258644 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,   517,  1158,  6863,  8359,   518... 1928,   117,  1921,   886,   722,  6662,   102])\n",
      "00:25:14.258867 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.259614 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.260265 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.261386 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,   517,  1158,  6863,  8359,   51...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008872\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 209\n",
      "00:25:14.263809 call        14     def __getitem__(self, idx):\n",
      "00:25:14.263884 line        15         if self.mode == \"test\":\n",
      "00:25:14.263926 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '小学生自编舞蹈《樱花草》燃爆全场，现在的孩子不得了'\n",
      "New var:....... text_b = '小学生,舞蹈,樱花草'\n",
      "New var:....... label = 0\n",
      "00:25:14.264470 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.264567 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.264644 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.264904 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['小', '学', '生', '自', '编', '舞', '蹈', '《', '樱', '花...'场', '，', '现', '在', '的', '孩', '子', '不', '得', '了']\n",
      "00:25:14.265582 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '小', '学', '生', '自', '编', '舞', '蹈', '《'... '现', '在', '的', '孩', '子', '不', '得', '了', '[SEP]']\n",
      "00:25:14.265816 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:25:14.266008 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['小', '学', '生', ',', '舞', '蹈', ',', '樱', '花', '草']\n",
      "00:25:14.266409 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '小', '学', '生', '自', '编', '舞', '蹈', '《'... '生', ',', '舞', '蹈', ',', '樱', '花', '草', '[SEP]']\n",
      "00:25:14.266614 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 11\n",
      "00:25:14.266811 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2207, 2110, 4495, 5632, 5356, 5659, 6688, ...495, 117, 5659, 6688, 117, 3569, 5709, 5770, 102]\n",
      "00:25:14.267118 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2207, 2110, 4495, 5632, 5356, 5659...5659, 6688,  117, 3569, 5709,        5770,  102])\n",
      "00:25:14.267371 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.267932 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.268522 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.269505 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2207, 2110, 4495, 5632, 5356, 565... 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008483\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 210\n",
      "00:25:14.272341 call        14     def __getitem__(self, idx):\n",
      "00:25:14.272431 line        15         if self.mode == \"test\":\n",
      "00:25:14.272473 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '明星拍吻戏各出奇招，李沁用手挡，热巴用银行卡，但我只服杨幂'\n",
      "New var:....... text_b = '千金归来,拍吻戏,李易峰,吻戏,杨幂,创业时代,李沁'\n",
      "New var:....... label = 0\n",
      "00:25:14.273089 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.273190 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.273369 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.273594 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['明', '星', '拍', '吻', '戏', '各', '出', '奇', '招', '，...'银', '行', '卡', '，', '但', '我', '只', '服', '杨', '幂']\n",
      "00:25:14.274424 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '明', '星', '拍', '吻', '戏', '各', '出', '奇'... '卡', '，', '但', '我', '只', '服', '杨', '幂', '[SEP]']\n",
      "00:25:14.274797 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:14.274999 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['千', '金', '归', '来', ',', '拍', '吻', '戏', ',', '李...'杨', '幂', ',', '创', '业', '时', '代', ',', '李', '沁']\n",
      "00:25:14.275687 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '明', '星', '拍', '吻', '戏', '各', '出', '奇'... ',', '创', '业', '时', '代', ',', '李', '沁', '[SEP]']\n",
      "00:25:14.275896 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 27\n",
      "00:25:14.276099 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3209, 3215, 2864, 1431, 2767, 1392, 1139, ... 117, 1158, 689, 3198, 807, 117, 3330, 3751, 102]\n",
      "00:25:14.276320 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3209, 3215, 2864, 1431, 2767, 1392... 1158,  689, 3198,  807,  117, 3330, 3751,  102])\n",
      "00:25:14.276549 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.277348 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.278080 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.279285 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3209, 3215, 2864, 1431, 2767, 139...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009078\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 211\n",
      "00:25:14.281449 call        14     def __getitem__(self, idx):\n",
      "00:25:14.281552 line        15         if self.mode == \"test\":\n",
      "00:25:14.281592 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '她坚持领养危险犬类本应受到指责，结果却感动了美国上万人'\n",
      "New var:....... text_b = '伊拉克战争,战犬瑞克斯,瑞克斯,忠犬八公,退役军犬,安乐死'\n",
      "New var:....... label = 0\n",
      "00:25:14.282212 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.282316 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.282397 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.282735 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['她', '坚', '持', '领', '养', '危', '险', '犬', '类', '本...'果', '却', '感', '动', '了', '美', '国', '上', '万', '人']\n",
      "00:25:14.283461 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '她', '坚', '持', '领', '养', '危', '险', '犬'... '感', '动', '了', '美', '国', '上', '万', '人', '[SEP]']\n",
      "00:25:14.283662 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:14.283855 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['伊', '拉', '克', '战', '争', ',', '战', '犬', '瑞', '克...'公', ',', '退', '役', '军', '犬', ',', '安', '乐', '死']\n",
      "00:25:14.284595 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '她', '坚', '持', '领', '养', '危', '险', '犬'... '退', '役', '军', '犬', ',', '安', '乐', '死', '[SEP]']\n",
      "00:25:14.284986 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 30\n",
      "00:25:14.285233 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1961, 1780, 2898, 7566, 1075, 1314, 7372, ...842, 2514, 1092, 4305, 117, 2128, 727, 3647, 102]\n",
      "00:25:14.285474 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1961, 1780, 2898, 7566, 1075, 1314... 2514, 1092, 4305,  117, 2128,  727, 3647,  102])\n",
      "00:25:14.285707 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.286400 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.287639 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.288780 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1961, 1780, 2898, 7566, 1075, 131...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010418\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 212\n",
      "00:25:14.291908 call        14     def __getitem__(self, idx):\n",
      "00:25:14.291995 line        15         if self.mode == \"test\":\n",
      "00:25:14.292036 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '少林斗喇嘛，宋玉亭大力金刚指对战智光易筋经金钟罩，真武侠经典'\n",
      "New var:....... text_b = '香港,宋玉亭,金刚指,易筋经,少林斗喇嘛'\n",
      "New var:....... label = 0\n",
      "00:25:14.292650 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.292903 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.293115 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.293494 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['少', '林', '斗', '喇', '嘛', '，', '宋', '玉', '亭', '大...'经', '金', '钟', '罩', '，', '真', '武', '侠', '经', '典']\n",
      "00:25:14.294330 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '少', '林', '斗', '喇', '嘛', '，', '宋', '玉'... '钟', '罩', '，', '真', '武', '侠', '经', '典', '[SEP]']\n",
      "00:25:14.294583 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:14.294746 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['香', '港', ',', '宋', '玉', '亭', ',', '金', '刚', '指', ',', '易', '筋', '经', ',', '少', '林', '斗', '喇', '嘛']\n",
      "00:25:14.295474 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '少', '林', '斗', '喇', '嘛', '，', '宋', '玉'... '筋', '经', ',', '少', '林', '斗', '喇', '嘛', '[SEP]']\n",
      "00:25:14.295730 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:25:14.295978 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2208, 3360, 3159, 1589, 1658, 8024, 2129, ...25, 5307, 117, 2208, 3360, 3159, 1589, 1658, 102]\n",
      "00:25:14.296243 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2208, 3360, 3159, 1589, 1658, 8024... 117, 2208,        3360, 3159, 1589, 1658,  102])\n",
      "00:25:14.296629 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.297763 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1])\n",
      "00:25:14.298549 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.299889 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2208, 3360, 3159, 1589, 1658, 802...1, 1, 1, 1, 1,        1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010435\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 213\n",
      "00:25:14.302377 call        14     def __getitem__(self, idx):\n",
      "00:25:14.302455 line        15         if self.mode == \"test\":\n",
      "00:25:14.302495 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '明星的皮肤为什么那么好？只用护肤品就可以吗？'\n",
      "New var:....... text_b = '卸妆水,希思黎,唐艺昕,面膜,参考价'\n",
      "New var:....... label = 0\n",
      "00:25:14.303111 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.303214 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.303445 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.303797 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['明', '星', '的', '皮', '肤', '为', '什', '么', '那', '么...'只', '用', '护', '肤', '品', '就', '可', '以', '吗', '？']\n",
      "00:25:14.304476 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '明', '星', '的', '皮', '肤', '为', '什', '么'... '护', '肤', '品', '就', '可', '以', '吗', '？', '[SEP]']\n",
      "00:25:14.304951 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:25:14.305196 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['卸', '妆', '水', ',', '希', '思', '黎', ',', '唐', '艺', '昕', ',', '面', '膜', ',', '参', '考', '价']\n",
      "00:25:14.305795 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '明', '星', '的', '皮', '肤', '为', '什', '么'... '昕', ',', '面', '膜', ',', '参', '考', '价', '[SEP]']\n",
      "00:25:14.306047 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:25:14.306294 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3209, 3215, 4638, 4649, 5502, 711, 784, 72...3213, 117, 7481, 5606, 117, 1346, 5440, 817, 102]\n",
      "00:25:14.306554 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3209, 3215, 4638, 4649, 5502,  711...       7481, 5606,  117, 1346, 5440,  817,  102])\n",
      "00:25:14.306823 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.307645 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.308319 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.309456 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3209, 3215, 4638, 4649, 5502,  71... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009303\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 214\n",
      "00:25:14.311714 call        14     def __getitem__(self, idx):\n",
      "00:25:14.311788 line        15         if self.mode == \"test\":\n",
      "00:25:14.311827 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '你一定不知道的漫威最强的二人组——罗素兄弟'\n",
      "New var:....... text_b = '复仇者联盟3 、 4,废材联盟,星战,美国队长2、 3,漫威,复联,兄弟俩,罗素兄弟,复仇者联盟3,美队3,史密斯夫妇,罗素,欢迎来到科林伍德,美队2,发展受阻'\n",
      "New var:....... label = 0\n",
      "00:25:14.312450 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.312542 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.312621 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.312899 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['你', '一', '定', '不', '知', '道', '的', '漫', '威', '最..., '人', '组', '[UNK]', '[UNK]', '罗', '素', '兄', '弟']\n",
      "00:25:14.313496 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '你', '一', '定', '不', '知', '道', '的', '漫'...', '[UNK]', '[UNK]', '罗', '素', '兄', '弟', '[SEP]']\n",
      "00:25:14.313725 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:25:14.313968 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['复', '仇', '者', '联', '盟', '3', '、', '4', ',', '废...'德', ',', '美', '队', '2', ',', '发', '展', '受', '阻']\n",
      "00:25:14.315616 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '你', '一', '定', '不', '知', '道', '的', '漫'... '美', '队', '2', ',', '发', '展', '受', '阻', '[SEP]']\n",
      "00:25:14.315894 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 78\n",
      "00:25:14.316158 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 872, 671, 2137, 679, 4761, 6887, 4638, 403...401, 7339, 123, 117, 1355, 2245, 1358, 7349, 102]\n",
      "00:25:14.316453 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  872,  671, 2137,  679, 4761, 6887... 123,  117,        1355, 2245, 1358, 7349,  102])\n",
      "00:25:14.316744 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.317867 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1])\n",
      "00:25:14.319041 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.320863 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  872,  671, 2137,  679, 4761, 688...1, 1, 1, 1, 1,        1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.012760\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 215\n",
      "00:25:14.324508 call        14     def __getitem__(self, idx):\n",
      "00:25:14.324584 line        15         if self.mode == \"test\":\n",
      "00:25:14.324623 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张翰偷偷拍摄婚纱照？对方比郑爽有实力，比娜扎漂亮！网友：隐婚'\n",
      "New var:....... text_b = '一起来看流星雨,张翰,渣男,花儿与少年,花花公子,郑爽'\n",
      "New var:....... label = 0\n",
      "00:25:14.325218 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.325439 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.325520 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.325755 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '翰', '偷', '偷', '拍', '摄', '婚', '纱', '照', '？...'娜', '扎', '漂', '亮', '！', '网', '友', '：', '隐', '婚']\n",
      "00:25:14.326574 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '翰', '偷', '偷', '拍', '摄', '婚', '纱'... '漂', '亮', '！', '网', '友', '：', '隐', '婚', '[SEP]']\n",
      "00:25:14.326819 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:14.327058 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['一', '起', '来', '看', '流', '星', '雨', ',', '张', '翰...'少', '年', ',', '花', '花', '公', '子', ',', '郑', '爽']\n",
      "00:25:14.327813 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '翰', '偷', '偷', '拍', '摄', '婚', '纱'... ',', '花', '花', '公', '子', ',', '郑', '爽', '[SEP]']\n",
      "00:25:14.328183 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 28\n",
      "00:25:14.328475 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 5432, 982, 982, 2864, 3029, 2042, 52...17, 5709, 5709, 1062, 2094, 117, 6948, 4272, 102]\n",
      "00:25:14.328798 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476, 5432,  982,  982, 2864, 3029... 5709, 5709, 1062, 2094,  117, 6948, 4272,  102])\n",
      "00:25:14.329086 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.329882 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.330791 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.332098 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2476, 5432,  982,  982, 2864, 302... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010494\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 216\n",
      "00:25:14.335034 call        14     def __getitem__(self, idx):\n",
      "00:25:14.335108 line        15         if self.mode == \"test\":\n",
      "00:25:14.335148 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '霍思燕穿着衣服和嗯哼一起洗澡，却被嗯哼三个问题问到脸红！'\n",
      "New var:....... text_b = '霍思燕,穿着衣服洗,嗯哼,脱衣服,妈妈是超人'\n",
      "New var:....... label = 0\n",
      "00:25:14.335704 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.335928 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.336008 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.336245 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['霍', '思', '燕', '穿', '着', '衣', '服', '和', '嗯', '哼...'哼', '三', '个', '问', '题', '问', '到', '脸', '红', '！']\n",
      "00:25:14.336938 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '霍', '思', '燕', '穿', '着', '衣', '服', '和'... '个', '问', '题', '问', '到', '脸', '红', '！', '[SEP]']\n",
      "00:25:14.337188 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:25:14.337471 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['霍', '思', '燕', ',', '穿', '着', '衣', '服', '洗', ',...',', '脱', '衣', '服', ',', '妈', '妈', '是', '超', '人']\n",
      "00:25:14.338147 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '霍', '思', '燕', '穿', '着', '衣', '服', '和'... '衣', '服', ',', '妈', '妈', '是', '超', '人', '[SEP]']\n",
      "00:25:14.338525 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:25:14.338776 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7452, 2590, 4242, 4959, 4708, 6132, 3302, ...132, 3302, 117, 1968, 1968, 3221, 6631, 782, 102]\n",
      "00:25:14.339040 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7452, 2590, 4242, 4959, 4708, 6132... 117, 1968,        1968, 3221, 6631,  782,  102])\n",
      "00:25:14.339311 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.340153 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1])\n",
      "00:25:14.341085 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.342394 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7452, 2590, 4242, 4959, 4708, 613...1, 1, 1, 1, 1,        1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009943\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 217\n",
      "00:25:14.345010 call        14     def __getitem__(self, idx):\n",
      "00:25:14.345085 line        15         if self.mode == \"test\":\n",
      "00:25:14.345125 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '大S终止三胎，他们却得了一个儿子'\n",
      "New var:....... text_b = '大s,李晟,大S,李佳航'\n",
      "New var:....... label = 0\n",
      "00:25:14.345698 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.345796 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.345874 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.346230 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['大', 's', '终', '止', '三', '胎', '，', '他', '们', '却', '得', '了', '一', '个', '儿', '子']\n",
      "00:25:14.346793 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '大', 's', '终', '止', '三', '胎', '，', '他', '们', '却', '得', '了', '一', '个', '儿', '子', '[SEP]']\n",
      "00:25:14.347120 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 18\n",
      "00:25:14.347618 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['大', 's', ',', '李', '晟', ',', '大', 's', ',', '李', '佳', '航']\n",
      "00:25:14.348187 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '大', 's', '终', '止', '三', '胎', '，', '他'... '晟', ',', '大', 's', ',', '李', '佳', '航', '[SEP]']\n",
      "00:25:14.348568 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 13\n",
      "00:25:14.348818 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1920, 161, 5303, 3632, 676, 5522, 8024, 80... 3244, 117, 1920, 161, 117, 3330, 881, 5661, 102]\n",
      "00:25:14.349077 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1920,  161, 5303, 3632,  676, 5522...       1920,  161,  117, 3330,  881, 5661,  102])\n",
      "00:25:14.349345 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.349949 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.350554 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.351749 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1920,  161, 5303, 3632,  676, 552...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008579\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 218\n",
      "00:25:14.353622 call        14     def __getitem__(self, idx):\n",
      "00:25:14.353696 line        15         if self.mode == \"test\":\n",
      "00:25:14.353736 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '同样是生日，杨紫送给张一山和蒋欣的生日却截然不同'\n",
      "New var:....... text_b = '杨紫,张一山,欢乐颂,范冰冰,蒋欣'\n",
      "New var:....... label = 0\n",
      "00:25:14.354448 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.354547 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.354629 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.354892 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['同', '样', '是', '生', '日', '，', '杨', '紫', '送', '给...'蒋', '欣', '的', '生', '日', '却', '截', '然', '不', '同']\n",
      "00:25:14.355518 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '同', '样', '是', '生', '日', '，', '杨', '紫'... '的', '生', '日', '却', '截', '然', '不', '同', '[SEP]']\n",
      "00:25:14.355744 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:14.356103 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['杨', '紫', ',', '张', '一', '山', ',', '欢', '乐', '颂', ',', '范', '冰', '冰', ',', '蒋', '欣']\n",
      "00:25:14.356682 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '同', '样', '是', '生', '日', '，', '杨', '紫'... '颂', ',', '范', '冰', '冰', ',', '蒋', '欣', '[SEP]']\n",
      "00:25:14.356935 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:14.357183 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1398, 3416, 3221, 4495, 3189, 8024, 3342, ...563, 117, 5745, 1102, 1102, 117, 5882, 3615, 102]\n",
      "00:25:14.357445 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1398, 3416, 3221, 4495, 3189, 8024...  117, 5745, 1102, 1102,  117, 5882, 3615,  102])\n",
      "00:25:14.357744 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.358583 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.359300 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.360410 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1398, 3416, 3221, 4495, 3189, 802... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009079\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 219\n",
      "00:25:14.362738 call        14     def __getitem__(self, idx):\n",
      "00:25:14.362817 line        15         if self.mode == \"test\":\n",
      "00:25:14.362858 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '明星叫外卖网购用的名字是什么？这三个人的你一定猜不到'\n",
      "New var:....... text_b = '艺人,杨幂,张艺兴,张富贵,迪丽热巴'\n",
      "New var:....... label = 0\n",
      "00:25:14.363488 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.363588 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.363796 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.364147 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['明', '星', '叫', '外', '卖', '网', '购', '用', '的', '名...'三', '个', '人', '的', '你', '一', '定', '猜', '不', '到']\n",
      "00:25:14.364906 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '明', '星', '叫', '外', '卖', '网', '购', '用'... '人', '的', '你', '一', '定', '猜', '不', '到', '[SEP]']\n",
      "00:25:14.365154 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:14.365395 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['艺', '人', ',', '杨', '幂', ',', '张', '艺', '兴', ',', '张', '富', '贵', ',', '迪', '丽', '热', '巴']\n",
      "00:25:14.365994 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '明', '星', '叫', '外', '卖', '网', '购', '用'... '张', '富', '贵', ',', '迪', '丽', '热', '巴', '[SEP]']\n",
      "00:25:14.366363 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:25:14.366613 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3209, 3215, 1373, 1912, 1297, 5381, 6579, ...476, 2168, 6586, 117, 6832, 714, 4178, 2349, 102]\n",
      "00:25:14.366881 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3209, 3215, 1373, 1912, 1297, 5381... 2168, 6586,  117, 6832,  714, 4178, 2349,  102])\n",
      "00:25:14.367273 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.367983 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.369004 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.370206 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3209, 3215, 1373, 1912, 1297, 538... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009819\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 220\n",
      "00:25:14.372595 call        14     def __getitem__(self, idx):\n",
      "00:25:14.372674 line        15         if self.mode == \"test\":\n",
      "00:25:14.372715 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '51岁TVB不老女神，忍受丈夫赌博喝酒花心多年，最终离婚活出自我'\n",
      "New var:....... text_b = '江欣燕,tvb,逆缘,花心,TVB'\n",
      "New var:....... label = 0\n",
      "00:25:14.373329 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.373428 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.373641 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.373992 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['51', '岁', 'tvb', '不', '老', '女', '神', '，', '忍',...'年', '，', '最', '终', '离', '婚', '活', '出', '自', '我']\n",
      "00:25:14.374827 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '51', '岁', 'tvb', '不', '老', '女', '神', ... '最', '终', '离', '婚', '活', '出', '自', '我', '[SEP]']\n",
      "00:25:14.375075 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:14.375316 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['江', '欣', '燕', ',', 'tvb', ',', '逆', '缘', ',', '花', '心', ',', 'tvb']\n",
      "00:25:14.375838 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '51', '岁', 'tvb', '不', '老', '女', '神', ...,', '逆', '缘', ',', '花', '心', ',', 'tvb', '[SEP]']\n",
      "00:25:14.376244 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "00:25:14.376519 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 8246, 2259, 9312, 679, 5439, 1957, 4868, 8...117, 6847, 5357, 117, 5709, 2552, 117, 9312, 102]\n",
      "00:25:14.376792 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 8246, 2259, 9312,  679, 5439, 1957... 6847, 5357,  117, 5709, 2552,  117, 9312,  102])\n",
      "00:25:14.377065 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.377781 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.378670 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.379797 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 8246, 2259, 9312,  679, 5439, 195... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009478\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 221\n",
      "00:25:14.382102 call        14     def __getitem__(self, idx):\n",
      "00:25:14.382173 line        15         if self.mode == \"test\":\n",
      "00:25:14.382211 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '甜歌天后杨钰莹早年重庆奉节县之行，一次感动小城百姓的特殊演出'\n",
      "New var:....... text_b = '茶山情歌,爱你就是真心喜欢你,奉节县,奉节,演唱会,荧屏寄情,杨钰莹,风含情水含笑,总公司'\n",
      "New var:....... label = 0\n",
      "00:25:14.382746 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.382844 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.382920 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.383274 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['甜', '歌', '天', '后', '杨', '钰', '莹', '早', '年', '重...'动', '小', '城', '百', '姓', '的', '特', '殊', '演', '出']\n",
      "00:25:14.384094 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '甜', '歌', '天', '后', '杨', '钰', '莹', '早'... '城', '百', '姓', '的', '特', '殊', '演', '出', '[SEP]']\n",
      "00:25:14.384350 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:14.384648 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['茶', '山', '情', '歌', ',', '爱', '你', '就', '是', '真...'风', '含', '情', '水', '含', '笑', ',', '总', '公', '司']\n",
      "00:25:14.385782 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '甜', '歌', '天', '后', '杨', '钰', '莹', '早'... '情', '水', '含', '笑', ',', '总', '公', '司', '[SEP]']\n",
      "00:25:14.386394 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 46\n",
      "00:25:14.386676 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4494, 3625, 1921, 1400, 3342, 7177, 5816, ...58, 3717, 1419, 5010, 117, 2600, 1062, 1385, 102]\n",
      "00:25:14.387012 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4494, 3625, 1921, 1400, 3342, 7177...1419,        5010,  117, 2600, 1062, 1385,  102])\n",
      "00:25:14.387464 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.388981 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:25:14.390221 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.392306 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4494, 3625, 1921, 1400, 3342, 717...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013931\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 222\n",
      "00:25:14.396078 call        14     def __getitem__(self, idx):\n",
      "00:25:14.396166 line        15         if self.mode == \"test\":\n",
      "00:25:14.396209 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '奔跑吧：节目组只为收视率？邓超李晨首次痛哭，鹿晗宣布暂时告别'\n",
      "New var:....... text_b = '跑男团,邓超,陈赫,鹿晗,李晨'\n",
      "New var:....... label = 0\n",
      "00:25:14.396858 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.396978 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.397209 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.397452 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['奔', '跑', '吧', '：', '节', '目', '组', '只', '为', '收...'哭', '，', '鹿', '晗', '宣', '布', '暂', '时', '告', '别']\n",
      "00:25:14.398285 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '奔', '跑', '吧', '：', '节', '目', '组', '只'... '鹿', '晗', '宣', '布', '暂', '时', '告', '别', '[SEP]']\n",
      "00:25:14.398782 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:14.399161 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['跑', '男', '团', ',', '邓', '超', ',', '陈', '赫', ',', '鹿', '晗', ',', '李', '晨']\n",
      "00:25:14.399796 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '奔', '跑', '吧', '：', '节', '目', '组', '只'... '陈', '赫', ',', '鹿', '晗', ',', '李', '晨', '[SEP]']\n",
      "00:25:14.400069 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 16\n",
      "00:25:14.400325 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1944, 6651, 1416, 8038, 5688, 4680, 5299, ...357, 6622, 117, 7922, 3240, 117, 3330, 3247, 102]\n",
      "00:25:14.400598 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1944, 6651, 1416, 8038, 5688, 4680... 6622,  117, 7922, 3240,  117, 3330, 3247,  102])\n",
      "00:25:14.401111 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.401831 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.402547 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.403803 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1944, 6651, 1416, 8038, 5688, 468... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010183\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 223\n",
      "00:25:14.406293 call        14     def __getitem__(self, idx):\n",
      "00:25:14.406369 line        15         if self.mode == \"test\":\n",
      "00:25:14.406409 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《归去来》新主题曲MV处处透着忧伤，唐嫣罗晋眼泪虐不停'\n",
      "New var:....... text_b = '哭戏,归去来,唐嫣,主题曲,罗晋'\n",
      "New var:....... label = 0\n",
      "00:25:14.407025 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.407124 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.407202 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.407441 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '归', '去', '来', '》', '新', '主', '题', '曲', 'm...'，', '唐', '嫣', '罗', '晋', '眼', '泪', '虐', '不', '停']\n",
      "00:25:14.408196 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '归', '去', '来', '》', '新', '主', '题'... '嫣', '罗', '晋', '眼', '泪', '虐', '不', '停', '[SEP]']\n",
      "00:25:14.408556 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:14.408796 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['哭', '戏', ',', '归', '去', '来', ',', '唐', '嫣', ',', '主', '题', '曲', ',', '罗', '晋']\n",
      "00:25:14.409350 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '归', '去', '来', '》', '新', '主', '题'... '嫣', ',', '主', '题', '曲', ',', '罗', '晋', '[SEP]']\n",
      "00:25:14.409601 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 17\n",
      "00:25:14.409846 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 2495, 1343, 3341, 518, 3173, 712, 757...2073, 117, 712, 7579, 3289, 117, 5384, 3232, 102]\n",
      "00:25:14.410106 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 2495, 1343, 3341,  518, 3173...  117,  712, 7579, 3289,  117, 5384, 3232,  102])\n",
      "00:25:14.410292 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.411080 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.411771 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.412888 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 2495, 1343, 3341,  518, 317... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008854\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 224\n",
      "00:25:14.415177 call        14     def __getitem__(self, idx):\n",
      "00:25:14.415248 line        15         if self.mode == \"test\":\n",
      "00:25:14.415286 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '粉丝晒与高圆圆合影小腹微隆，高圆圆出席活动说明了一切'\n",
      "New var:....... text_b = '撒狗粮,高圆圆,赵又廷,粉丝,女神'\n",
      "New var:....... label = 0\n",
      "00:25:14.415806 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.415901 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.415976 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.416326 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['粉', '丝', '晒', '与', '高', '圆', '圆', '合', '影', '小...'圆', '出', '席', '活', '动', '说', '明', '了', '一', '切']\n",
      "00:25:14.417091 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '粉', '丝', '晒', '与', '高', '圆', '圆', '合'... '席', '活', '动', '说', '明', '了', '一', '切', '[SEP]']\n",
      "00:25:14.417343 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:14.417586 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['撒', '狗', '粮', ',', '高', '圆', '圆', ',', '赵', '又', '廷', ',', '粉', '丝', ',', '女', '神']\n",
      "00:25:14.418164 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '粉', '丝', '晒', '与', '高', '圆', '圆', '合'... '又', '廷', ',', '粉', '丝', ',', '女', '神', '[SEP]']\n",
      "00:25:14.418562 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:14.418820 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5106, 692, 3235, 680, 7770, 1749, 1749, 13...1348, 2455, 117, 5106, 692, 117, 1957, 4868, 102]\n",
      "00:25:14.419087 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5106,  692, 3235,  680, 7770, 1749... 2455,  117, 5106,  692,  117, 1957, 4868,  102])\n",
      "00:25:14.419357 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.420056 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.421355 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.422426 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5106,  692, 3235,  680, 7770, 174... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008725\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 225\n",
      "00:25:14.423936 call        14     def __getitem__(self, idx):\n",
      "00:25:14.424010 line        15         if self.mode == \"test\":\n",
      "00:25:14.424050 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '他放弃千万年薪当演员，拍戏10年不瘟不火'\n",
      "New var:....... text_b = '上海教育电视台,谭凯,李逍遥,仙剑奇侠传,胡歌'\n",
      "New var:....... label = 0\n",
      "00:25:14.424682 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.424781 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.424867 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.425069 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['他', '放', '弃', '千', '万', '年', '薪', '当', '演', '员', '，', '拍', '戏', '10', '年', '不', '瘟', '不', '火']\n",
      "00:25:14.425659 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '他', '放', '弃', '千', '万', '年', '薪', '当'...'拍', '戏', '10', '年', '不', '瘟', '不', '火', '[SEP]']\n",
      "00:25:14.425856 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:25:14.426116 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['上', '海', '教', '育', '电', '视', '台', ',', '谭', '凯...'遥', ',', '仙', '剑', '奇', '侠', '传', ',', '胡', '歌']\n",
      "00:25:14.426759 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '他', '放', '弃', '千', '万', '年', '薪', '当'... '仙', '剑', '奇', '侠', '传', ',', '胡', '歌', '[SEP]']\n",
      "00:25:14.427003 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "00:25:14.427217 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 800, 3123, 2461, 1283, 674, 2399, 5959, 24... 803, 1187, 1936, 899, 837, 117, 5529, 3625, 102]\n",
      "00:25:14.427441 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  800, 3123, 2461, 1283,  674, 2399... 1187, 1936,  899,  837,  117, 5529, 3625,  102])\n",
      "00:25:14.427669 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.428339 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.428934 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.429902 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  800, 3123, 2461, 1283,  674, 239... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007948\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 226\n",
      "00:25:14.431918 call        14     def __getitem__(self, idx):\n",
      "00:25:14.431995 line        15         if self.mode == \"test\":\n",
      "00:25:14.432035 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '聂耳的小故事'\n",
      "New var:....... text_b = '中华人民共和国国歌,音乐家,聂耳,卖报歌,义勇军进行曲,毕业歌,天伦之爱,塞外村女,采茶歌,一个女明星,昭君和番,聂守信,圆舞曲,金蛇狂舞'\n",
      "New var:....... label = 0\n",
      "00:25:14.432610 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.432707 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.432785 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.432995 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['聂', '耳', '的', '小', '故', '事']\n",
      "00:25:14.433426 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '聂', '耳', '的', '小', '故', '事', '[SEP]']\n",
      "00:25:14.433640 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 8\n",
      "00:25:14.433834 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['中', '华', '人', '民', '共', '和', '国', '国', '歌', ',...'信', ',', '圆', '舞', '曲', ',', '金', '蛇', '狂', '舞']\n",
      "00:25:14.435269 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '聂', '耳', '的', '小', '故', '事', '[SEP]',... '圆', '舞', '曲', ',', '金', '蛇', '狂', '舞', '[SEP]']\n",
      "00:25:14.435525 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 70\n",
      "00:25:14.435735 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5462, 5455, 4638, 2207, 3125, 752, 102, 70...49, 5659, 3289, 117, 7032, 6026, 4312, 5659, 102]\n",
      "00:25:14.435966 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5462, 5455, 4638, 2207, 3125,  752...3289,         117, 7032, 6026, 4312, 5659,  102])\n",
      "00:25:14.436199 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.437010 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1...1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:25:14.437911 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.439368 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5462, 5455, 4638, 2207, 3125,  75...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010242\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 227\n",
      "00:25:14.442190 call        14     def __getitem__(self, idx):\n",
      "00:25:14.442264 line        15         if self.mode == \"test\":\n",
      "00:25:14.442304 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '超能造梦第二季什么时候出？超能造梦第二季在哪里可以看'\n",
      "New var:....... text_b = '高林豹,夏至未至,鲜肉老师,花千骨,丁一,郑合惠子,刘芮麟,无心法师,超能造梦'\n",
      "New var:....... label = 0\n",
      "00:25:14.442863 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.443037 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.443117 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.443304 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['超', '能', '造', '梦', '第', '二', '季', '什', '么', '时...'梦', '第', '二', '季', '在', '哪', '里', '可', '以', '看']\n",
      "00:25:14.443996 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '超', '能', '造', '梦', '第', '二', '季', '什'... '二', '季', '在', '哪', '里', '可', '以', '看', '[SEP]']\n",
      "00:25:14.444193 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:14.444384 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['高', '林', '豹', ',', '夏', '至', '未', '至', ',', '鲜...',', '无', '心', '法', '师', ',', '超', '能', '造', '梦']\n",
      "00:25:14.445309 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '超', '能', '造', '梦', '第', '二', '季', '什'... '心', '法', '师', ',', '超', '能', '造', '梦', '[SEP]']\n",
      "00:25:14.445586 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 40\n",
      "00:25:14.445794 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6631, 5543, 6863, 3457, 5018, 753, 2108, 7...52, 3791, 2360, 117, 6631, 5543, 6863, 3457, 102]\n",
      "00:25:14.446023 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6631, 5543, 6863, 3457, 5018,  753... 3791, 2360,  117, 6631, 5543, 6863, 3457,  102])\n",
      "00:25:14.446252 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.447040 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.447877 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.449199 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6631, 5543, 6863, 3457, 5018,  75... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009522\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 228\n",
      "00:25:14.451744 call        14     def __getitem__(self, idx):\n",
      "00:25:14.451819 line        15         if self.mode == \"test\":\n",
      "00:25:14.451859 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '继《仙剑奇侠传三》后，又1部“仙侠剧”热播中，你追了吗'\n",
      "New var:....... text_b = '李国立,黄志玮,纪宁,仙侠剧,莽荒纪,仙剑奇侠传三'\n",
      "New var:....... label = 0\n",
      "00:25:14.452440 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.452537 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.452616 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.452878 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['继', '《', '仙', '剑', '奇', '侠', '传', '三', '》', '后... '[UNK]', '热', '播', '中', '，', '你', '追', '了', '吗']\n",
      "00:25:14.453595 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '继', '《', '仙', '剑', '奇', '侠', '传', '三'... '热', '播', '中', '，', '你', '追', '了', '吗', '[SEP]']\n",
      "00:25:14.453795 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:14.453989 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['李', '国', '立', ',', '黄', '志', '玮', ',', '纪', '宁...'莽', '荒', '纪', ',', '仙', '剑', '奇', '侠', '传', '三']\n",
      "00:25:14.454713 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '继', '《', '仙', '剑', '奇', '侠', '传', '三'... '纪', ',', '仙', '剑', '奇', '侠', '传', '三', '[SEP]']\n",
      "00:25:14.455057 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 26\n",
      "00:25:14.455275 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5326, 517, 803, 1187, 1936, 899, 837, 676,..., 5279, 117, 803, 1187, 1936, 899, 837, 676, 102]\n",
      "00:25:14.455500 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5326,  517,  803, 1187, 1936,  899...        803, 1187, 1936,  899,  837,  676,  102])\n",
      "00:25:14.455726 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.456500 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.457352 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.458456 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5326,  517,  803, 1187, 1936,  89...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008887\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 229\n",
      "00:25:14.460663 call        14     def __getitem__(self, idx):\n",
      "00:25:14.460736 line        15         if self.mode == \"test\":\n",
      "00:25:14.460775 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '印度女星惊艳亮相戛纳 传统纱丽充满神秘气息 风采胜中国女星'\n",
      "New var:....... text_b = '戛纳电影节,纱丽,迪皮卡,印度,范冰冰,极限特工3,帕德玛瓦蒂王后'\n",
      "New var:....... label = 0\n",
      "00:25:14.461377 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.461475 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.461553 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.461812 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['印', '度', '女', '星', '惊', '艳', '亮', '相', '戛', '纳...'秘', '气', '息', '风', '采', '胜', '中', '国', '女', '星']\n",
      "00:25:14.462544 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '印', '度', '女', '星', '惊', '艳', '亮', '相'... '息', '风', '采', '胜', '中', '国', '女', '星', '[SEP]']\n",
      "00:25:14.462756 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:14.462957 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['戛', '纳', '电', '影', '节', ',', '纱', '丽', ',', '迪...'工', '3', ',', '帕', '德', '玛', '瓦', '蒂', '王', '后']\n",
      "00:25:14.463781 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '印', '度', '女', '星', '惊', '艳', '亮', '相'... ',', '帕', '德', '玛', '瓦', '蒂', '王', '后', '[SEP]']\n",
      "00:25:14.464106 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 34\n",
      "00:25:14.464327 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1313, 2428, 1957, 3215, 2661, 5683, 778, 4...7, 2364, 2548, 4377, 4482, 5881, 4374, 1400, 102]\n",
      "00:25:14.464634 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1313, 2428, 1957, 3215, 2661, 5683...2548, 4377, 4482, 5881,        4374, 1400,  102])\n",
      "00:25:14.464899 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.465570 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.466429 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.467617 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1313, 2428, 1957, 3215, 2661, 568... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009369\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 230\n",
      "00:25:14.470065 call        14     def __getitem__(self, idx):\n",
      "00:25:14.470141 line        15         if self.mode == \"test\":\n",
      "00:25:14.470182 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '景甜出席Tom Ford品牌活动，破长胖怀孕传闻'\n",
      "New var:....... text_b = 'Tom,景甜,东方ic,东方IC,Ford'\n",
      "New var:....... label = 0\n",
      "00:25:14.470762 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.470865 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.470946 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.471266 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['景', '甜', '出', '席', 'tom', 'ford', '品', '牌', '活', '动', '，', '破', '长', '胖', '怀', '孕', '传', '闻']\n",
      "00:25:14.471861 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '景', '甜', '出', '席', 'tom', 'ford', '品'... '，', '破', '长', '胖', '怀', '孕', '传', '闻', '[SEP]']\n",
      "00:25:14.472043 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 20\n",
      "00:25:14.472239 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['tom', ',', '景', '甜', ',', '东', '方', 'ic', ',', '东', '方', 'ic', ',', 'ford']\n",
      "00:25:14.472747 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '景', '甜', '出', '席', 'tom', 'ford', '品'... 'ic', ',', '东', '方', 'ic', ',', 'ford', '[SEP]']\n",
      "00:25:14.472952 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 15\n",
      "00:25:14.473149 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3250, 4494, 1139, 2375, 9487, 10296, 1501,...175, 8577, 117, 691, 3175, 8577, 117, 10296, 102]\n",
      "00:25:14.473440 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  3250,  4494,  1139,  2375,  9487...  691,         3175,  8577,   117, 10296,   102])\n",
      "00:25:14.473663 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.474193 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.474727 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.475654 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  3250,  4494,  1139,  2375,  948...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007139\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 231\n",
      "00:25:14.477233 call        14     def __getitem__(self, idx):\n",
      "00:25:14.477301 line        15         if self.mode == \"test\":\n",
      "00:25:14.477339 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《高能少年团》许魏洲红袜抢眼，腹肌照送给你们，颜值实力并存'\n",
      "New var:....... text_b = '男主角,黄景瑜,高能少年团,上瘾,大理站,许魏洲,白洛因'\n",
      "New var:....... label = 0\n",
      "00:25:14.477884 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.478073 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.478152 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.478338 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '高', '能', '少', '年', '团', '》', '许', '魏', '洲...'给', '你', '们', '，', '颜', '值', '实', '力', '并', '存']\n",
      "00:25:14.479088 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '高', '能', '少', '年', '团', '》', '许'... '们', '，', '颜', '值', '实', '力', '并', '存', '[SEP]']\n",
      "00:25:14.479285 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:14.479475 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['男', '主', '角', ',', '黄', '景', '瑜', ',', '高', '能...'理', '站', ',', '许', '魏', '洲', ',', '白', '洛', '因']\n",
      "00:25:14.480199 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '高', '能', '少', '年', '团', '》', '许'... ',', '许', '魏', '洲', ',', '白', '洛', '因', '[SEP]']\n",
      "00:25:14.480472 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 29\n",
      "00:25:14.480675 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 7770, 5543, 2208, 2399, 1730, 518, 63...17, 6387, 7794, 3828, 117, 4635, 3821, 1728, 102]\n",
      "00:25:14.480897 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 7770, 5543, 2208, 2399, 1730... 6387, 7794, 3828,  117, 4635, 3821, 1728,  102])\n",
      "00:25:14.481123 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.481809 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.482530 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.484521 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 7770, 5543, 2208, 2399, 173... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009798\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 232\n",
      "00:25:14.487069 call        14     def __getitem__(self, idx):\n",
      "00:25:14.487148 line        15         if self.mode == \"test\":\n",
      "00:25:14.487189 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '看张翰的街拍照，这身材真是好，难怪穿什么都帅！'\n",
      "New var:....... text_b = '张翰,张翰穿'\n",
      "New var:....... label = 0\n",
      "00:25:14.487827 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.487927 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.488091 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.488349 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['看', '张', '翰', '的', '街', '拍', '照', '，', '这', '身...'好', '，', '难', '怪', '穿', '什', '么', '都', '帅', '！']\n",
      "00:25:14.489118 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '看', '张', '翰', '的', '街', '拍', '照', '，'... '难', '怪', '穿', '什', '么', '都', '帅', '！', '[SEP]']\n",
      "00:25:14.489854 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:14.489982 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['张', '翰', ',', '张', '翰', '穿']\n",
      "00:25:14.490279 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '看', '张', '翰', '的', '街', '拍', '照', '，'..., '[SEP]', '张', '翰', ',', '张', '翰', '穿', '[SEP]']\n",
      "00:25:14.490523 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 7\n",
      "00:25:14.490682 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4692, 2476, 5432, 4638, 6125, 2864, 4212, ...013, 102, 2476, 5432, 117, 2476, 5432, 4959, 102]\n",
      "00:25:14.497600 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4692, 2476, 5432, 4638, 6125, 2864...  102, 2476, 5432,  117, 2476, 5432, 4959,  102])\n",
      "00:25:14.497974 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.498395 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0, 0, 0,        0, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.498907 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.499954 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4692, 2476, 5432, 4638, 6125, 286...0, 0,        0, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.015317\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 233\n",
      "00:25:14.502418 call        14     def __getitem__(self, idx):\n",
      "00:25:14.502492 line        15         if self.mode == \"test\":\n",
      "00:25:14.502530 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = 'EXO KAI金钟仁 7日父亲去世'\n",
      "New var:....... text_b = '演艺界,金钟仁,exo,EXO,KAI'\n",
      "New var:....... label = 0\n",
      "00:25:14.503133 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.503231 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.503313 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.503569 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['exo', 'kai', '金', '钟', '仁', '7', '日', '父', '亲', '去', '世']\n",
      "00:25:14.504100 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'exo', 'kai', '金', '钟', '仁', '7', '日', '父', '亲', '去', '世', '[SEP]']\n",
      "00:25:14.504348 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 13\n",
      "00:25:14.504732 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['演', '艺', '界', ',', '金', '钟', '仁', ',', 'exo', ',', 'exo', ',', 'kai']\n",
      "00:25:14.505309 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'exo', 'kai', '金', '钟', '仁', '7', '日',...'仁', ',', 'exo', ',', 'exo', ',', 'kai', '[SEP]']\n",
      "00:25:14.505549 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "00:25:14.505728 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 11184, 13072, 7032, 7164, 785, 128, 3189, ...64, 785, 117, 11184, 117, 11184, 117, 13072, 102]\n",
      "00:25:14.505996 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101, 11184, 13072,  7032,  7164,   785...  117, 11184,   117, 11184,   117, 13072,   102])\n",
      "00:25:14.506263 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.506984 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:25:14.507586 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.508470 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101, 11184, 13072,  7032,  7164,   78...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008068\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 234\n",
      "00:25:14.510519 call        14     def __getitem__(self, idx):\n",
      "00:25:14.510594 line        15         if self.mode == \"test\":\n",
      "00:25:14.510634 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《欢乐颂》\\u2002的结局是什么？'\n",
      "New var:....... text_b = '欢乐颂,邱莹莹,关关,谢滨,欢乐颂2,曲筱绡'\n",
      "New var:....... label = 0\n",
      "00:25:14.511207 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.511304 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.511381 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.511740 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '欢', '乐', '颂', '》', '的', '结', '局', '是', '什', '么', '？']\n",
      "00:25:14.512232 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '欢', '乐', '颂', '》', '的', '结', '局', '是', '什', '么', '？', '[SEP]']\n",
      "00:25:14.512471 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 14\n",
      "00:25:14.512704 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['欢', '乐', '颂', ',', '邱', '莹', '莹', ',', '关', '关...'滨', ',', '欢', '乐', '颂', '2', ',', '曲', '筱', '绡']\n",
      "00:25:14.513358 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '欢', '乐', '颂', '》', '的', '结', '局'... '欢', '乐', '颂', '2', ',', '曲', '筱', '绡', '[SEP]']\n",
      "00:25:14.513606 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:25:14.513965 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 3614, 727, 7563, 518, 4638, 5310, 222...3614, 727, 7563, 123, 117, 3289, 5036, 5321, 102]\n",
      "00:25:14.514223 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 3614,  727, 7563,  518, 4638...7563,  123,  117, 3289, 5036, 5321,         102])\n",
      "00:25:14.514486 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.515124 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.515911 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.517074 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 3614,  727, 7563,  518, 463... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008824\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 235\n",
      "00:25:14.519375 call        14     def __getitem__(self, idx):\n",
      "00:25:14.519450 line        15         if self.mode == \"test\":\n",
      "00:25:14.519489 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '4部最适合保存在硬盘里的电影，少一部都不行'\n",
      "New var:....... text_b = '爱情与灵药,海瑟薇,电影,赵寅成,不伦之恋'\n",
      "New var:....... label = 0\n",
      "00:25:14.520026 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.520123 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.520199 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.520441 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['4', '部', '最', '适', '合', '保', '存', '在', '硬', '盘...'的', '电', '影', '，', '少', '一', '部', '都', '不', '行']\n",
      "00:25:14.521163 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '4', '部', '最', '适', '合', '保', '存', '在'... '影', '，', '少', '一', '部', '都', '不', '行', '[SEP]']\n",
      "00:25:14.521504 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:25:14.521673 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['爱', '情', '与', '灵', '药', ',', '海', '瑟', '薇', ',...'影', ',', '赵', '寅', '成', ',', '不', '伦', '之', '恋']\n",
      "00:25:14.522324 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '4', '部', '最', '适', '合', '保', '存', '在'... '赵', '寅', '成', ',', '不', '伦', '之', '恋', '[SEP]']\n",
      "00:25:14.522575 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 22\n",
      "00:25:14.522822 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 125, 6956, 3297, 6844, 1394, 924, 2100, 17... 6627, 2165, 2768, 117, 679, 840, 722, 2605, 102]\n",
      "00:25:14.523084 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  125, 6956, 3297, 6844, 1394,  924... 2165, 2768,  117,  679,  840,  722, 2605,  102])\n",
      "00:25:14.523353 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.524165 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.524851 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.525943 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  125, 6956, 3297, 6844, 1394,  92... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008830\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 236\n",
      "00:25:14.528236 call        14     def __getitem__(self, idx):\n",
      "00:25:14.528309 line        15         if self.mode == \"test\":\n",
      "00:25:14.528347 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '复联3无剧透观影指南.doc'\n",
      "New var:....... text_b = 'Avengers,漫威漫画,Marvel,漫威,复联3,复仇者联盟,权游'\n",
      "New var:....... label = 0\n",
      "00:25:14.528895 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.528990 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.529067 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.529434 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['复', '联', '3', '无', '剧', '透', '观', '影', '指', '南', '.', 'doc']\n",
      "00:25:14.529938 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '复', '联', '3', '无', '剧', '透', '观', '影', '指', '南', '.', 'doc', '[SEP]']\n",
      "00:25:14.530179 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 14\n",
      "00:25:14.530411 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['av', '##eng', '##ers', ',', '漫', '威', '漫', '画'...'3', ',', '复', '仇', '者', '联', '盟', ',', '权', '游']\n",
      "00:25:14.531207 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '复', '联', '3', '无', '剧', '透', '观', '影'... '复', '仇', '者', '联', '盟', ',', '权', '游', '[SEP]']\n",
      "00:25:14.531580 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 28\n",
      "00:25:14.531828 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1908, 5468, 124, 3187, 1196, 6851, 6225, 2...908, 790, 5442, 5468, 4673, 117, 3326, 3952, 102]\n",
      "00:25:14.532100 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  1908,  5468,   124,  3187,  1196... 5468,  4673,   117,  3326,         3952,   102])\n",
      "00:25:14.532367 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.533058 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.533843 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.534906 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  1908,  5468,   124,  3187,  119... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009054\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 237\n",
      "00:25:14.537321 call        14     def __getitem__(self, idx):\n",
      "00:25:14.537394 line        15         if self.mode == \"test\":\n",
      "00:25:14.537433 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '81岁谢贤对张柏芝念念不忘，小孙子这话打脸谢霆锋'\n",
      "New var:....... text_b = '谢贤,谢霆锋,张柏芝,孙悟空,小孙子'\n",
      "New var:....... label = 0\n",
      "00:25:14.537986 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.538083 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.538288 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.538522 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['81', '岁', '谢', '贤', '对', '张', '柏', '芝', '念', '...'小', '孙', '子', '这', '话', '打', '脸', '谢', '霆', '锋']\n",
      "00:25:14.539185 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '81', '岁', '谢', '贤', '对', '张', '柏', '芝... '子', '这', '话', '打', '脸', '谢', '霆', '锋', '[SEP]']\n",
      "00:25:14.539357 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:14.539596 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['谢', '贤', ',', '谢', '霆', '锋', ',', '张', '柏', '芝', ',', '孙', '悟', '空', ',', '小', '孙', '子']\n",
      "00:25:14.540188 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '81', '岁', '谢', '贤', '对', '张', '柏', '芝... ',', '孙', '悟', '空', ',', '小', '孙', '子', '[SEP]']\n",
      "00:25:14.540573 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:25:14.540758 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 8424, 2259, 6468, 6570, 2190, 2476, 3377, ...17, 2101, 2640, 4958, 117, 2207, 2101, 2094, 102]\n",
      "00:25:14.541027 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 8424, 2259, 6468, 6570, 2190, 2476... 2101, 2640, 4958,  117, 2207, 2101, 2094,  102])\n",
      "00:25:14.541298 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.541979 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.542779 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.543878 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 8424, 2259, 6468, 6570, 2190, 247... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008870\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 238\n",
      "00:25:14.546221 call        14     def __getitem__(self, idx):\n",
      "00:25:14.546292 line        15         if self.mode == \"test\":\n",
      "00:25:14.546331 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '真性情还是情商低？那些年蒋欣插的刀，说赵丽颖假唱认领刘涛黑料'\n",
      "New var:....... text_b = '蒋欣,欢乐颂,甄嬛传,假唱,华妃,刘涛,赵丽颖'\n",
      "New var:....... label = 0\n",
      "00:25:14.546863 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.546966 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.547043 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.547417 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['真', '性', '情', '还', '是', '情', '商', '低', '？', '那...'丽', '颖', '假', '唱', '认', '领', '刘', '涛', '黑', '料']\n",
      "00:25:14.548236 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '真', '性', '情', '还', '是', '情', '商', '低'... '假', '唱', '认', '领', '刘', '涛', '黑', '料', '[SEP]']\n",
      "00:25:14.548481 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:14.548722 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['蒋', '欣', ',', '欢', '乐', '颂', ',', '甄', '嬛', '传...',', '华', '妃', ',', '刘', '涛', ',', '赵', '丽', '颖']\n",
      "00:25:14.549400 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '真', '性', '情', '还', '是', '情', '商', '低'... '妃', ',', '刘', '涛', ',', '赵', '丽', '颖', '[SEP]']\n",
      "00:25:14.549805 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "00:25:14.550126 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4696, 2595, 2658, 6820, 3221, 2658, 1555, ...1964, 117, 1155, 3875, 117, 6627, 714, 7577, 102]\n",
      "00:25:14.550483 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4696, 2595, 2658, 6820, 3221, 2658...  117, 1155, 3875,  117, 6627,  714, 7577,  102])\n",
      "00:25:14.550800 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.551690 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.552610 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.553857 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4696, 2595, 2658, 6820, 3221, 265...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009769\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 239\n",
      "00:25:14.556024 call        14     def __getitem__(self, idx):\n",
      "00:25:14.556099 line        15         if self.mode == \"test\":\n",
      "00:25:14.556139 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '范冰冰与李宇春在夏纳红毯上拼着，“大魔王”穿着旧衣服经过'\n",
      "New var:....... text_b = '范冰冰,李宇春,电影节,戛纳国际电影节,凯特·布兰切特'\n",
      "New var:....... label = 0\n",
      "00:25:14.556746 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.556844 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.556926 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.557156 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['范', '冰', '冰', '与', '李', '宇', '春', '在', '夏', '纳... '王', '[UNK]', '穿', '着', '旧', '衣', '服', '经', '过']\n",
      "00:25:14.557901 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '范', '冰', '冰', '与', '李', '宇', '春', '在'...NK]', '穿', '着', '旧', '衣', '服', '经', '过', '[SEP]']\n",
      "00:25:14.558177 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:25:14.558373 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['范', '冰', '冰', ',', '李', '宇', '春', ',', '电', '影...'影', '节', ',', '凯', '特', '·', '布', '兰', '切', '特']\n",
      "00:25:14.559086 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '范', '冰', '冰', '与', '李', '宇', '春', '在'... ',', '凯', '特', '·', '布', '兰', '切', '特', '[SEP]']\n",
      "00:25:14.559293 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 28\n",
      "00:25:14.559494 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5745, 1102, 1102, 680, 3330, 2126, 3217, 1...17, 1132, 4294, 185, 2357, 1065, 1147, 4294, 102]\n",
      "00:25:14.559715 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5745, 1102, 1102,  680, 3330, 2126... 1132, 4294,  185, 2357, 1065, 1147, 4294,  102])\n",
      "00:25:14.559939 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.560629 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.561313 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.562441 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5745, 1102, 1102,  680, 3330, 212...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008702\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 240\n",
      "00:25:14.564757 call        14     def __getitem__(self, idx):\n",
      "00:25:14.564827 line        15         if self.mode == \"test\":\n",
      "00:25:14.564866 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《前任3》林佳的孩子到底是谁的？'\n",
      "New var:....... text_b = '孟云,聊聊吧,林佳,痛彻心扉,电影'\n",
      "New var:....... label = 0\n",
      "00:25:14.565439 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.565534 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.565610 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.565866 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '前', '任', '3', '》', '林', '佳', '的', '孩', '子', '到', '底', '是', '谁', '的', '？']\n",
      "00:25:14.566374 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '前', '任', '3', '》', '林', '佳', '的', '孩', '子', '到', '底', '是', '谁', '的', '？', '[SEP]']\n",
      "00:25:14.566565 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 18\n",
      "00:25:14.566749 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['孟', '云', ',', '聊', '聊', '吧', ',', '林', '佳', ',', '痛', '彻', '心', '扉', ',', '电', '影']\n",
      "00:25:14.567265 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '前', '任', '3', '》', '林', '佳', '的'... ',', '痛', '彻', '心', '扉', ',', '电', '影', '[SEP]']\n",
      "00:25:14.567460 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:14.567653 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 1184, 818, 124, 518, 3360, 881, 4638,...17, 4578, 2515, 2552, 2796, 117, 4510, 2512, 102]\n",
      "00:25:14.567857 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 1184,  818,  124,  518, 3360... 4578, 2515, 2552, 2796,  117, 4510, 2512,  102])\n",
      "00:25:14.568174 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.568765 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.569337 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.570305 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 1184,  818,  124,  518, 336... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007364\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 241\n",
      "00:25:14.572152 call        14     def __getitem__(self, idx):\n",
      "00:25:14.572226 line        15         if self.mode == \"test\":\n",
      "00:25:14.572265 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '网友在秀场偶遇迪丽热巴 没有打光 纯路人拍 这颜值你们感受下'\n",
      "New var:....... text_b = '热巴,颜值,路人拍,秀场偶遇迪丽热巴,秀场'\n",
      "New var:....... label = 0\n",
      "00:25:14.572823 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.573077 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.573170 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.573455 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['网', '友', '在', '秀', '场', '偶', '遇', '迪', '丽', '热...'人', '拍', '这', '颜', '值', '你', '们', '感', '受', '下']\n",
      "00:25:14.574196 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '网', '友', '在', '秀', '场', '偶', '遇', '迪'... '这', '颜', '值', '你', '们', '感', '受', '下', '[SEP]']\n",
      "00:25:14.574396 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:14.574625 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['热', '巴', ',', '颜', '值', ',', '路', '人', '拍', ',...'场', '偶', '遇', '迪', '丽', '热', '巴', ',', '秀', '场']\n",
      "00:25:14.575235 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '网', '友', '在', '秀', '场', '偶', '遇', '迪'... '遇', '迪', '丽', '热', '巴', ',', '秀', '场', '[SEP]']\n",
      "00:25:14.575531 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 22\n",
      "00:25:14.575745 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5381, 1351, 1762, 4899, 1767, 981, 6878, 6...878, 6832, 714, 4178, 2349, 117, 4899, 1767, 102]\n",
      "00:25:14.575972 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5381, 1351, 1762, 4899, 1767,  981... 714, 4178, 2349,  117,        4899, 1767,  102])\n",
      "00:25:14.576199 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.576700 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:25:14.577484 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.578578 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5381, 1351, 1762, 4899, 1767,  98...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008548\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 242\n",
      "00:25:14.580734 call        14     def __getitem__(self, idx):\n",
      "00:25:14.580809 line        15         if self.mode == \"test\":\n",
      "00:25:14.580849 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '蓝盈莹妈妈气质太好，50岁身材美貌不输20岁！'\n",
      "New var:....... text_b = '刘亦菲,母亲节,蓝盈莹,温碧霞,娱乐圈'\n",
      "New var:....... label = 0\n",
      "00:25:14.581463 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.581560 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.581640 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.581830 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['蓝', '盈', '莹', '妈', '妈', '气', '质', '太', '好', '，...岁', '身', '材', '美', '貌', '不', '输', '20', '岁', '！']\n",
      "00:25:14.582457 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '蓝', '盈', '莹', '妈', '妈', '气', '质', '太'...'材', '美', '貌', '不', '输', '20', '岁', '！', '[SEP]']\n",
      "00:25:14.582666 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:25:14.582929 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['刘', '亦', '菲', ',', '母', '亲', '节', ',', '蓝', '盈', '莹', ',', '温', '碧', '霞', ',', '娱', '乐', '圈']\n",
      "00:25:14.583501 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '蓝', '盈', '莹', '妈', '妈', '气', '质', '太'... ',', '温', '碧', '霞', ',', '娱', '乐', '圈', '[SEP]']\n",
      "00:25:14.583704 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:14.583903 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5905, 4659, 5816, 1968, 1968, 3698, 6574, ...117, 3946, 4819, 7459, 117, 2031, 727, 1750, 102]\n",
      "00:25:14.584119 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5905, 4659, 5816, 1968, 1968, 3698...       4819, 7459,  117, 2031,  727, 1750,  102])\n",
      "00:25:14.584337 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.584917 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.585561 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.586500 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5905, 4659, 5816, 1968, 1968, 369... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007720\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 243\n",
      "00:25:14.588490 call        14     def __getitem__(self, idx):\n",
      "00:25:14.588565 line        15         if self.mode == \"test\":\n",
      "00:25:14.588605 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '台湾女星低胸比基尼回馈粉丝，33岁生日撒福利'\n",
      "New var:....... text_b = '新昨夜星辰,李毓芬,社交网,台湾,比基尼,我型我秀,Dream Girls'\n",
      "New var:....... label = 0\n",
      "00:25:14.589220 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.589318 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.589398 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.589589 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['台', '湾', '女', '星', '低', '胸', '比', '基', '尼', '回...粉', '丝', '，', '33', '岁', '生', '日', '撒', '福', '利']\n",
      "00:25:14.590204 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '台', '湾', '女', '星', '低', '胸', '比', '基'...'，', '33', '岁', '生', '日', '撒', '福', '利', '[SEP]']\n",
      "00:25:14.590402 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:25:14.590693 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['新', '昨', '夜', '星', '辰', ',', '李', '毓', '芬', ',..., ',', '我', '型', '我', '秀', ',', 'dream', 'girls']\n",
      "00:25:14.591523 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '台', '湾', '女', '星', '低', '胸', '比', '基'...', '型', '我', '秀', ',', 'dream', 'girls', '[SEP]']\n",
      "00:25:14.591742 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 29\n",
      "00:25:14.591952 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1378, 3968, 1957, 3215, 856, 5541, 3683, 1..., 2769, 1798, 2769, 4899, 117, 10252, 10803, 102]\n",
      "00:25:14.592179 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  1378,  3968,  1957,  3215,   856... 2769,  4899,   117, 10252,        10803,   102])\n",
      "00:25:14.592406 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.593086 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:25:14.593754 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.594816 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  1378,  3968,  1957,  3215,   85...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008485\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 244\n",
      "00:25:14.597008 call        14     def __getitem__(self, idx):\n",
      "00:25:14.597078 line        15         if self.mode == \"test\":\n",
      "00:25:14.597118 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《甄嬛传》气死皇上的不是眉庄和温实初的孩子，而是甄嬛的这句话'\n",
      "New var:....... text_b = '大理寺少卿,华妃,政治,眉庄,甄嬛传'\n",
      "New var:....... label = 0\n",
      "00:25:14.597721 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.597812 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.597886 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.598002 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '甄', '嬛', '传', '》', '气', '死', '皇', '上', '的...'子', '，', '而', '是', '甄', '嬛', '的', '这', '句', '话']\n",
      "00:25:14.598808 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '甄', '嬛', '传', '》', '气', '死', '皇'... '而', '是', '甄', '嬛', '的', '这', '句', '话', '[SEP]']\n",
      "00:25:14.598940 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:14.599208 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['大', '理', '寺', '少', '卿', ',', '华', '妃', ',', '政', '治', ',', '眉', '庄', ',', '甄', '嬛', '传']\n",
      "00:25:14.600107 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '甄', '嬛', '传', '》', '气', '死', '皇'... '治', ',', '眉', '庄', ',', '甄', '嬛', '传', '[SEP]']\n",
      "00:25:14.600600 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:25:14.600843 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 4488, 2083, 837, 518, 3698, 3647, 464...3780, 117, 4691, 2411, 117, 4488, 2083, 837, 102]\n",
      "00:25:14.601107 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 4488, 2083,  837,  518, 3698...4691, 2411,  117, 4488,        2083,  837,  102])\n",
      "00:25:14.601403 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.602045 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:25:14.602746 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.603898 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 4488, 2083,  837,  518, 369...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009071\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 245\n",
      "00:25:14.606110 call        14     def __getitem__(self, idx):\n",
      "00:25:14.606179 line        15         if self.mode == \"test\":\n",
      "00:25:14.606217 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '长润影视的发展模式'\n",
      "New var:....... text_b = '演员,票房,剧本,电影,影视公司'\n",
      "New var:....... label = 0\n",
      "00:25:14.606818 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.606923 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.607001 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.607223 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['长', '润', '影', '视', '的', '发', '展', '模', '式']\n",
      "00:25:14.607549 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '长', '润', '影', '视', '的', '发', '展', '模', '式', '[SEP]']\n",
      "00:25:14.607672 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 11\n",
      "00:25:14.607786 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['演', '员', ',', '票', '房', ',', '剧', '本', ',', '电', '影', ',', '影', '视', '公', '司']\n",
      "00:25:14.608218 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '长', '润', '影', '视', '的', '发', '展', '模'... ',', '电', '影', ',', '影', '视', '公', '司', '[SEP]']\n",
      "00:25:14.608454 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 17\n",
      "00:25:14.608654 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7270, 3883, 2512, 6228, 4638, 1355, 2245, ...17, 4510, 2512, 117, 2512, 6228, 1062, 1385, 102]\n",
      "00:25:14.608859 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7270, 3883, 2512, 6228, 4638, 1355...2512,  117, 2512,        6228, 1062, 1385,  102])\n",
      "00:25:14.609071 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.609617 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:25:14.610099 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.610875 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7270, 3883, 2512, 6228, 4638, 135...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006304\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 246\n",
      "00:25:14.612447 call        14     def __getitem__(self, idx):\n",
      "00:25:14.612519 line        15         if self.mode == \"test\":\n",
      "00:25:14.612558 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '娘娘气场直逼两米三 蒋欣霸气回京大白腿太晃眼'\n",
      "New var:....... text_b = '蒋欣,娘娘'\n",
      "New var:....... label = 0\n",
      "00:25:14.613206 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.613304 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.613384 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.613578 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['娘', '娘', '气', '场', '直', '逼', '两', '米', '三', '蒋...'霸', '气', '回', '京', '大', '白', '腿', '太', '晃', '眼']\n",
      "00:25:14.614191 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娘', '娘', '气', '场', '直', '逼', '两', '米'... '回', '京', '大', '白', '腿', '太', '晃', '眼', '[SEP]']\n",
      "00:25:14.614385 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:25:14.614641 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['蒋', '欣', ',', '娘', '娘']\n",
      "00:25:14.614953 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娘', '娘', '气', '场', '直', '逼', '两', '米'..., '眼', '[SEP]', '蒋', '欣', ',', '娘', '娘', '[SEP]']\n",
      "00:25:14.615152 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 6\n",
      "00:25:14.615348 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2023, 2023, 3698, 1767, 4684, 6873, 697, 5...230, 4706, 102, 5882, 3615, 117, 2023, 2023, 102]\n",
      "00:25:14.615551 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2023, 2023, 3698, 1767, 4684, 6873... 102, 5882,        3615,  117, 2023, 2023,  102])\n",
      "00:25:14.615762 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.616249 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0, 0, 0, 0, 0, 1,        1, 1, 1, 1, 1])\n",
      "00:25:14.616939 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.617727 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2023, 2023, 3698, 1767, 4684, 687...0, 0, 0, 0, 1,        1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006810\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 247\n",
      "00:25:14.619291 call        14     def __getitem__(self, idx):\n",
      "00:25:14.619365 line        15         if self.mode == \"test\":\n",
      "00:25:14.619406 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '李敏镐豪宅内景疑曝光墙上贴着自己的帅照'\n",
      "New var:....... text_b = '棒球帽,豪宅,粉丝,李敏镐'\n",
      "New var:....... label = 0\n",
      "00:25:14.620025 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.620124 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.620203 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.620393 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['李', '敏', '镐', '豪', '宅', '内', '景', '疑', '曝', '光', '墙', '上', '贴', '着', '自', '己', '的', '帅', '照']\n",
      "00:25:14.620962 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '敏', '镐', '豪', '宅', '内', '景', '疑'... '上', '贴', '着', '自', '己', '的', '帅', '照', '[SEP]']\n",
      "00:25:14.621157 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:25:14.621412 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['棒', '球', '帽', ',', '豪', '宅', ',', '粉', '丝', ',', '李', '敏', '镐']\n",
      "00:25:14.621866 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '敏', '镐', '豪', '宅', '内', '景', '疑'... '宅', ',', '粉', '丝', ',', '李', '敏', '镐', '[SEP]']\n",
      "00:25:14.622109 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "00:25:14.622315 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3330, 3130, 7256, 6498, 2125, 1079, 3250, ...2125, 117, 5106, 692, 117, 3330, 3130, 7256, 102]\n",
      "00:25:14.622527 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3330, 3130, 7256, 6498, 2125, 1079...  117, 5106,  692,  117, 3330, 3130, 7256,  102])\n",
      "00:25:14.622747 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.623281 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.623819 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.624663 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3330, 3130, 7256, 6498, 2125, 107...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007100\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 248\n",
      "00:25:14.626431 call        14     def __getitem__(self, idx):\n",
      "00:25:14.626511 line        15         if self.mode == \"test\":\n",
      "00:25:14.626552 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '李敏镐晒照疑曝光自家房产 千万豪宅内部长这样'\n",
      "New var:....... text_b = '社交网站,翻糖蛋糕,粉丝,李敏镐,豪宅'\n",
      "New var:....... label = 0\n",
      "00:25:14.627206 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.627307 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.627496 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.627764 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['李', '敏', '镐', '晒', '照', '疑', '曝', '光', '自', '家...'产', '千', '万', '豪', '宅', '内', '部', '长', '这', '样']\n",
      "00:25:14.628384 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '敏', '镐', '晒', '照', '疑', '曝', '光'... '万', '豪', '宅', '内', '部', '长', '这', '样', '[SEP]']\n",
      "00:25:14.628681 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:25:14.628902 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['社', '交', '网', '站', ',', '翻', '糖', '蛋', '糕', ',', '粉', '丝', ',', '李', '敏', '镐', ',', '豪', '宅']\n",
      "00:25:14.629485 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '敏', '镐', '晒', '照', '疑', '曝', '光'... '丝', ',', '李', '敏', '镐', ',', '豪', '宅', '[SEP]']\n",
      "00:25:14.629696 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:14.629895 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3330, 3130, 7256, 3235, 4212, 4542, 3284, ...692, 117, 3330, 3130, 7256, 117, 6498, 2125, 102]\n",
      "00:25:14.630111 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3330, 3130, 7256, 3235, 4212, 4542...       3330, 3130, 7256,  117, 6498, 2125,  102])\n",
      "00:25:14.630332 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.631018 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.631640 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.632586 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3330, 3130, 7256, 3235, 4212, 454... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008135\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 249\n",
      "00:25:14.634598 call        14     def __getitem__(self, idx):\n",
      "00:25:14.634673 line        15         if self.mode == \"test\":\n",
      "00:25:14.634713 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '胡彦斌《爱不得 恨不得 舍不得》打造爆款情歌'\n",
      "New var:....... text_b = '胡彦斌,爱不得恨不得舍不得,情歌,覅忒好,QQ音乐,创造101,酷狗音乐'\n",
      "New var:....... label = 0\n",
      "00:25:14.635389 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.635494 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.635583 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.635964 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['胡', '彦', '斌', '《', '爱', '不', '得', '恨', '不', '得', '舍', '不', '得', '》', '打', '造', '爆', '款', '情', '歌']\n",
      "00:25:14.636566 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '胡', '彦', '斌', '《', '爱', '不', '得', '恨'... '得', '》', '打', '造', '爆', '款', '情', '歌', '[SEP]']\n",
      "00:25:14.636774 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 22\n",
      "00:25:14.636966 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['胡', '彦', '斌', ',', '爱', '不', '得', '恨', '不', '得...', ',', '创', '造', '101', ',', '酷', '狗', '音', '乐']\n",
      "00:25:14.637806 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '胡', '彦', '斌', '《', '爱', '不', '得', '恨'...创', '造', '101', ',', '酷', '狗', '音', '乐', '[SEP]']\n",
      "00:25:14.638135 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 34\n",
      "00:25:14.638368 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5529, 2504, 3154, 517, 4263, 679, 2533, 26...158, 6863, 8359, 117, 6999, 4318, 7509, 727, 102]\n",
      "00:25:14.638600 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5529, 2504, 3154,  517, 4263,  679... 6863, 8359,  117, 6999, 4318, 7509,  727,  102])\n",
      "00:25:14.638834 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.639503 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.640164 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.641336 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5529, 2504, 3154,  517, 4263,  67...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008930\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 250\n",
      "00:25:14.643562 call        14     def __getitem__(self, idx):\n",
      "00:25:14.643636 line        15         if self.mode == \"test\":\n",
      "00:25:14.643675 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '琅琊榜：风起长林江左盟为何消失？梅长苏曾犯下大错，江左盟解散'\n",
      "New var:....... text_b = '琅琊榜,江左,梅长苏,江左盟,接班人'\n",
      "New var:....... label = 0\n",
      "00:25:14.644306 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.644406 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.644486 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.644674 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['琅', '琊', '榜', '：', '风', '起', '长', '林', '江', '左...'犯', '下', '大', '错', '，', '江', '左', '盟', '解', '散']\n",
      "00:25:14.645446 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '琅', '琊', '榜', '：', '风', '起', '长', '林'... '大', '错', '，', '江', '左', '盟', '解', '散', '[SEP]']\n",
      "00:25:14.645711 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:14.645904 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['琅', '琊', '榜', ',', '江', '左', ',', '梅', '长', '苏', ',', '江', '左', '盟', ',', '接', '班', '人']\n",
      "00:25:14.646449 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '琅', '琊', '榜', '：', '风', '起', '长', '林'... ',', '江', '左', '盟', ',', '接', '班', '人', '[SEP]']\n",
      "00:25:14.646651 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:25:14.646851 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4414, 4418, 3528, 8038, 7599, 6629, 7270, ...117, 3736, 2340, 4673, 117, 2970, 4408, 782, 102]\n",
      "00:25:14.647126 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4414, 4418, 3528, 8038, 7599, 6629...2340, 4673,  117, 2970,        4408,  782,  102])\n",
      "00:25:14.647375 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.648088 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:25:14.648729 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.649782 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4414, 4418, 3528, 8038, 7599, 662...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008338\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 251\n",
      "00:25:14.651933 call        14     def __getitem__(self, idx):\n",
      "00:25:14.652009 line        15         if self.mode == \"test\":\n",
      "00:25:14.652048 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '阚清子自曝曾看心理医生？携奚梦瑶、谢依霖草莓园放飞自我'\n",
      "New var:....... text_b = '谢依霖,奚梦瑶,草莓园,浙江卫视,阚清子'\n",
      "New var:....... label = 0\n",
      "00:25:14.652654 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.652752 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.652834 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.653088 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['阚', '清', '子', '自', '曝', '曾', '看', '心', '理', '医...'谢', '依', '霖', '草', '莓', '园', '放', '飞', '自', '我']\n",
      "00:25:14.653810 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '阚', '清', '子', '自', '曝', '曾', '看', '心'... '霖', '草', '莓', '园', '放', '飞', '自', '我', '[SEP]']\n",
      "00:25:14.654049 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:14.654284 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['谢', '依', '霖', ',', '奚', '梦', '瑶', ',', '草', '莓', '园', ',', '浙', '江', '卫', '视', ',', '阚', '清', '子']\n",
      "00:25:14.654886 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '阚', '清', '子', '自', '曝', '曾', '看', '心'... '浙', '江', '卫', '视', ',', '阚', '清', '子', '[SEP]']\n",
      "00:25:14.655095 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:25:14.655309 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7337, 3926, 2094, 5632, 3284, 3295, 4692, ...51, 3736, 1310, 6228, 117, 7337, 3926, 2094, 102]\n",
      "00:25:14.655530 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7337, 3926, 2094, 5632, 3284, 3295...1310, 6228,  117, 7337, 3926,        2094,  102])\n",
      "00:25:14.655755 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.656381 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:25:14.657043 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.658454 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7337, 3926, 2094, 5632, 3284, 329...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008535\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 252\n",
      "00:25:14.660502 call        14     def __getitem__(self, idx):\n",
      "00:25:14.660647 line        15         if self.mode == \"test\":\n",
      "00:25:14.660688 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '舒淇发素颜自拍照，脸上的疙瘩，斑点，头上的白发，这要掉多少粉'\n",
      "New var:....... text_b = '三级片,女演员,舒淇,商业片,文艺片'\n",
      "New var:....... label = 0\n",
      "00:25:14.661293 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.661393 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.661476 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.661667 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['舒', '淇', '发', '素', '颜', '自', '拍', '照', '，', '脸...'的', '白', '发', '，', '这', '要', '掉', '多', '少', '粉']\n",
      "00:25:14.662486 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '舒', '淇', '发', '素', '颜', '自', '拍', '照'... '发', '，', '这', '要', '掉', '多', '少', '粉', '[SEP]']\n",
      "00:25:14.662700 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:14.662983 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['三', '级', '片', ',', '女', '演', '员', ',', '舒', '淇', ',', '商', '业', '片', ',', '文', '艺', '片']\n",
      "00:25:14.663544 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '舒', '淇', '发', '素', '颜', '自', '拍', '照'... ',', '商', '业', '片', ',', '文', '艺', '片', '[SEP]']\n",
      "00:25:14.663751 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:25:14.663949 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5653, 3899, 1355, 5162, 7582, 5632, 2864, ...117, 1555, 689, 4275, 117, 3152, 5686, 4275, 102]\n",
      "00:25:14.664168 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5653, 3899, 1355, 5162, 7582, 5632... 689, 4275,  117, 3152,        5686, 4275,  102])\n",
      "00:25:14.664392 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.665238 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:25:14.665912 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.666971 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5653, 3899, 1355, 5162, 7582, 563...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008626\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 253\n",
      "00:25:14.669165 call        14     def __getitem__(self, idx):\n",
      "00:25:14.669244 line        15         if self.mode == \"test\":\n",
      "00:25:14.669284 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '黄圣依做蛋炒饭大手笔，撒了一层猪头肉！安迪：我现在就想吃'\n",
      "New var:....... text_b = '黄圣依,猪头肉,安迪,蛋炒饭'\n",
      "New var:....... label = 0\n",
      "00:25:14.669900 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.670001 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.670088 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.670558 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['黄', '圣', '依', '做', '蛋', '炒', '饭', '大', '手', '笔...'！', '安', '迪', '：', '我', '现', '在', '就', '想', '吃']\n",
      "00:25:14.671323 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '黄', '圣', '依', '做', '蛋', '炒', '饭', '大'... '迪', '：', '我', '现', '在', '就', '想', '吃', '[SEP]']\n",
      "00:25:14.671531 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:25:14.671726 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['黄', '圣', '依', ',', '猪', '头', '肉', ',', '安', '迪', ',', '蛋', '炒', '饭']\n",
      "00:25:14.672206 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '黄', '圣', '依', '做', '蛋', '炒', '饭', '大'... '肉', ',', '安', '迪', ',', '蛋', '炒', '饭', '[SEP]']\n",
      "00:25:14.672411 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 15\n",
      "00:25:14.672611 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7942, 1760, 898, 976, 6028, 4143, 7649, 19...489, 117, 2128, 6832, 117, 6028, 4143, 7649, 102]\n",
      "00:25:14.672895 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7942, 1760,  898,  976, 6028, 4143...  117, 2128, 6832,  117, 6028, 4143, 7649,  102])\n",
      "00:25:14.673188 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.673798 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.674413 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.675886 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7942, 1760,  898,  976, 6028, 414... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009447\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 254\n",
      "00:25:14.678667 call        14     def __getitem__(self, idx):\n",
      "00:25:14.678767 line        15         if self.mode == \"test\":\n",
      "00:25:14.678810 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '宋茜发布会“内衣外穿”，网民：为什么要穿两件呢？搞不懂'\n",
      "New var:....... text_b = '内衣外穿,服装设计师,宋茜'\n",
      "New var:....... label = 0\n",
      "00:25:14.679582 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.679709 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.679943 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.680247 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['宋', '茜', '发', '布', '会', '[UNK]', '内', '衣', '外'...'么', '要', '穿', '两', '件', '呢', '？', '搞', '不', '懂']\n",
      "00:25:14.680983 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '宋', '茜', '发', '布', '会', '[UNK]', '内',... '穿', '两', '件', '呢', '？', '搞', '不', '懂', '[SEP]']\n",
      "00:25:14.681261 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:14.681461 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['内', '衣', '外', '穿', ',', '服', '装', '设', '计', '师', ',', '宋', '茜']\n",
      "00:25:14.681926 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '宋', '茜', '发', '布', '会', '[UNK]', '内',... '服', '装', '设', '计', '师', ',', '宋', '茜', '[SEP]']\n",
      "00:25:14.682132 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "00:25:14.682450 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2129, 5752, 1355, 2357, 833, 100, 1079, 61...02, 6163, 6392, 6369, 2360, 117, 2129, 5752, 102]\n",
      "00:25:14.682690 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2129, 5752, 1355, 2357,  833,  100...       6392, 6369, 2360,  117, 2129, 5752,  102])\n",
      "00:25:14.682919 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.683604 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.684235 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.685189 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2129, 5752, 1355, 2357,  833,  10... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008438\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 255\n",
      "00:25:14.687140 call        14     def __getitem__(self, idx):\n",
      "00:25:14.687217 line        15         if self.mode == \"test\":\n",
      "00:25:14.687258 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '林依晨老公背景太强大 网友：难怪不选郑元畅、胡歌'\n",
      "New var:....... text_b = '郑元畅,林于超,袁湘琴,林依晨,胡歌'\n",
      "New var:....... label = 0\n",
      "00:25:14.687873 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:14.687974 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:14.688185 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:14.688510 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['林', '依', '晨', '老', '公', '背', '景', '太', '强', '大...'难', '怪', '不', '选', '郑', '元', '畅', '、', '胡', '歌']\n",
      "00:25:14.689190 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '依', '晨', '老', '公', '背', '景', '太'... '不', '选', '郑', '元', '畅', '、', '胡', '歌', '[SEP]']\n",
      "00:25:14.689394 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:14.689587 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['郑', '元', '畅', ',', '林', '于', '超', ',', '袁', '湘', '琴', ',', '林', '依', '晨', ',', '胡', '歌']\n",
      "00:25:14.690301 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '依', '晨', '老', '公', '背', '景', '太'... '琴', ',', '林', '依', '晨', ',', '胡', '歌', '[SEP]']\n",
      "00:25:14.690582 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:25:14.699370 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3360, 898, 3247, 5439, 1062, 5520, 3250, 1...4433, 117, 3360, 898, 3247, 117, 5529, 3625, 102]\n",
      "00:25:14.699643 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3360,  898, 3247, 5439, 1062, 5520...  117, 3360,  898, 3247,  117, 5529, 3625,  102])\n",
      "00:25:14.699862 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:14.706092 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:14.708754 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:14.709561 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3360,  898, 3247, 5439, 1062, 552... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.024108\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 256\n",
      "00:25:24.182813 call        14     def __getitem__(self, idx):\n",
      "00:25:24.183095 line        15         if self.mode == \"test\":\n",
      "00:25:24.183180 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '韩国美食家在成都街头痛哭：不要收走我的饭！'\n",
      "New var:....... text_b = '陈晓卿,担担面,街头美食斗士,儿化音,丧心病狂,大排档'\n",
      "New var:....... label = 0\n",
      "00:25:24.184300 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.184516 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.184674 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.185433 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['韩', '国', '美', '食', '家', '在', '成', '都', '街', '头...'哭', '：', '不', '要', '收', '走', '我', '的', '饭', '！']\n",
      "00:25:24.186790 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '国', '美', '食', '家', '在', '成', '都'... '不', '要', '收', '走', '我', '的', '饭', '！', '[SEP]']\n",
      "00:25:24.188893 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:25:24.191268 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['陈', '晓', '卿', ',', '担', '担', '面', ',', '街', '头...'音', ',', '丧', '心', '病', '狂', ',', '大', '排', '档']\n",
      "00:25:24.193401 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '国', '美', '食', '家', '在', '成', '都'... '丧', '心', '病', '狂', ',', '大', '排', '档', '[SEP]']\n",
      "00:25:24.193878 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 28\n",
      "00:25:24.194599 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7506, 1744, 5401, 7608, 2157, 1762, 2768, ...00, 2552, 4567, 4312, 117, 1920, 2961, 3440, 102]\n",
      "00:25:24.195308 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7506, 1744, 5401, 7608, 2157, 1762...4567, 4312,  117, 1920,        2961, 3440,  102])\n",
      "00:25:24.196001 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.197420 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:25:24.198795 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.200604 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7506, 1744, 5401, 7608, 2157, 176...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.021504\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 257\n",
      "00:25:24.204373 call        14     def __getitem__(self, idx):\n",
      "00:25:24.204535 line        15         if self.mode == \"test\":\n",
      "00:25:24.204637 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '组图：英国著名演员艾米莉亚·克拉克，微笑现身纽约肯尼迪机场'\n",
      "New var:....... text_b = '权力的游戏,唐·海明威,终结者：创世纪,女演员,独立电影,星球大战韩·索罗外传,丹妮莉丝,东方IC,艾米莉亚·克拉克'\n",
      "New var:....... label = 0\n",
      "00:25:24.205808 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.206441 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.206633 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.207260 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['组', '图', '：', '英', '国', '著', '名', '演', '员', '艾...'笑', '现', '身', '纽', '约', '肯', '尼', '迪', '机', '场']\n",
      "00:25:24.208521 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '组', '图', '：', '英', '国', '著', '名', '演'... '身', '纽', '约', '肯', '尼', '迪', '机', '场', '[SEP]']\n",
      "00:25:24.209109 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:24.209454 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['权', '力', '的', '游', '戏', ',', '唐', '·', '海', '明...ic', ',', '艾', '米', '莉', '亚', '·', '克', '拉', '克']\n",
      "00:25:24.211778 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '组', '图', '：', '英', '国', '著', '名', '演'... '艾', '米', '莉', '亚', '·', '克', '拉', '克', '[SEP]']\n",
      "00:25:24.212493 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 58\n",
      "00:25:24.212978 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5299, 1745, 8038, 5739, 1744, 5865, 1399, ...687, 5101, 5799, 762, 185, 1046, 2861, 1046, 102]\n",
      "00:25:24.213538 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5299, 1745, 8038, 5739, 1744, 5865...5799,  762,         185, 1046, 2861, 1046,  102])\n",
      "00:25:24.214176 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.216392 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.219116 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.221813 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5299, 1745, 8038, 5739, 1744, 586... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.020904\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 258\n",
      "00:25:24.225326 call        14     def __getitem__(self, idx):\n",
      "00:25:24.225441 line        15         if self.mode == \"test\":\n",
      "00:25:24.225500 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '真正“民歌天后”董文华，沉寂十年再次出道，费玉清却这样评价她'\n",
      "New var:....... text_b = '费玉清,血染的风采,春天的故事,长城长,董文华'\n",
      "New var:....... label = 0\n",
      "00:25:24.226361 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.226520 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.226665 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.227111 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['真', '正', '[UNK]', '民', '歌', '天', '后', '[UNK]',...'，', '费', '玉', '清', '却', '这', '样', '评', '价', '她']\n",
      "00:25:24.228800 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '真', '正', '[UNK]', '民', '歌', '天', '后',... '玉', '清', '却', '这', '样', '评', '价', '她', '[SEP]']\n",
      "00:25:24.229214 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:24.229668 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['费', '玉', '清', ',', '血', '染', '的', '风', '采', ',...'故', '事', ',', '长', '城', '长', ',', '董', '文', '华']\n",
      "00:25:24.230976 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '真', '正', '[UNK]', '民', '歌', '天', '后',... ',', '长', '城', '长', ',', '董', '文', '华', '[SEP]']\n",
      "00:25:24.231418 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "00:25:24.231830 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4696, 3633, 100, 3696, 3625, 1921, 1400, 1...17, 7270, 1814, 7270, 117, 5869, 3152, 1290, 102]\n",
      "00:25:24.232216 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4696, 3633,  100, 3696, 3625, 1921... 7270, 1814, 7270,  117, 5869, 3152, 1290,  102])\n",
      "00:25:24.232795 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.235397 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.237761 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.240569 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4696, 3633,  100, 3696, 3625, 192...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.018416\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 259\n",
      "00:25:24.243804 call        14     def __getitem__(self, idx):\n",
      "00:25:24.243946 line        15         if self.mode == \"test\":\n",
      "00:25:24.244021 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '贾静雯发长文霸气护娃 修杰楷一字评论 然而网友们的注意点却在……'\n",
      "New var:....... text_b = '真人秀,妈妈是超人,霍思燕,修杰楷,繁体字,贾静雯'\n",
      "New var:....... label = 0\n",
      "00:25:24.245116 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.245304 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.246014 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.246682 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['贾', '静', '雯', '发', '长', '文', '霸', '气', '护', '娃..., '的', '注', '意', '点', '却', '在', '[UNK]', '[UNK]']\n",
      "00:25:24.248249 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '贾', '静', '雯', '发', '长', '文', '霸', '气'...', '意', '点', '却', '在', '[UNK]', '[UNK]', '[SEP]']\n",
      "00:25:24.249076 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:24.249780 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['真', '人', '秀', ',', '妈', '妈', '是', '超', '人', ',...'杰', '楷', ',', '繁', '体', '字', ',', '贾', '静', '雯']\n",
      "00:25:24.251300 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '贾', '静', '雯', '发', '长', '文', '霸', '气'... ',', '繁', '体', '字', ',', '贾', '静', '雯', '[SEP]']\n",
      "00:25:24.252074 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 26\n",
      "00:25:24.252558 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6593, 7474, 7435, 1355, 7270, 3152, 7464, ...117, 5246, 860, 2099, 117, 6593, 7474, 7435, 102]\n",
      "00:25:24.253001 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6593, 7474, 7435, 1355, 7270, 3152... 5246,  860, 2099,  117, 6593, 7474, 7435,  102])\n",
      "00:25:24.253443 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.255083 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.256764 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.259166 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6593, 7474, 7435, 1355, 7270, 315...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.017758\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 260\n",
      "00:25:24.261635 call        14     def __getitem__(self, idx):\n",
      "00:25:24.261783 line        15         if self.mode == \"test\":\n",
      "00:25:24.261869 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张桐曝光大变样，帅气沉稳尽显型男魅力，好帅'\n",
      "New var:....... text_b = '别样魅力,时尚杂志拍摄最新时尚,新晋视帝张桐受邀,最新时尚写真大片曝光,抗战'\n",
      "New var:....... label = 0\n",
      "00:25:24.262904 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.263107 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.263259 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.263937 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '桐', '曝', '光', '大', '变', '样', '，', '帅', '气...'稳', '尽', '显', '型', '男', '魅', '力', '，', '好', '帅']\n",
      "00:25:24.265018 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '桐', '曝', '光', '大', '变', '样', '，'... '显', '型', '男', '魅', '力', '，', '好', '帅', '[SEP]']\n",
      "00:25:24.265495 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:25:24.266387 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['别', '样', '魅', '力', ',', '时', '尚', '杂', '志', '拍...'尚', '写', '真', '大', '片', '曝', '光', ',', '抗', '战']\n",
      "00:25:24.268345 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '桐', '曝', '光', '大', '变', '样', '，'... '真', '大', '片', '曝', '光', ',', '抗', '战', '[SEP]']\n",
      "00:25:24.268939 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 39\n",
      "00:25:24.269357 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 3432, 3284, 1045, 1920, 1359, 3416, ...96, 1920, 4275, 3284, 1045, 117, 2834, 2773, 102]\n",
      "00:25:24.269980 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476, 3432, 3284, 1045, 1920, 1359...4275, 3284, 1045,  117, 2834,        2773,  102])\n",
      "00:25:24.270408 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.271936 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.273486 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.275869 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2476, 3432, 3284, 1045, 1920, 135... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.019201\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 261\n",
      "00:25:24.280892 call        14     def __getitem__(self, idx):\n",
      "00:25:24.281003 line        15         if self.mode == \"test\":\n",
      "00:25:24.281056 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '54岁影帝从小被父母嫌弃，与妻子19年恩爱如初，死后遗产全捐社会'\n",
      "New var:....... text_b = '周润发,大时代,我要成名,刘青云,郭蔼明'\n",
      "New var:....... label = 0\n",
      "00:25:24.281889 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.282051 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.282165 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.282927 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['54', '岁', '影', '帝', '从', '小', '被', '父', '母', '...'初', '，', '死', '后', '遗', '产', '全', '捐', '社', '会']\n",
      "00:25:24.284135 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '54', '岁', '影', '帝', '从', '小', '被', '父... '死', '后', '遗', '产', '全', '捐', '社', '会', '[SEP]']\n",
      "00:25:24.284678 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:24.285228 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['周', '润', '发', ',', '大', '时', '代', ',', '我', '要', '成', '名', ',', '刘', '青', '云', ',', '郭', '蔼', '明']\n",
      "00:25:24.286354 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '54', '岁', '影', '帝', '从', '小', '被', '父... ',', '刘', '青', '云', ',', '郭', '蔼', '明', '[SEP]']\n",
      "00:25:24.286965 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:25:24.287600 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 8267, 2259, 2512, 2370, 794, 2207, 6158, 4...117, 1155, 7471, 756, 117, 6958, 5928, 3209, 102]\n",
      "00:25:24.288101 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 8267, 2259, 2512, 2370,  794, 2207...7471,  756,         117, 6958, 5928, 3209,  102])\n",
      "00:25:24.288565 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.289932 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1])\n",
      "00:25:24.290969 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.293281 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 8267, 2259, 2512, 2370,  794, 220...1, 1, 1, 1, 1,        1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.017671\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 262\n",
      "00:25:24.298620 call        14     def __getitem__(self, idx):\n",
      "00:25:24.298740 line        15         if self.mode == \"test\":\n",
      "00:25:24.298833 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '韩瑜出席活动，网友：好了，不说了，我去健身了！'\n",
      "New var:....... text_b = '韩瑜,韩瑜出席活动'\n",
      "New var:....... label = 0\n",
      "00:25:24.299949 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.300363 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.300559 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.301072 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['韩', '瑜', '出', '席', '活', '动', '，', '网', '友', '：...'不', '说', '了', '，', '我', '去', '健', '身', '了', '！']\n",
      "00:25:24.302498 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '瑜', '出', '席', '活', '动', '，', '网'... '了', '，', '我', '去', '健', '身', '了', '！', '[SEP]']\n",
      "00:25:24.303173 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:24.303861 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['韩', '瑜', ',', '韩', '瑜', '出', '席', '活', '动']\n",
      "00:25:24.304634 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '瑜', '出', '席', '活', '动', '，', '网'... '瑜', ',', '韩', '瑜', '出', '席', '活', '动', '[SEP]']\n",
      "00:25:24.305435 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 10\n",
      "00:25:24.305669 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7506, 4447, 1139, 2375, 3833, 1220, 8024, ...47, 117, 7506, 4447, 1139, 2375, 3833, 1220, 102]\n",
      "00:25:24.305895 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7506, 4447, 1139, 2375, 3833, 1220...  117, 7506, 4447, 1139, 2375, 3833, 1220,  102])\n",
      "00:25:24.306320 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.307107 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0,        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.308344 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.309821 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7506, 4447, 1139, 2375, 3833, 122...    0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.015211\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 263\n",
      "00:25:24.314030 call        14     def __getitem__(self, idx):\n",
      "00:25:24.314170 line        15         if self.mode == \"test\":\n",
      "00:25:24.314231 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '女生跟你喝酒表示很开放？印度这部直击人性的电影把我看呆了'\n",
      "New var:....... text_b = '红粉惊魂,印度,塞加尔,女孩子,米纳尔,女孩安全手册,印度电影'\n",
      "New var:....... label = 0\n",
      "00:25:24.315246 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.315417 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.315590 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.316229 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['女', '生', '跟', '你', '喝', '酒', '表', '示', '很', '开...'人', '性', '的', '电', '影', '把', '我', '看', '呆', '了']\n",
      "00:25:24.317563 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '生', '跟', '你', '喝', '酒', '表', '示'... '的', '电', '影', '把', '我', '看', '呆', '了', '[SEP]']\n",
      "00:25:24.318001 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:25:24.318701 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['红', '粉', '惊', '魂', ',', '印', '度', ',', '塞', '加...'孩', '安', '全', '手', '册', ',', '印', '度', '电', '影']\n",
      "00:25:24.320013 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '生', '跟', '你', '喝', '酒', '表', '示'... '全', '手', '册', ',', '印', '度', '电', '影', '[SEP]']\n",
      "00:25:24.320494 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 32\n",
      "00:25:24.320863 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1957, 4495, 6656, 872, 1600, 6983, 6134, 4...59, 2797, 1085, 117, 1313, 2428, 4510, 2512, 102]\n",
      "00:25:24.321152 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1957, 4495, 6656,  872, 1600, 6983...1085,  117, 1313, 2428, 4510,        2512,  102])\n",
      "00:25:24.321513 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.322984 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.324256 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.326034 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1957, 4495, 6656,  872, 1600, 698... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.016862\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 264\n",
      "00:25:24.330956 call        14     def __getitem__(self, idx):\n",
      "00:25:24.331105 line        15         if self.mode == \"test\":\n",
      "00:25:24.331170 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '柔弱似水，外冷内热，纯情可人的钟欣潼'\n",
      "New var:....... text_b = '情剑,钟欣潼,浙江卫视,赏金猎人,内地电视剧,迫在眉睫'\n",
      "New var:....... label = 0\n",
      "00:25:24.332055 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.332271 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.332429 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.333086 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['柔', '弱', '似', '水', '，', '外', '冷', '内', '热', '，', '纯', '情', '可', '人', '的', '钟', '欣', '潼']\n",
      "00:25:24.333947 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '柔', '弱', '似', '水', '，', '外', '冷', '内'... '纯', '情', '可', '人', '的', '钟', '欣', '潼', '[SEP]']\n",
      "00:25:24.334370 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 20\n",
      "00:25:24.334752 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['情', '剑', ',', '钟', '欣', '潼', ',', '浙', '江', '卫...'内', '地', '电', '视', '剧', ',', '迫', '在', '眉', '睫']\n",
      "00:25:24.336099 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '柔', '弱', '似', '水', '，', '外', '冷', '内'... '电', '视', '剧', ',', '迫', '在', '眉', '睫', '[SEP]']\n",
      "00:25:24.336845 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 28\n",
      "00:25:24.337471 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3382, 2483, 849, 3717, 8024, 1912, 1107, 1...10, 6228, 1196, 117, 6833, 1762, 4691, 4724, 102]\n",
      "00:25:24.337893 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3382, 2483,  849, 3717, 8024, 1912... 6228, 1196,  117, 6833, 1762, 4691, 4724,  102])\n",
      "00:25:24.338239 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.339235 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.340717 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.342920 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3382, 2483,  849, 3717, 8024, 191... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.015411\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 265\n",
      "00:25:24.346404 call        14     def __getitem__(self, idx):\n",
      "00:25:24.346499 line        15         if self.mode == \"test\":\n",
      "00:25:24.346549 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '神医喜来乐传奇-喜来乐治好土匪女儿的病，被逼着娶土匪的女儿'\n",
      "New var:....... text_b = '神医喜来乐传奇,喜来乐,神医喜来乐,土匪'\n",
      "New var:....... label = 0\n",
      "00:25:24.347241 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.347491 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.347610 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.347954 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['神', '医', '喜', '来', '乐', '传', '奇', '-', '喜', '来...'，', '被', '逼', '着', '娶', '土', '匪', '的', '女', '儿']\n",
      "00:25:24.348955 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '神', '医', '喜', '来', '乐', '传', '奇', '-'... '逼', '着', '娶', '土', '匪', '的', '女', '儿', '[SEP]']\n",
      "00:25:24.349117 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:24.349261 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['神', '医', '喜', '来', '乐', '传', '奇', ',', '喜', '来', '乐', ',', '神', '医', '喜', '来', '乐', ',', '土', '匪']\n",
      "00:25:24.349838 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '神', '医', '喜', '来', '乐', '传', '奇', '-'... '神', '医', '喜', '来', '乐', ',', '土', '匪', '[SEP]']\n",
      "00:25:24.350108 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:25:24.350269 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4868, 1278, 1599, 3341, 727, 837, 1936, 11...868, 1278, 1599, 3341, 727, 117, 1759, 1272, 102]\n",
      "00:25:24.350441 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4868, 1278, 1599, 3341,  727,  837...1599, 3341,  727,         117, 1759, 1272,  102])\n",
      "00:25:24.350615 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.351242 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:25:24.352329 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.353639 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4868, 1278, 1599, 3341,  727,  83...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010326\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 266\n",
      "00:25:24.356801 call        14     def __getitem__(self, idx):\n",
      "00:25:24.356897 line        15         if self.mode == \"test\":\n",
      "00:25:24.356944 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '必看的鬼才导演昆汀高分恶趣味神作！'\n",
      "New var:....... text_b = '无耻混蛋,昆汀,迪亚戈,赏金猎人,马奎斯,罪恶之城,恶趣味'\n",
      "New var:....... label = 0\n",
      "00:25:24.357643 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.357893 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.358136 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.358537 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['必', '看', '的', '鬼', '才', '导', '演', '昆', '汀', '高', '分', '恶', '趣', '味', '神', '作', '！']\n",
      "00:25:24.359174 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '必', '看', '的', '鬼', '才', '导', '演', '昆'... '高', '分', '恶', '趣', '味', '神', '作', '！', '[SEP]']\n",
      "00:25:24.359530 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 19\n",
      "00:25:24.359759 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['无', '耻', '混', '蛋', ',', '昆', '汀', ',', '迪', '亚...'斯', ',', '罪', '恶', '之', '城', ',', '恶', '趣', '味']\n",
      "00:25:24.360798 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '必', '看', '的', '鬼', '才', '导', '演', '昆'... '罪', '恶', '之', '城', ',', '恶', '趣', '味', '[SEP]']\n",
      "00:25:24.361344 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 30\n",
      "00:25:24.361616 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2553, 4692, 4638, 7787, 2798, 2193, 4028, ...389, 2626, 722, 1814, 117, 2626, 6637, 1456, 102]\n",
      "00:25:24.362391 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2553, 4692, 4638, 7787, 2798, 2193... 722, 1814,  117, 2626, 6637, 1456,         102])\n",
      "00:25:24.362899 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.363802 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:25:24.364707 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.365987 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2553, 4692, 4638, 7787, 2798, 219...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011582\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 267\n",
      "00:25:24.368415 call        14     def __getitem__(self, idx):\n",
      "00:25:24.368499 line        15         if self.mode == \"test\":\n",
      "00:25:24.368544 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '爸爸给女儿买衣服图便宜买了一身奶奶装，后妈站在一旁都看不下去'\n",
      "New var:....... text_b = '后妈'\n",
      "New var:....... label = 0\n",
      "00:25:24.369250 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.369520 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.369634 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.370023 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['爸', '爸', '给', '女', '儿', '买', '衣', '服', '图', '便...'妈', '站', '在', '一', '旁', '都', '看', '不', '下', '去']\n",
      "00:25:24.371015 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '爸', '爸', '给', '女', '儿', '买', '衣', '服'... '在', '一', '旁', '都', '看', '不', '下', '去', '[SEP]']\n",
      "00:25:24.371270 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:24.371594 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['后', '妈']\n",
      "00:25:24.371929 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '爸', '爸', '给', '女', '儿', '买', '衣', '服'..., '看', '不', '下', '去', '[SEP]', '后', '妈', '[SEP]']\n",
      "00:25:24.372165 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 3\n",
      "00:25:24.372437 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4268, 4268, 5314, 1957, 1036, 743, 6132, 3...6963, 4692, 679, 678, 1343, 102, 1400, 1968, 102]\n",
      "00:25:24.372673 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4268, 4268, 5314, 1957, 1036,  743... 4692,  679,  678, 1343,  102, 1400, 1968,  102])\n",
      "00:25:24.372921 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.373523 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0,        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "00:25:24.374185 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.375161 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4268, 4268, 5314, 1957, 1036,  74...    0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008966\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 268\n",
      "00:25:24.377423 call        14     def __getitem__(self, idx):\n",
      "00:25:24.377514 line        15         if self.mode == \"test\":\n",
      "00:25:24.377560 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '人气演员加老戏骨 能否让现实主义剧回暖？'\n",
      "New var:....... text_b = '黎明之前,温州一家人,老农民,远大前程,婚姻保卫战,钢铁年代,蔡炳坤,真爱的谎言之破冰者,归去来,赵氏孤儿,罗晋,老戏骨,张晨光,老男孩,高满堂'\n",
      "New var:....... label = 0\n",
      "00:25:24.378190 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.378302 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.378395 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.378613 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['人', '气', '演', '员', '加', '老', '戏', '骨', '能', '否', '让', '现', '实', '主', '义', '剧', '回', '暖', '？']\n",
      "00:25:24.379314 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '人', '气', '演', '员', '加', '老', '戏', '骨'... '现', '实', '主', '义', '剧', '回', '暖', '？', '[SEP]']\n",
      "00:25:24.379611 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:25:24.379825 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['黎', '明', '之', '前', ',', '温', '州', '一', '家', '人...'晨', '光', ',', '老', '男', '孩', ',', '高', '满', '堂']\n",
      "00:25:24.381529 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '人', '气', '演', '员', '加', '老', '戏', '骨'... ',', '老', '男', '孩', ',', '高', '满', '堂', '[SEP]']\n",
      "00:25:24.381894 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 73\n",
      "00:25:24.382132 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 782, 3698, 4028, 1447, 1217, 5439, 2767, 7...17, 5439, 4511, 2111, 117, 7770, 4007, 1828, 102]\n",
      "00:25:24.382398 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  782, 3698, 4028, 1447, 1217, 5439... 5439, 4511, 2111,  117, 7770, 4007, 1828,  102])\n",
      "00:25:24.382666 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.383704 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.385592 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.397541 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  782, 3698, 4028, 1447, 1217, 543... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.030726\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 269\n",
      "00:25:24.408197 call        14     def __getitem__(self, idx):\n",
      "00:25:24.408295 line        15         if self.mode == \"test\":\n",
      "00:25:24.408343 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '主题是足球？《奔跑吧》录制名牌曝光 明星阵容惹期待'\n",
      "New var:....... text_b = '王祖蓝,范志毅,奔跑吧,张云龙,足球,杨幂'\n",
      "New var:....... label = 0\n",
      "00:25:24.409048 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.409175 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.409282 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.409437 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['主', '题', '是', '足', '球', '？', '《', '奔', '跑', '吧...'牌', '曝', '光', '明', '星', '阵', '容', '惹', '期', '待']\n",
      "00:25:24.410160 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '主', '题', '是', '足', '球', '？', '《', '奔'... '光', '明', '星', '阵', '容', '惹', '期', '待', '[SEP]']\n",
      "00:25:24.410718 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:24.410905 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['王', '祖', '蓝', ',', '范', '志', '毅', ',', '奔', '跑...',', '张', '云', '龙', ',', '足', '球', ',', '杨', '幂']\n",
      "00:25:24.411782 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '主', '题', '是', '足', '球', '？', '《', '奔'... '云', '龙', ',', '足', '球', ',', '杨', '幂', '[SEP]']\n",
      "00:25:24.412676 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 22\n",
      "00:25:24.413174 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 712, 7579, 3221, 6639, 4413, 8043, 517, 19...756, 7987, 117, 6639, 4413, 117, 3342, 2386, 102]\n",
      "00:25:24.413505 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  712, 7579, 3221, 6639, 4413, 8043... 7987,  117, 6639, 4413,  117, 3342, 2386,  102])\n",
      "00:25:24.414166 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.415730 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.416761 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.419312 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  712, 7579, 3221, 6639, 4413, 804... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013463\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 270\n",
      "00:25:24.421705 call        14     def __getitem__(self, idx):\n",
      "00:25:24.421798 line        15         if self.mode == \"test\":\n",
      "00:25:24.421845 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '谢娜晒出双胞胎百天照，小天使们慢慢在长大'\n",
      "New var:....... text_b = '综艺节目,谢娜,小天使,娱乐圈,张杰,快乐大本营'\n",
      "New var:....... label = 0\n",
      "00:25:24.422527 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.422642 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.422867 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.423086 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['谢', '娜', '晒', '出', '双', '胞', '胎', '百', '天', '照', '，', '小', '天', '使', '们', '慢', '慢', '在', '长', '大']\n",
      "00:25:24.423757 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '谢', '娜', '晒', '出', '双', '胞', '胎', '百'... '天', '使', '们', '慢', '慢', '在', '长', '大', '[SEP]']\n",
      "00:25:24.424050 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 22\n",
      "00:25:24.424265 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['综', '艺', '节', '目', ',', '谢', '娜', ',', '小', '天...'圈', ',', '张', '杰', ',', '快', '乐', '大', '本', '营']\n",
      "00:25:24.425056 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '谢', '娜', '晒', '出', '双', '胞', '胎', '百'... '张', '杰', ',', '快', '乐', '大', '本', '营', '[SEP]']\n",
      "00:25:24.425343 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 25\n",
      "00:25:24.426085 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6468, 2025, 3235, 1139, 1352, 5528, 5522, ...476, 3345, 117, 2571, 727, 1920, 3315, 5852, 102]\n",
      "00:25:24.426867 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6468, 2025, 3235, 1139, 1352, 5528... 3345,  117, 2571,  727, 1920, 3315, 5852,  102])\n",
      "00:25:24.427105 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.427866 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.429176 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.430260 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6468, 2025, 3235, 1139, 1352, 552... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011310\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 271\n",
      "00:25:24.433056 call        14     def __getitem__(self, idx):\n",
      "00:25:24.433142 line        15         if self.mode == \"test\":\n",
      "00:25:24.433188 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '韩星赵容弼50周年演唱会 将由SEVENTEEN开场'\n",
      "New var:....... text_b = '演唱会,SEVENTEEN,韩星,不朽的名曲,赵容弼'\n",
      "New var:....... label = 0\n",
      "00:25:24.433858 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.433976 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.434213 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.434604 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['韩', '星', '赵', '容', '弼', '50', '周', '年', '演', '...'会', '将', '由', 'seven', '##tee', '##n', '开', '场']\n",
      "00:25:24.435294 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '星', '赵', '容', '弼', '50', '周', '年... '由', 'seven', '##tee', '##n', '开', '场', '[SEP]']\n",
      "00:25:24.435520 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 20\n",
      "00:25:24.435734 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['演', '唱', '会', ',', 'seven', '##tee', '##n', ',...',', '不', '朽', '的', '名', '曲', ',', '赵', '容', '弼']\n",
      "00:25:24.436414 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '星', '赵', '容', '弼', '50', '周', '年... '朽', '的', '名', '曲', ',', '赵', '容', '弼', '[SEP]']\n",
      "00:25:24.437212 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:25:24.437559 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7506, 3215, 6627, 2159, 2488, 8145, 1453, ...23, 4638, 1399, 3289, 117, 6627, 2159, 2488, 102]\n",
      "00:25:24.438156 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  7506,  3215,  6627,  2159,  2488... 3289,   117,  6627,  2159,  2488,          102])\n",
      "00:25:24.439343 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.440681 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.441791 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.443688 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  7506,  3215,  6627,  2159,  248... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.015562\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 272\n",
      "00:25:24.448765 call        14     def __getitem__(self, idx):\n",
      "00:25:24.449249 line        15         if self.mode == \"test\":\n",
      "00:25:24.449336 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '李诞因上综艺太懒被骂惨，但看到他女友时，众人都说他好幸福'\n",
      "New var:....... text_b = '个人主页,张绍刚,李诞,黑尾酱,刘宪华'\n",
      "New var:....... label = 0\n",
      "00:25:24.450614 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.451676 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.452260 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.453379 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['李', '诞', '因', '上', '综', '艺', '太', '懒', '被', '骂...'时', '，', '众', '人', '都', '说', '他', '好', '幸', '福']\n",
      "00:25:24.455743 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '诞', '因', '上', '综', '艺', '太', '懒'... '众', '人', '都', '说', '他', '好', '幸', '福', '[SEP]']\n",
      "00:25:24.457594 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:25:24.458402 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['个', '人', '主', '页', ',', '张', '绍', '刚', ',', '李', '诞', ',', '黑', '尾', '酱', ',', '刘', '宪', '华']\n",
      "00:25:24.461489 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '诞', '因', '上', '综', '艺', '太', '懒'... ',', '黑', '尾', '酱', ',', '刘', '宪', '华', '[SEP]']\n",
      "00:25:24.463597 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:24.464101 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3330, 6414, 1728, 677, 5341, 5686, 1922, 2...17, 7946, 2227, 6996, 117, 1155, 2150, 1290, 102]\n",
      "00:25:24.464486 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3330, 6414, 1728,  677, 5341, 5686...2227, 6996,  117, 1155, 2150,        1290,  102])\n",
      "00:25:24.465789 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.466983 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:25:24.468062 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.469115 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3330, 6414, 1728,  677, 5341, 568...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.022748\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 273\n",
      "00:25:24.471552 call        14     def __getitem__(self, idx):\n",
      "00:25:24.471638 line        15         if self.mode == \"test\":\n",
      "00:25:24.471683 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '两部《唐探》结局秦风都放走了真正的凶手，陈思诚这样安排的用意'\n",
      "New var:....... text_b = '唐人街探案,电影,秦风,陈思诚,杀人案'\n",
      "New var:....... label = 0\n",
      "00:25:24.472391 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.472550 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.472659 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.473088 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['两', '部', '《', '唐', '探', '》', '结', '局', '秦', '风...'陈', '思', '诚', '这', '样', '安', '排', '的', '用', '意']\n",
      "00:25:24.474013 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '两', '部', '《', '唐', '探', '》', '结', '局'... '诚', '这', '样', '安', '排', '的', '用', '意', '[SEP]']\n",
      "00:25:24.474619 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:24.474971 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['唐', '人', '街', '探', '案', ',', '电', '影', ',', '秦', '风', ',', '陈', '思', '诚', ',', '杀', '人', '案']\n",
      "00:25:24.475849 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '两', '部', '《', '唐', '探', '》', '结', '局'... ',', '陈', '思', '诚', ',', '杀', '人', '案', '[SEP]']\n",
      "00:25:24.476179 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:24.476480 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 697, 6956, 517, 1538, 2968, 518, 5310, 222...117, 7357, 2590, 6411, 117, 3324, 782, 3428, 102]\n",
      "00:25:24.477088 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  697, 6956,  517, 1538, 2968,  518...2590, 6411,  117,        3324,  782, 3428,  102])\n",
      "00:25:24.477450 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.479098 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:25:24.480046 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.482252 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  697, 6956,  517, 1538, 2968,  51...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013364\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 274\n",
      "00:25:24.484958 call        14     def __getitem__(self, idx):\n",
      "00:25:24.485140 line        15         if self.mode == \"test\":\n",
      "00:25:24.485203 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '安以轩晒美照“天亮说晚安” 皮肤细腻红唇美甲送香吻'\n",
      "New var:....... text_b = '安以轩,美甲,晚安吻,天亮说晚安'\n",
      "New var:....... label = 0\n",
      "00:25:24.485899 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.486027 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.486157 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.486703 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['安', '以', '轩', '晒', '美', '照', '[UNK]', '天', '亮'...'肤', '细', '腻', '红', '唇', '美', '甲', '送', '香', '吻']\n",
      "00:25:24.487416 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '安', '以', '轩', '晒', '美', '照', '[UNK]',... '腻', '红', '唇', '美', '甲', '送', '香', '吻', '[SEP]']\n",
      "00:25:24.487760 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:24.488292 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['安', '以', '轩', ',', '美', '甲', ',', '晚', '安', '吻', ',', '天', '亮', '说', '晚', '安']\n",
      "00:25:24.488973 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '安', '以', '轩', '晒', '美', '照', '[UNK]',... '安', '吻', ',', '天', '亮', '说', '晚', '安', '[SEP]']\n",
      "00:25:24.489163 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 17\n",
      "00:25:24.490258 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2128, 809, 6759, 3235, 5401, 4212, 100, 19...128, 1431, 117, 1921, 778, 6432, 3241, 2128, 102]\n",
      "00:25:24.490700 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2128,  809, 6759, 3235, 5401, 4212...        117, 1921,  778, 6432, 3241, 2128,  102])\n",
      "00:25:24.491009 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.492458 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.493309 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.496229 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2128,  809, 6759, 3235, 5401, 421... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013963\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 275\n",
      "00:25:24.498967 call        14     def __getitem__(self, idx):\n",
      "00:25:24.499137 line        15         if self.mode == \"test\":\n",
      "00:25:24.499454 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '演过罗成，演过秦琼，演过薛仁贵跟薛丁山，堪称真正的隋唐专业户'\n",
      "New var:....... text_b = '薛丁山,聂远,黄海冰,三国,薛仁贵,隋唐英雄,秦琼,隋唐英雄传,水浒,隋唐演义,隋唐英雄345,隋唐'\n",
      "New var:....... label = 0\n",
      "00:25:24.500420 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.500966 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.501514 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.501832 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['演', '过', '罗', '成', '，', '演', '过', '秦', '琼', '，...'堪', '称', '真', '正', '的', '隋', '唐', '专', '业', '户']\n",
      "00:25:24.502833 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '演', '过', '罗', '成', '，', '演', '过', '秦'... '真', '正', '的', '隋', '唐', '专', '业', '户', '[SEP]']\n",
      "00:25:24.503538 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:24.503706 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['薛', '丁', '山', ',', '聂', '远', ',', '黄', '海', '冰...', ',', '隋', '唐', '英', '雄', '345', ',', '隋', '唐']\n",
      "00:25:24.505203 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '演', '过', '罗', '成', '，', '演', '过', '秦'...隋', '唐', '英', '雄', '345', ',', '隋', '唐', '[SEP]']\n",
      "00:25:24.506280 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 49\n",
      "00:25:24.507322 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4028, 6814, 5384, 2768, 8024, 4028, 6814, ...7, 1538, 5739, 7413, 11434, 117, 7387, 1538, 102]\n",
      "00:25:24.507833 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  4028,  6814,  5384,  2768,  8024... 7413, 11434,   117,  7387,  1538,          102])\n",
      "00:25:24.508879 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.510975 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.512401 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.514788 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  4028,  6814,  5384,  2768,  802...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.020000\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 276\n",
      "00:25:24.519264 call        14     def __getitem__(self, idx):\n",
      "00:25:24.520120 line        15         if self.mode == \"test\":\n",
      "00:25:24.520213 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '安徽名人淮南歌唱家张旭唱响北京国家大剧院'\n",
      "New var:....... text_b = '五四青年节,国家大剧院,青春修炼手册,音乐会,流行歌曲,龙的传人,歌唱祖国,绒花,保卫黄河,马克思,游击队歌,革命人永远是年轻'\n",
      "New var:....... label = 0\n",
      "00:25:24.521301 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.522587 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.523108 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.524375 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['安', '徽', '名', '人', '淮', '南', '歌', '唱', '家', '张', '旭', '唱', '响', '北', '京', '国', '家', '大', '剧', '院']\n",
      "00:25:24.526526 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '安', '徽', '名', '人', '淮', '南', '歌', '唱'... '响', '北', '京', '国', '家', '大', '剧', '院', '[SEP]']\n",
      "00:25:24.527538 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 22\n",
      "00:25:24.527788 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['五', '四', '青', '年', '节', ',', '国', '家', '大', '剧...'歌', ',', '革', '命', '人', '永', '远', '是', '年', '轻']\n",
      "00:25:24.531037 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '安', '徽', '名', '人', '淮', '南', '歌', '唱'... '革', '命', '人', '永', '远', '是', '年', '轻', '[SEP]']\n",
      "00:25:24.531754 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 64\n",
      "00:25:24.532831 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2128, 2551, 1399, 782, 3917, 1298, 3625, 1...84, 1462, 782, 3719, 6823, 3221, 2399, 6768, 102]\n",
      "00:25:24.533822 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2128, 2551, 1399,  782, 3917, 1298... 782, 3719, 6823, 3221, 2399,        6768,  102])\n",
      "00:25:24.534573 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.537851 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.541230 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.545088 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2128, 2551, 1399,  782, 3917, 129... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.042515\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 277\n",
      "00:25:24.561855 call        14     def __getitem__(self, idx):\n",
      "00:25:24.562836 line        15         if self.mode == \"test\":\n",
      "00:25:24.564358 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '央视著名主持人康辉 慰问演出主持全程无手稿'\n",
      "New var:....... text_b = '康辉,中央电视台新闻频道,中央电视台,主持人,节目主持'\n",
      "New var:....... label = 0\n",
      "00:25:24.575967 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.581489 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.584424 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.587281 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['央', '视', '著', '名', '主', '持', '人', '康', '辉', '慰', '问', '演', '出', '主', '持', '全', '程', '无', '手', '稿']\n",
      "00:25:24.589722 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '央', '视', '著', '名', '主', '持', '人', '康'... '出', '主', '持', '全', '程', '无', '手', '稿', '[SEP]']\n",
      "00:25:24.594414 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 22\n",
      "00:25:24.595676 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['康', '辉', ',', '中', '央', '电', '视', '台', '新', '闻...'台', ',', '主', '持', '人', ',', '节', '目', '主', '持']\n",
      "00:25:24.597881 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '央', '视', '著', '名', '主', '持', '人', '康'... '主', '持', '人', ',', '节', '目', '主', '持', '[SEP]']\n",
      "00:25:24.601135 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 28\n",
      "00:25:24.601598 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1925, 6228, 5865, 1399, 712, 2898, 782, 24... 712, 2898, 782, 117, 5688, 4680, 712, 2898, 102]\n",
      "00:25:24.602242 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1925, 6228, 5865, 1399,  712, 2898... 782,  117, 5688, 4680,  712,        2898,  102])\n",
      "00:25:24.603290 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.604935 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:25:24.606735 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.608514 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1925, 6228, 5865, 1399,  712, 289...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.049493\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 278\n",
      "00:25:24.611422 call        14     def __getitem__(self, idx):\n",
      "00:25:24.611570 line        15         if self.mode == \"test\":\n",
      "00:25:24.611672 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '破百万福利到底应该送粉丝什么？'\n",
      "New var:....... text_b = '陈泓宇,以父之名,偶像,偶像练习生,粉丝福利,蔡徐坤,粉丝'\n",
      "New var:....... label = 0\n",
      "00:25:24.612680 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.617056 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.618941 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.619513 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['破', '百', '万', '福', '利', '到', '底', '应', '该', '送', '粉', '丝', '什', '么', '？']\n",
      "00:25:24.620491 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '破', '百', '万', '福', '利', '到', '底', '应', '该', '送', '粉', '丝', '什', '么', '？', '[SEP]']\n",
      "00:25:24.623436 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 17\n",
      "00:25:24.623848 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['陈', '泓', '宇', ',', '以', '父', '之', '名', ',', '偶...'丝', '福', '利', ',', '蔡', '徐', '坤', ',', '粉', '丝']\n",
      "00:25:24.627622 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '破', '百', '万', '福', '利', '到', '底', '应'... '利', ',', '蔡', '徐', '坤', ',', '粉', '丝', '[SEP]']\n",
      "00:25:24.630164 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 30\n",
      "00:25:24.632010 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4788, 4636, 674, 4886, 1164, 1168, 2419, 2...1164, 117, 5918, 2528, 1787, 117, 5106, 692, 102]\n",
      "00:25:24.632446 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4788, 4636,  674, 4886, 1164, 1168...  117, 5918, 2528, 1787,  117, 5106,  692,  102])\n",
      "00:25:24.633245 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.634216 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.635510 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.637195 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4788, 4636,  674, 4886, 1164, 116... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.029118\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 279\n",
      "00:25:24.640601 call        14     def __getitem__(self, idx):\n",
      "00:25:24.641021 line        15         if self.mode == \"test\":\n",
      "00:25:24.641180 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《奔跑吧》收视率一路狂跌，总导演发博求助，网友：有她可逆转！'\n",
      "New var:....... text_b = '总导演,奔跑吧,第六季'\n",
      "New var:....... label = 0\n",
      "00:25:24.642063 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.642955 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.643455 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.649041 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '奔', '跑', '吧', '》', '收', '视', '率', '一', '路...'，', '网', '友', '：', '有', '她', '可', '逆', '转', '！']\n",
      "00:25:24.651476 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '奔', '跑', '吧', '》', '收', '视', '率'... '友', '：', '有', '她', '可', '逆', '转', '！', '[SEP]']\n",
      "00:25:24.652717 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:24.653554 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['总', '导', '演', ',', '奔', '跑', '吧', ',', '第', '六', '季']\n",
      "00:25:24.657230 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '奔', '跑', '吧', '》', '收', '视', '率'... ',', '奔', '跑', '吧', ',', '第', '六', '季', '[SEP]']\n",
      "00:25:24.658206 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 12\n",
      "00:25:24.664612 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 1944, 6651, 1416, 518, 3119, 6228, 43...17, 1944, 6651, 1416, 117, 5018, 1063, 2108, 102]\n",
      "00:25:24.670024 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 1944, 6651, 1416,  518, 3119... 1944, 6651, 1416,  117, 5018, 1063, 2108,  102])\n",
      "00:25:24.671111 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.672071 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.673483 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.674929 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 1944, 6651, 1416,  518, 311... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.037360\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 280\n",
      "00:25:24.678045 call        14     def __getitem__(self, idx):\n",
      "00:25:24.678387 line        15         if self.mode == \"test\":\n",
      "00:25:24.678628 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '他是刘嘉玲的初恋，最帅气版“李世民”，复出拍戏已满头白发！'\n",
      "New var:....... text_b = '娱乐圈,笑傲江湖,唐太宗李世民,林俊贤,飞越迷情,天神剑女,法中情,刘嘉玲,临歧,上海大风暴,无线电视艺员训练班,男儿本色,傅艺伟,最佳嫌疑人,捉鬼专门店'\n",
      "New var:....... label = 0\n",
      "00:25:24.679645 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.680015 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.680233 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.681392 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['他', '是', '刘', '嘉', '玲', '的', '初', '恋', '，', '最...'复', '出', '拍', '戏', '已', '满', '头', '白', '发', '！']\n",
      "00:25:24.682922 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '他', '是', '刘', '嘉', '玲', '的', '初', '恋'... '拍', '戏', '已', '满', '头', '白', '发', '！', '[SEP]']\n",
      "00:25:24.684115 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:24.684414 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['娱', '乐', '圈', ',', '笑', '傲', '江', '湖', ',', '唐...'佳', '嫌', '疑', '人', ',', '捉', '鬼', '专', '门', '店']\n",
      "00:25:24.686867 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '他', '是', '刘', '嘉', '玲', '的', '初', '恋'... '疑', '人', ',', '捉', '鬼', '专', '门', '店', '[SEP]']\n",
      "00:25:24.687449 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 78\n",
      "00:25:24.687818 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 800, 3221, 1155, 1649, 4386, 4638, 1159, 2...4542, 782, 117, 2929, 7787, 683, 7305, 2421, 102]\n",
      "00:25:24.688185 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  800, 3221, 1155, 1649, 4386, 4638... 117, 2929, 7787,  683, 7305, 2421,         102])\n",
      "00:25:24.689202 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.691228 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.694953 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.699630 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  800, 3221, 1155, 1649, 4386, 463... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.035272\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 281\n",
      "00:25:24.713390 call        14     def __getitem__(self, idx):\n",
      "00:25:24.714018 line        15         if self.mode == \"test\":\n",
      "00:25:24.714173 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '25年前疯狂追星父亲为其跳海身亡，如今竟表示还想再见刘德华！'\n",
      "New var:....... text_b = '刘德华,刘雨欣,杨丽娟,想再见,演唱会'\n",
      "New var:....... label = 0\n",
      "00:25:24.715231 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.715808 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.715984 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.716728 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['25', '年', '前', '疯', '狂', '追', '星', '父', '亲', '...'表', '示', '还', '想', '再', '见', '刘', '德', '华', '！']\n",
      "00:25:24.717845 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '25', '年', '前', '疯', '狂', '追', '星', '父... '还', '想', '再', '见', '刘', '德', '华', '！', '[SEP]']\n",
      "00:25:24.718493 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:24.718961 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['刘', '德', '华', ',', '刘', '雨', '欣', ',', '杨', '丽', '娟', ',', '想', '再', '见', ',', '演', '唱', '会']\n",
      "00:25:24.719630 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '25', '年', '前', '疯', '狂', '追', '星', '父... ',', '想', '再', '见', ',', '演', '唱', '会', '[SEP]']\n",
      "00:25:24.719849 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:24.720342 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 8132, 2399, 1184, 4556, 4312, 6841, 3215, ...117, 2682, 1086, 6224, 117, 4028, 1548, 833, 102]\n",
      "00:25:24.720868 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 8132, 2399, 1184, 4556, 4312, 6841...1086, 6224,  117, 4028,        1548,  833,  102])\n",
      "00:25:24.721260 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.722144 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:25:24.723918 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.725846 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 8132, 2399, 1184, 4556, 4312, 684...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.015691\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 282\n",
      "00:25:24.729117 call        14     def __getitem__(self, idx):\n",
      "00:25:24.729208 line        15         if self.mode == \"test\":\n",
      "00:25:24.729253 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '妃子大婚豪礼不少，皇后吃醋'\n",
      "New var:....... text_b = '吃醋,皇后,妃子'\n",
      "New var:....... label = 0\n",
      "00:25:24.730082 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.730411 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.730527 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.731219 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['妃', '子', '大', '婚', '豪', '礼', '不', '少', '，', '皇', '后', '吃', '醋']\n",
      "00:25:24.731823 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '妃', '子', '大', '婚', '豪', '礼', '不', '少', '，', '皇', '后', '吃', '醋', '[SEP]']\n",
      "00:25:24.732150 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "00:25:24.732480 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['吃', '醋', ',', '皇', '后', ',', '妃', '子']\n",
      "00:25:24.732969 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '妃', '子', '大', '婚', '豪', '礼', '不', '少'... '吃', '醋', ',', '皇', '后', ',', '妃', '子', '[SEP]']\n",
      "00:25:24.733456 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 9\n",
      "00:25:24.733623 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1964, 2094, 1920, 2042, 6498, 4851, 679, 2...391, 7005, 117, 4640, 1400, 117, 1964, 2094, 102]\n",
      "00:25:24.733862 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1964, 2094, 1920, 2042, 6498, 4851... 7005,  117, 4640, 1400,  117, 1964, 2094,  102])\n",
      "00:25:24.734276 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.735154 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.736155 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.738531 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1964, 2094, 1920, 2042, 6498, 485... 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.012245\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 283\n",
      "00:25:24.741520 call        14     def __getitem__(self, idx):\n",
      "00:25:24.741891 line        15         if self.mode == \"test\":\n",
      "00:25:24.741957 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '新月格格：雁姬在努达海面前暴打新月，努达海对她彻底没耐心了'\n",
      "New var:....... text_b = '新月格格,雁姬,努达海'\n",
      "New var:....... label = 0\n",
      "00:25:24.742648 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.742768 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.742865 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.743217 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['新', '月', '格', '格', '：', '雁', '姬', '在', '努', '达...'达', '海', '对', '她', '彻', '底', '没', '耐', '心', '了']\n",
      "00:25:24.744220 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '新', '月', '格', '格', '：', '雁', '姬', '在'... '对', '她', '彻', '底', '没', '耐', '心', '了', '[SEP]']\n",
      "00:25:24.744789 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:24.745616 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['新', '月', '格', '格', ',', '雁', '姬', ',', '努', '达', '海']\n",
      "00:25:24.746751 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '新', '月', '格', '格', '：', '雁', '姬', '在'... '格', ',', '雁', '姬', ',', '努', '达', '海', '[SEP]']\n",
      "00:25:24.747358 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 12\n",
      "00:25:24.747937 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3173, 3299, 3419, 3419, 8038, 7412, 2010, ...419, 117, 7412, 2010, 117, 1222, 6809, 3862, 102]\n",
      "00:25:24.748282 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3173, 3299, 3419, 3419, 8038, 7412...       7412, 2010,  117, 1222, 6809, 3862,  102])\n",
      "00:25:24.749014 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.749654 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.750510 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.751450 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3173, 3299, 3419, 3419, 8038, 741... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011852\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 284\n",
      "00:25:24.753489 call        14     def __getitem__(self, idx):\n",
      "00:25:24.753682 line        15         if self.mode == \"test\":\n",
      "00:25:24.753762 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '“北京女子图鉴”热播！“人鱼妆”大火后戚薇挑战拉伸式问答'\n",
      "New var:....... text_b = '北京,戚哥,戚薇'\n",
      "New var:....... label = 0\n",
      "00:25:24.754856 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.755233 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.755370 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.756000 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['[UNK]', '北', '京', '女', '子', '图', '鉴', '[UNK]',...'后', '戚', '薇', '挑', '战', '拉', '伸', '式', '问', '答']\n",
      "00:25:24.757185 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '[UNK]', '北', '京', '女', '子', '图', '鉴',... '薇', '挑', '战', '拉', '伸', '式', '问', '答', '[SEP]']\n",
      "00:25:24.757568 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:25:24.758340 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['北', '京', ',', '戚', '哥', ',', '戚', '薇']\n",
      "00:25:24.759030 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '[UNK]', '北', '京', '女', '子', '图', '鉴',... '北', '京', ',', '戚', '哥', ',', '戚', '薇', '[SEP]']\n",
      "00:25:24.759428 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 9\n",
      "00:25:24.759822 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 100, 1266, 776, 1957, 2094, 1745, 7063, 10...1266, 776, 117, 2774, 1520, 117, 2774, 5948, 102]\n",
      "00:25:24.760676 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  100, 1266,  776, 1957, 2094, 1745... 117, 2774, 1520,  117,        2774, 5948,  102])\n",
      "00:25:24.761266 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.762898 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.764401 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.765198 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  100, 1266,  776, 1957, 2094, 174... 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013422\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 285\n",
      "00:25:24.766943 call        14     def __getitem__(self, idx):\n",
      "00:25:24.767025 line        15         if self.mode == \"test\":\n",
      "00:25:24.767069 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '新水浒中的重要女主演，你知道她吗？'\n",
      "New var:....... text_b = '水浒,新水浒,甘婷婷,潘金莲,排进中国美女榜前列'\n",
      "New var:....... label = 0\n",
      "00:25:24.767706 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.768000 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.768108 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.768439 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['新', '水', '浒', '中', '的', '重', '要', '女', '主', '演', '，', '你', '知', '道', '她', '吗', '？']\n",
      "00:25:24.769418 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '新', '水', '浒', '中', '的', '重', '要', '女'... '演', '，', '你', '知', '道', '她', '吗', '？', '[SEP]']\n",
      "00:25:24.769726 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 19\n",
      "00:25:24.769871 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['水', '浒', ',', '新', '水', '浒', ',', '甘', '婷', '婷...',', '排', '进', '中', '国', '美', '女', '榜', '前', '列']\n",
      "00:25:24.770864 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '新', '水', '浒', '中', '的', '重', '要', '女'... '进', '中', '国', '美', '女', '榜', '前', '列', '[SEP]']\n",
      "00:25:24.771048 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 25\n",
      "00:25:24.771467 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3173, 3717, 3848, 704, 4638, 7028, 6206, 1...22, 704, 1744, 5401, 1957, 3528, 1184, 1154, 102]\n",
      "00:25:24.771869 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3173, 3717, 3848,  704, 4638, 7028...  704, 1744, 5401, 1957, 3528, 1184, 1154,  102])\n",
      "00:25:24.772534 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.773587 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.774192 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.775781 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3173, 3717, 3848,  704, 4638, 702... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013005\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 286\n",
      "00:25:24.779999 call        14     def __getitem__(self, idx):\n",
      "00:25:24.780105 line        15         if self.mode == \"test\":\n",
      "00:25:24.780157 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '2018年，杨幂、杨颖、唐嫣、迪丽热巴等，有哪些好看的IP大剧会播出？'\n",
      "New var:....... text_b = '创业时代,朱一龙,三生三世十里桃花,扶摇,花飞花谢花满天,唐嫣,王子文,莽荒纪,芸汐传,烈火如歌,杨幂,张馨予,李心艾,权力的游戏,赵又廷,王鸥,迪丽热巴,太古神王'\n",
      "New var:....... label = 0\n",
      "00:25:24.781000 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.781154 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.781467 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.782276 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['2018', '年', '，', '杨', '幂', '、', '杨', '颖', '、',...好', '看', '的', 'ip', '大', '剧', '会', '播', '出', '？']\n",
      "00:25:24.783856 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '2018', '年', '，', '杨', '幂', '、', '杨', ...'的', 'ip', '大', '剧', '会', '播', '出', '？', '[SEP]']\n",
      "00:25:24.784405 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 33\n",
      "00:25:24.784760 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['创', '业', '时', '代', ',', '朱', '一', '龙', ',', '三...',', '迪', '丽', '热', '巴', ',', '太', '古', '神', '王']\n",
      "00:25:24.786907 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '2018', '年', '，', '杨', '幂', '、', '杨', ... '丽', '热', '巴', ',', '太', '古', '神', '王', '[SEP]']\n",
      "00:25:24.787666 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 83\n",
      "00:25:24.788302 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 8271, 2399, 8024, 3342, 2386, 510, 3342, 7...14, 4178, 2349, 117, 1922, 1367, 4868, 4374, 102]\n",
      "00:25:24.788696 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 8271, 2399, 8024, 3342, 2386,  510... 4178, 2349,  117, 1922, 1367, 4868, 4374,  102])\n",
      "00:25:24.789207 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.790896 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.792434 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.795202 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 8271, 2399, 8024, 3342, 2386,  51... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.020137\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 287\n",
      "00:25:24.800204 call        14     def __getitem__(self, idx):\n",
      "00:25:24.800356 line        15         if self.mode == \"test\":\n",
      "00:25:24.800434 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '小哥费玉清模仿周华健唱歌，笑翻现场观众，小哥真逗'\n",
      "New var:....... text_b = '哥真逗,周华健,费玉清'\n",
      "New var:....... label = 0\n",
      "00:25:24.801333 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.801476 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.807789 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.808129 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['小', '哥', '费', '玉', '清', '模', '仿', '周', '华', '健...'翻', '现', '场', '观', '众', '，', '小', '哥', '真', '逗']\n",
      "00:25:24.808807 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '小', '哥', '费', '玉', '清', '模', '仿', '周'... '场', '观', '众', '，', '小', '哥', '真', '逗', '[SEP]']\n",
      "00:25:24.809031 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:24.809511 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['哥', '真', '逗', ',', '周', '华', '健', ',', '费', '玉', '清']\n",
      "00:25:24.810904 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '小', '哥', '费', '玉', '清', '模', '仿', '周'... ',', '周', '华', '健', ',', '费', '玉', '清', '[SEP]']\n",
      "00:25:24.811564 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 12\n",
      "00:25:24.811735 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2207, 1520, 6589, 4373, 3926, 3563, 820, 1...117, 1453, 1290, 978, 117, 6589, 4373, 3926, 102]\n",
      "00:25:24.812030 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2207, 1520, 6589, 4373, 3926, 3563...1290,  978,  117, 6589, 4373,        3926,  102])\n",
      "00:25:24.812233 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.812978 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.814032 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.814880 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2207, 1520, 6589, 4373, 3926, 356... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.017878\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 288\n",
      "00:25:24.818117 call        14     def __getitem__(self, idx):\n",
      "00:25:24.818250 line        15         if self.mode == \"test\":\n",
      "00:25:24.818297 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '泰坦尼克号重大的事故却拍成了爱情片？阿黛尔办派对扮相引发众怒'\n",
      "New var:....... text_b = '露丝,生日派对,泰坦尼克号,救生衣,阿黛尔,爱情片'\n",
      "New var:....... label = 0\n",
      "00:25:24.819023 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.819286 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.819504 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.819658 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['泰', '坦', '尼', '克', '号', '重', '大', '的', '事', '故...'尔', '办', '派', '对', '扮', '相', '引', '发', '众', '怒']\n",
      "00:25:24.820434 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '泰', '坦', '尼', '克', '号', '重', '大', '的'... '派', '对', '扮', '相', '引', '发', '众', '怒', '[SEP]']\n",
      "00:25:24.820703 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:24.820928 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['露', '丝', ',', '生', '日', '派', '对', ',', '泰', '坦...'生', '衣', ',', '阿', '黛', '尔', ',', '爱', '情', '片']\n",
      "00:25:24.821866 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '泰', '坦', '尼', '克', '号', '重', '大', '的'... ',', '阿', '黛', '尔', ',', '爱', '情', '片', '[SEP]']\n",
      "00:25:24.822358 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 26\n",
      "00:25:24.822824 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3805, 1788, 2225, 1046, 1384, 7028, 1920, ...17, 7350, 7950, 2209, 117, 4263, 2658, 4275, 102]\n",
      "00:25:24.823228 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3805, 1788, 2225, 1046, 1384, 7028... 7350, 7950, 2209,  117, 4263, 2658, 4275,  102])\n",
      "00:25:24.823593 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.824764 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.825877 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.827584 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3805, 1788, 2225, 1046, 1384, 702...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013319\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 289\n",
      "00:25:24.831477 call        14     def __getitem__(self, idx):\n",
      "00:25:24.831590 line        15         if self.mode == \"test\":\n",
      "00:25:24.831659 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '章子怡出席某活动，汪峰：孩子妈，叫你多穿点，你非不听！'\n",
      "New var:....... text_b = '章子怡,汪峰'\n",
      "New var:....... label = 0\n",
      "00:25:24.832372 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.832636 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.832760 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.833178 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['章', '子', '怡', '出', '席', '某', '活', '动', '，', '汪...'你', '多', '穿', '点', '，', '你', '非', '不', '听', '！']\n",
      "00:25:24.834100 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '章', '子', '怡', '出', '席', '某', '活', '动'... '穿', '点', '，', '你', '非', '不', '听', '！', '[SEP]']\n",
      "00:25:24.834794 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:24.835133 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['章', '子', '怡', ',', '汪', '峰']\n",
      "00:25:24.835453 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '章', '子', '怡', '出', '席', '某', '活', '动'..., '[SEP]', '章', '子', '怡', ',', '汪', '峰', '[SEP]']\n",
      "00:25:24.835606 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 7\n",
      "00:25:24.835749 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4995, 2094, 2592, 1139, 2375, 3378, 3833, ...013, 102, 4995, 2094, 2592, 117, 3742, 2292, 102]\n",
      "00:25:24.836079 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4995, 2094, 2592, 1139, 2375, 3378...  102, 4995, 2094, 2592,  117, 3742, 2292,  102])\n",
      "00:25:24.836476 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.837398 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0,        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.838308 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.839684 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4995, 2094, 2592, 1139, 2375, 337... 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010702\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 290\n",
      "00:25:24.842220 call        14     def __getitem__(self, idx):\n",
      "00:25:24.842370 line        15         if self.mode == \"test\":\n",
      "00:25:24.842428 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '古装戏中的抢婚戏，蒋勤勤直接刺死新郎，吴奇隆两次抢亲最深情'\n",
      "New var:....... text_b = '倚天屠龙记,聂风,张无忌,天下第一,吴奇隆,神雕侠侣,犀利仁师,宝莲灯,侠女闯天关,风云雄霸天下,孔慈,新郎,天涯明月刀'\n",
      "New var:....... label = 0\n",
      "00:25:24.843730 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.843912 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.844025 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.844184 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['古', '装', '戏', '中', '的', '抢', '婚', '戏', '，', '蒋...'吴', '奇', '隆', '两', '次', '抢', '亲', '最', '深', '情']\n",
      "00:25:24.845124 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '古', '装', '戏', '中', '的', '抢', '婚', '戏'... '隆', '两', '次', '抢', '亲', '最', '深', '情', '[SEP]']\n",
      "00:25:24.845549 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:24.845812 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['倚', '天', '屠', '龙', '记', ',', '聂', '风', ',', '张...'慈', ',', '新', '郎', ',', '天', '涯', '明', '月', '刀']\n",
      "00:25:24.847502 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '古', '装', '戏', '中', '的', '抢', '婚', '戏'... '新', '郎', ',', '天', '涯', '明', '月', '刀', '[SEP]']\n",
      "00:25:24.848314 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 61\n",
      "00:25:24.848771 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1367, 6163, 2767, 704, 4638, 2843, 2042, 2...73, 6947, 117, 1921, 3889, 3209, 3299, 1143, 102]\n",
      "00:25:24.849096 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1367, 6163, 2767,  704, 4638, 2843... 6947,  117, 1921, 3889, 3209, 3299, 1143,  102])\n",
      "00:25:24.849480 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.850864 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.852164 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.854463 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1367, 6163, 2767,  704, 4638, 284... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.019338\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 291\n",
      "00:25:24.861682 call        14     def __getitem__(self, idx):\n",
      "00:25:24.862057 line        15         if self.mode == \"test\":\n",
      "00:25:24.862116 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '陈红在巅峰期的美貌拿到现在，能媲美范冰冰、杨幂等一线女星吗？'\n",
      "New var:....... text_b = '陈红,陈凯歌,红楼梦,范冰冰,杨幂,太平公主,大明宫词'\n",
      "New var:....... label = 0\n",
      "00:25:24.862785 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.862919 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.863027 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.864083 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['陈', '红', '在', '巅', '峰', '期', '的', '美', '貌', '拿...'、', '杨', '幂', '等', '一', '线', '女', '星', '吗', '？']\n",
      "00:25:24.865616 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '红', '在', '巅', '峰', '期', '的', '美'... '幂', '等', '一', '线', '女', '星', '吗', '？', '[SEP]']\n",
      "00:25:24.866376 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:24.866638 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['陈', '红', ',', '陈', '凯', '歌', ',', '红', '楼', '梦...',', '太', '平', '公', '主', ',', '大', '明', '宫', '词']\n",
      "00:25:24.867524 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '红', '在', '巅', '峰', '期', '的', '美'... '平', '公', '主', ',', '大', '明', '宫', '词', '[SEP]']\n",
      "00:25:24.868033 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 28\n",
      "00:25:24.868515 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7357, 5273, 1762, 2330, 2292, 3309, 4638, ...398, 1062, 712, 117, 1920, 3209, 2151, 6404, 102]\n",
      "00:25:24.868792 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7357, 5273, 1762, 2330, 2292, 3309... 1062,  712,  117, 1920, 3209, 2151, 6404,  102])\n",
      "00:25:24.869094 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.870843 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.871518 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.872771 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7357, 5273, 1762, 2330, 2292, 330... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013488\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 292\n",
      "00:25:24.875208 call        14     def __getitem__(self, idx):\n",
      "00:25:24.875299 line        15         if self.mode == \"test\":\n",
      "00:25:24.875345 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '六小龄童不止会演孙悟空，这些角色你们见过吗'\n",
      "New var:....... text_b = '连城诀,过年,孙悟空,花铁干,程志,六小龄童,王牌对王牌,西游记'\n",
      "New var:....... label = 0\n",
      "00:25:24.876018 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.876146 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.876385 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.877082 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['六', '小', '龄', '童', '不', '止', '会', '演', '孙', '悟...'，', '这', '些', '角', '色', '你', '们', '见', '过', '吗']\n",
      "00:25:24.878129 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '六', '小', '龄', '童', '不', '止', '会', '演'... '些', '角', '色', '你', '们', '见', '过', '吗', '[SEP]']\n",
      "00:25:24.878859 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:25:24.879267 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['连', '城', '诀', ',', '过', '年', ',', '孙', '悟', '空...',', '王', '牌', '对', '王', '牌', ',', '西', '游', '记']\n",
      "00:25:24.880403 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '六', '小', '龄', '童', '不', '止', '会', '演'... '牌', '对', '王', '牌', ',', '西', '游', '记', '[SEP]']\n",
      "00:25:24.881187 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 33\n",
      "00:25:24.881790 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1063, 2207, 7977, 4997, 679, 3632, 833, 40...77, 2190, 4374, 4277, 117, 6205, 3952, 6381, 102]\n",
      "00:25:24.882675 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1063, 2207, 7977, 4997,  679, 3632... 2190, 4374, 4277,  117, 6205, 3952, 6381,  102])\n",
      "00:25:24.882963 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.885032 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.887585 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.889005 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1063, 2207, 7977, 4997,  679, 363...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.016392\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 293\n",
      "00:25:24.891648 call        14     def __getitem__(self, idx):\n",
      "00:25:24.891755 line        15         if self.mode == \"test\":\n",
      "00:25:24.891806 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = 'TVB甘草、爸爸级演员出现断层 邀请原亚视小生出山拍TVB'\n",
      "New var:....... text_b = '跳灰,甘草演员,烈哥,包青天再起风云,TVB,特约演员,城寨英雄,亚视'\n",
      "New var:....... label = 0\n",
      "00:25:24.892780 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.893095 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.893321 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.893593 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['tvb', '甘', '草', '、', '爸', '爸', '级', '演', '员', ...', '原', '亚', '视', '小', '生', '出', '山', '拍', 'tvb']\n",
      "00:25:24.894661 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'tvb', '甘', '草', '、', '爸', '爸', '级', '...亚', '视', '小', '生', '出', '山', '拍', 'tvb', '[SEP]']\n",
      "00:25:24.895256 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:24.895679 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['跳', '灰', ',', '甘', '草', '演', '员', ',', '烈', '哥...'演', '员', ',', '城', '寨', '英', '雄', ',', '亚', '视']\n",
      "00:25:24.896697 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'tvb', '甘', '草', '、', '爸', '爸', '级', '... ',', '城', '寨', '英', '雄', ',', '亚', '视', '[SEP]']\n",
      "00:25:24.897425 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 34\n",
      "00:25:24.897867 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 9312, 4491, 5770, 510, 4268, 4268, 5277, 4...117, 1814, 2181, 5739, 7413, 117, 762, 6228, 102]\n",
      "00:25:24.898449 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 9312, 4491, 5770,  510, 4268, 4268... 1814, 2181, 5739, 7413,  117,  762, 6228,  102])\n",
      "00:25:24.898695 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.899450 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.900598 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.903005 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 9312, 4491, 5770,  510, 4268, 426... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.014780\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 294\n",
      "00:25:24.906472 call        14     def __getitem__(self, idx):\n",
      "00:25:24.906565 line        15         if self.mode == \"test\":\n",
      "00:25:24.906616 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '因亮剑的和尚一夜走红，却不幸得抑郁症，一年之内两次自杀未遂'\n",
      "New var:....... text_b = '重度抑郁症,上阵父子兵,张桐,亮剑,魏和尚,抗抑郁药,雾柳镇,闯关东'\n",
      "New var:....... label = 0\n",
      "00:25:24.907281 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.907399 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.907637 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.907971 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['因', '亮', '剑', '的', '和', '尚', '一', '夜', '走', '红...'一', '年', '之', '内', '两', '次', '自', '杀', '未', '遂']\n",
      "00:25:24.908831 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '因', '亮', '剑', '的', '和', '尚', '一', '夜'... '之', '内', '两', '次', '自', '杀', '未', '遂', '[SEP]']\n",
      "00:25:24.909162 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:24.909398 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['重', '度', '抑', '郁', '症', ',', '上', '阵', '父', '子...'郁', '药', ',', '雾', '柳', '镇', ',', '闯', '关', '东']\n",
      "00:25:24.910815 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '因', '亮', '剑', '的', '和', '尚', '一', '夜'... ',', '雾', '柳', '镇', ',', '闯', '关', '东', '[SEP]']\n",
      "00:25:24.911210 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 35\n",
      "00:25:24.911825 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1728, 778, 1187, 4638, 1469, 2213, 671, 19...117, 7443, 3394, 7252, 117, 7310, 1068, 691, 102]\n",
      "00:25:24.912157 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1728,  778, 1187, 4638, 1469, 2213...3394,        7252,  117, 7310, 1068,  691,  102])\n",
      "00:25:24.912478 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.913996 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.915313 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.917886 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1728,  778, 1187, 4638, 1469, 221... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013788\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 295\n",
      "00:25:24.920302 call        14     def __getitem__(self, idx):\n",
      "00:25:24.920391 line        15         if self.mode == \"test\":\n",
      "00:25:24.920438 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '孙艺珍3月携两部新作回归 与苏志燮爱情片预告流出'\n",
      "New var:....... text_b = '爱情片,两部,苏志燮,孙艺珍'\n",
      "New var:....... label = 0\n",
      "00:25:24.921072 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.921186 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.921280 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.921555 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['孙', '艺', '珍', '3', '月', '携', '两', '部', '新', '作...'苏', '志', '燮', '爱', '情', '片', '预', '告', '流', '出']\n",
      "00:25:24.922212 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '孙', '艺', '珍', '3', '月', '携', '两', '部'... '燮', '爱', '情', '片', '预', '告', '流', '出', '[SEP]']\n",
      "00:25:24.922471 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:24.922617 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['爱', '情', '片', ',', '两', '部', ',', '苏', '志', '燮', ',', '孙', '艺', '珍']\n",
      "00:25:24.923064 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '孙', '艺', '珍', '3', '月', '携', '两', '部'... ',', '苏', '志', '燮', ',', '孙', '艺', '珍', '[SEP]']\n",
      "00:25:24.923211 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 15\n",
      "00:25:24.923355 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2101, 5686, 4397, 124, 3299, 3025, 697, 69...17, 5722, 2562, 4250, 117, 2101, 5686, 4397, 102]\n",
      "00:25:24.923631 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2101, 5686, 4397,  124, 3299, 3025...2562, 4250,  117,        2101, 5686, 4397,  102])\n",
      "00:25:24.923883 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.924575 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.925190 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.926111 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2101, 5686, 4397,  124, 3299, 302... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008497\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 296\n",
      "00:25:24.928844 call        14     def __getitem__(self, idx):\n",
      "00:25:24.928943 line        15         if self.mode == \"test\":\n",
      "00:25:24.928991 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '高云翔案再现逆天翻转，呆在房间36分钟极有可能是在吐酒'\n",
      "New var:....... text_b = '王晶,香格里拉酒店,高云翔,性侵,董璇'\n",
      "New var:....... label = 0\n",
      "00:25:24.929621 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.929855 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.930034 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.930326 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['高', '云', '翔', '案', '再', '现', '逆', '天', '翻', '转...'分', '钟', '极', '有', '可', '能', '是', '在', '吐', '酒']\n",
      "00:25:24.931253 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '高', '云', '翔', '案', '再', '现', '逆', '天'... '极', '有', '可', '能', '是', '在', '吐', '酒', '[SEP]']\n",
      "00:25:24.931465 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:24.931614 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['王', '晶', ',', '香', '格', '里', '拉', '酒', '店', ',', '高', '云', '翔', ',', '性', '侵', ',', '董', '璇']\n",
      "00:25:24.932172 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '高', '云', '翔', '案', '再', '现', '逆', '天'... '云', '翔', ',', '性', '侵', ',', '董', '璇', '[SEP]']\n",
      "00:25:24.932477 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:24.932655 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7770, 756, 5425, 3428, 1086, 4385, 6847, 1... 756, 5425, 117, 2595, 909, 117, 5869, 4462, 102]\n",
      "00:25:24.932835 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7770,  756, 5425, 3428, 1086, 4385... 5425,  117, 2595,  909,  117, 5869, 4462,  102])\n",
      "00:25:24.933094 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.933790 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.934640 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.935749 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7770,  756, 5425, 3428, 1086, 438... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009816\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 297\n",
      "00:25:24.938701 call        14     def __getitem__(self, idx):\n",
      "00:25:24.938789 line        15         if self.mode == \"test\":\n",
      "00:25:24.938833 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '“复联三”出现了辣么多超级英雄，我却pick了灭霸！'\n",
      "New var:....... text_b = '漫威,钢铁侠1,星战7,钢铁侠,绯红女巫,复仇者联盟3 ：无限战争,灭霸,X战警：逆转未来,复联3,复联4,蚁人2,雷神2,雷神1,美队3,超级英雄'\n",
      "New var:....... label = 0\n",
      "00:25:24.939506 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.939706 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.939876 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.940153 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['[UNK]', '复', '联', '三', '[UNK]', '出', '现', '了',... '，', '我', '却', 'pi', '##ck', '了', '灭', '霸', '！']\n",
      "00:25:24.940916 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '[UNK]', '复', '联', '三', '[UNK]', '出', ..., '却', 'pi', '##ck', '了', '灭', '霸', '！', '[SEP]']\n",
      "00:25:24.941146 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:24.941357 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['漫', '威', ',', '钢', '铁', '侠', '1', ',', '星', '战...'1', ',', '美', '队', '3', ',', '超', '级', '英', '雄']\n",
      "00:25:24.943001 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '[UNK]', '复', '联', '三', '[UNK]', '出', ... '美', '队', '3', ',', '超', '级', '英', '雄', '[SEP]']\n",
      "00:25:24.943327 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 74\n",
      "00:25:24.943586 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 100, 1908, 5468, 676, 100, 1139, 4385, 749...401, 7339, 124, 117, 6631, 5277, 5739, 7413, 102]\n",
      "00:25:24.943880 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,   100,  1908,  5468,   676,   100...  124,   117,  6631,  5277,  5739,  7413,   102])\n",
      "00:25:24.944159 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.945473 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:25:24.946566 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.948651 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,   100,  1908,  5468,   676,   10...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.014988\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 298\n",
      "00:25:24.953751 call        14     def __getitem__(self, idx):\n",
      "00:25:24.953887 line        15         if self.mode == \"test\":\n",
      "00:25:24.953955 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '大衣哥为新婚粉丝送祝福被赞接地气，成名8年老婆被他宠上天！'\n",
      "New var:....... text_b = '朱之文,草根明星,粉丝,大衣哥,大衣'\n",
      "New var:....... label = 0\n",
      "00:25:24.954988 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.955161 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.955300 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.955668 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['大', '衣', '哥', '为', '新', '婚', '粉', '丝', '送', '祝...'8', '年', '老', '婆', '被', '他', '宠', '上', '天', '！']\n",
      "00:25:24.957205 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '大', '衣', '哥', '为', '新', '婚', '粉', '丝'... '老', '婆', '被', '他', '宠', '上', '天', '！', '[SEP]']\n",
      "00:25:24.957536 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:24.957856 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['朱', '之', '文', ',', '草', '根', '明', '星', ',', '粉', '丝', ',', '大', '衣', '哥', ',', '大', '衣']\n",
      "00:25:24.958969 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '大', '衣', '哥', '为', '新', '婚', '粉', '丝'... '丝', ',', '大', '衣', '哥', ',', '大', '衣', '[SEP]']\n",
      "00:25:24.959369 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:25:24.959528 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1920, 6132, 1520, 711, 3173, 2042, 5106, 6...692, 117, 1920, 6132, 1520, 117, 1920, 6132, 102]\n",
      "00:25:24.959717 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1920, 6132, 1520,  711, 3173, 2042...1920, 6132, 1520,  117, 1920,        6132,  102])\n",
      "00:25:24.960156 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.961361 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:25:24.962453 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.964376 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1920, 6132, 1520,  711, 3173, 204...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013845\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 299\n",
      "00:25:24.967631 call        14     def __getitem__(self, idx):\n",
      "00:25:24.967710 line        15         if self.mode == \"test\":\n",
      "00:25:24.967754 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '结爱前两集，你绝对关注不到的点！'\n",
      "New var:....... text_b = '黄景瑜,腾讯视频,宋茜'\n",
      "New var:....... label = 0\n",
      "00:25:24.968406 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.968507 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.968594 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.968725 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['结', '爱', '前', '两', '集', '，', '你', '绝', '对', '关', '注', '不', '到', '的', '点', '！']\n",
      "00:25:24.969231 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '结', '爱', '前', '两', '集', '，', '你', '绝', '对', '关', '注', '不', '到', '的', '点', '！', '[SEP]']\n",
      "00:25:24.969377 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 18\n",
      "00:25:24.969809 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['黄', '景', '瑜', ',', '腾', '讯', '视', '频', ',', '宋', '茜']\n",
      "00:25:24.970381 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '结', '爱', '前', '两', '集', '，', '你', '绝'... ',', '腾', '讯', '视', '频', ',', '宋', '茜', '[SEP]']\n",
      "00:25:24.970548 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 12\n",
      "00:25:24.970694 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5310, 4263, 1184, 697, 7415, 8024, 872, 53...17, 5596, 6380, 6228, 7574, 117, 2129, 5752, 102]\n",
      "00:25:24.970849 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5310, 4263, 1184,  697, 7415, 8024...6380,        6228, 7574,  117, 2129, 5752,  102])\n",
      "00:25:24.971013 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.971563 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:25:24.972191 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.972967 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5310, 4263, 1184,  697, 7415, 802...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007056\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 300\n",
      "00:25:24.974724 call        14     def __getitem__(self, idx):\n",
      "00:25:24.974806 line        15         if self.mode == \"test\":\n",
      "00:25:24.974850 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《温暖的弦》在温暖和南弦的婚礼上，朱临路的这句话，温暖哭了'\n",
      "New var:....... text_b = '张嘉倪,婚礼,温暖和南弦,花花公子,温暖的弦'\n",
      "New var:....... label = 0\n",
      "00:25:24.975501 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.975608 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.975696 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.975902 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '温', '暖', '的', '弦', '》', '在', '温', '暖', '和...'路', '的', '这', '句', '话', '，', '温', '暖', '哭', '了']\n",
      "00:25:24.976722 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '温', '暖', '的', '弦', '》', '在', '温'... '这', '句', '话', '，', '温', '暖', '哭', '了', '[SEP]']\n",
      "00:25:24.977032 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:24.977290 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['张', '嘉', '倪', ',', '婚', '礼', ',', '温', '暖', '和...',', '花', '花', '公', '子', ',', '温', '暖', '的', '弦']\n",
      "00:25:24.977996 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '温', '暖', '的', '弦', '》', '在', '温'... '花', '公', '子', ',', '温', '暖', '的', '弦', '[SEP]']\n",
      "00:25:24.978248 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:25:24.978475 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 3946, 3265, 4638, 2478, 518, 1762, 39...09, 1062, 2094, 117, 3946, 3265, 4638, 2478, 102]\n",
      "00:25:24.978717 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 3946, 3265, 4638, 2478,  518...2094,         117, 3946, 3265, 4638, 2478,  102])\n",
      "00:25:24.978965 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.979883 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:25:24.980640 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.981846 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 3946, 3265, 4638, 2478,  51...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009543\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 301\n",
      "00:25:24.984301 call        14     def __getitem__(self, idx):\n",
      "00:25:24.984385 line        15         if self.mode == \"test\":\n",
      "00:25:24.984429 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '谢娜母亲节晒怀孕前视频，说“差点要翻脸”，那时的她的确好瘦啊'\n",
      "New var:....... text_b = '谢娜,蜡像馆,母亲节,杜莎夫人蜡像馆,杜莎夫人'\n",
      "New var:....... label = 0\n",
      "00:25:24.985031 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.985137 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.985315 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.985517 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['谢', '娜', '母', '亲', '节', '晒', '怀', '孕', '前', '视...'，', '那', '时', '的', '她', '的', '确', '好', '瘦', '啊']\n",
      "00:25:24.986349 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '谢', '娜', '母', '亲', '节', '晒', '怀', '孕'... '时', '的', '她', '的', '确', '好', '瘦', '啊', '[SEP]']\n",
      "00:25:24.986612 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:24.986832 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['谢', '娜', ',', '蜡', '像', '馆', ',', '母', '亲', '节...'夫', '人', '蜡', '像', '馆', ',', '杜', '莎', '夫', '人']\n",
      "00:25:24.987537 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '谢', '娜', '母', '亲', '节', '晒', '怀', '孕'... '蜡', '像', '馆', ',', '杜', '莎', '夫', '人', '[SEP]']\n",
      "00:25:24.987879 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "00:25:24.988223 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6468, 2025, 3678, 779, 5688, 3235, 2577, 2...058, 1008, 7667, 117, 3336, 5801, 1923, 782, 102]\n",
      "00:25:24.988484 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6468, 2025, 3678,  779, 5688, 3235... 1008, 7667,  117, 3336, 5801, 1923,  782,  102])\n",
      "00:25:24.988739 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.989465 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.990254 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.991436 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6468, 2025, 3678,  779, 5688, 323...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009493\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 302\n",
      "00:25:24.993828 call        14     def __getitem__(self, idx):\n",
      "00:25:24.993917 line        15         if self.mode == \"test\":\n",
      "00:25:24.993962 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '蔡徐坤一条微博，引发粉丝反思：何为青年？'\n",
      "New var:....... text_b = '五四青年节,粉丝,蔡徐坤'\n",
      "New var:....... label = 0\n",
      "00:25:24.994584 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:24.994691 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:24.994778 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:24.995125 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['蔡', '徐', '坤', '一', '条', '微', '博', '，', '引', '发', '粉', '丝', '反', '思', '：', '何', '为', '青', '年', '？']\n",
      "00:25:24.995806 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '蔡', '徐', '坤', '一', '条', '微', '博', '，'... '反', '思', '：', '何', '为', '青', '年', '？', '[SEP]']\n",
      "00:25:24.996027 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 22\n",
      "00:25:24.996237 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['五', '四', '青', '年', '节', ',', '粉', '丝', ',', '蔡', '徐', '坤']\n",
      "00:25:24.996718 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '蔡', '徐', '坤', '一', '条', '微', '博', '，'... '节', ',', '粉', '丝', ',', '蔡', '徐', '坤', '[SEP]']\n",
      "00:25:24.996937 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 13\n",
      "00:25:24.997234 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5918, 2528, 1787, 671, 3340, 2544, 1300, 8...5688, 117, 5106, 692, 117, 5918, 2528, 1787, 102]\n",
      "00:25:24.997462 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5918, 2528, 1787,  671, 3340, 2544...  117, 5106,  692,  117, 5918, 2528, 1787,  102])\n",
      "00:25:24.997694 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:24.998265 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:24.998833 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:24.999807 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5918, 2528, 1787,  671, 3340, 254...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007686\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 303\n",
      "00:25:25.001546 call        14     def __getitem__(self, idx):\n",
      "00:25:25.001624 line        15         if self.mode == \"test\":\n",
      "00:25:25.001666 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '李冰冰出席活动，网友：优美静雅又充满古典韵味'\n",
      "New var:....... text_b = '充满古典韵味李冰冰出席,优美静雅,李冰冰'\n",
      "New var:....... label = 0\n",
      "00:25:25.002261 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.002481 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.002575 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.002798 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['李', '冰', '冰', '出', '席', '活', '动', '，', '网', '友...'美', '静', '雅', '又', '充', '满', '古', '典', '韵', '味']\n",
      "00:25:25.003487 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '冰', '冰', '出', '席', '活', '动', '，'... '雅', '又', '充', '满', '古', '典', '韵', '味', '[SEP]']\n",
      "00:25:25.003776 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:25:25.003991 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['充', '满', '古', '典', '韵', '味', '李', '冰', '冰', '出', '席', ',', '优', '美', '静', '雅', ',', '李', '冰', '冰']\n",
      "00:25:25.004630 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '冰', '冰', '出', '席', '活', '动', '，'... '优', '美', '静', '雅', ',', '李', '冰', '冰', '[SEP]']\n",
      "00:25:25.004928 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:25:25.005148 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3330, 1102, 1102, 1139, 2375, 3833, 1220, ...31, 5401, 7474, 7414, 117, 3330, 1102, 1102, 102]\n",
      "00:25:25.005441 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3330, 1102, 1102, 1139, 2375, 3833... 5401, 7474, 7414,  117, 3330, 1102, 1102,  102])\n",
      "00:25:25.005725 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.006402 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:25.007683 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.008826 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3330, 1102, 1102, 1139, 2375, 383... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009939\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 304\n",
      "00:25:25.011521 call        14     def __getitem__(self, idx):\n",
      "00:25:25.011602 line        15         if self.mode == \"test\":\n",
      "00:25:25.011647 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '王菲小号曝光和谢霆锋频秀恩爱，王迅也用小号秀过小娇妻恩爱照'\n",
      "New var:....... text_b = '谢霆锋,王菲,恩爱照,王迅,小娇妻'\n",
      "New var:....... label = 0\n",
      "00:25:25.012272 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.012589 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.012700 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.012858 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['王', '菲', '小', '号', '曝', '光', '和', '谢', '霆', '锋...'小', '号', '秀', '过', '小', '娇', '妻', '恩', '爱', '照']\n",
      "00:25:25.013636 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '菲', '小', '号', '曝', '光', '和', '谢'... '秀', '过', '小', '娇', '妻', '恩', '爱', '照', '[SEP]']\n",
      "00:25:25.013787 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:25.013927 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['谢', '霆', '锋', ',', '王', '菲', ',', '恩', '爱', '照', ',', '王', '迅', ',', '小', '娇', '妻']\n",
      "00:25:25.014729 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '菲', '小', '号', '曝', '光', '和', '谢'... '照', ',', '王', '迅', ',', '小', '娇', '妻', '[SEP]']\n",
      "00:25:25.015174 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:25.015450 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4374, 5838, 2207, 1384, 3284, 1045, 1469, ...212, 117, 4374, 6813, 117, 2207, 2019, 1988, 102]\n",
      "00:25:25.015743 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4374, 5838, 2207, 1384, 3284, 1045...4374, 6813,  117, 2207, 2019, 1988,         102])\n",
      "00:25:25.016043 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.016824 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:25:25.017725 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.018979 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4374, 5838, 2207, 1384, 3284, 104...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010085\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 305\n",
      "00:25:25.021640 call        14     def __getitem__(self, idx):\n",
      "00:25:25.021723 line        15         if self.mode == \"test\":\n",
      "00:25:25.021768 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '她19岁出道默默无闻，嫁给爱情暂退演艺圈，如今成最大人生赢家！'\n",
      "New var:....... text_b = '失恋33天,沈佳妮,朱亚文,向天真女孩投降,中央戏剧学院,爱在苍茫大地,演艺圈,默默无闻,风儿轻轻吹'\n",
      "New var:....... label = 0\n",
      "00:25:25.022426 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.022664 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.022753 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.023007 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['她', '19', '岁', '出', '道', '默', '默', '无', '闻', '...'如', '今', '成', '最', '大', '人', '生', '赢', '家', '！']\n",
      "00:25:25.023883 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '她', '19', '岁', '出', '道', '默', '默', '无... '成', '最', '大', '人', '生', '赢', '家', '！', '[SEP]']\n",
      "00:25:25.024103 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:25.024376 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['失', '恋', '33', '天', ',', '沈', '佳', '妮', ',', '...'默', '默', '无', '闻', ',', '风', '儿', '轻', '轻', '吹']\n",
      "00:25:25.025663 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '她', '19', '岁', '出', '道', '默', '默', '无... '无', '闻', ',', '风', '儿', '轻', '轻', '吹', '[SEP]']\n",
      "00:25:25.026111 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 50\n",
      "00:25:25.026426 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1961, 8131, 2259, 1139, 6887, 7949, 7949, ...87, 7319, 117, 7599, 1036, 6768, 6768, 1430, 102]\n",
      "00:25:25.026752 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1961, 8131, 2259, 1139, 6887, 7949... 7319,  117, 7599, 1036, 6768, 6768, 1430,  102])\n",
      "00:25:25.027065 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.028077 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:25.029281 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.030909 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1961, 8131, 2259, 1139, 6887, 794...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.012765\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 306\n",
      "00:25:25.034439 call        14     def __getitem__(self, idx):\n",
      "00:25:25.034520 line        15         if self.mode == \"test\":\n",
      "00:25:25.034563 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '21岁的赵雅芝，31岁的赵雅芝，63岁的赵雅芝，不老女神名不虚传'\n",
      "New var:....... text_b = '坚强不屈,观世音,百年沉浮,青花,赵雅芝出自,新白娘子传奇,赵雅芝,上海滩,剧中饰演,冯程程,乘风破浪'\n",
      "New var:....... label = 0\n",
      "00:25:25.035205 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.035468 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.035570 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.036117 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['21', '岁', '的', '赵', '雅', '芝', '，', '31', '岁', ...'芝', '，', '不', '老', '女', '神', '名', '不', '虚', '传']\n",
      "00:25:25.037029 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '21', '岁', '的', '赵', '雅', '芝', '，', '3... '不', '老', '女', '神', '名', '不', '虚', '传', '[SEP]']\n",
      "00:25:25.037346 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:25.037762 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['坚', '强', '不', '屈', ',', '观', '世', '音', ',', '百...'演', ',', '冯', '程', '程', ',', '乘', '风', '破', '浪']\n",
      "00:25:25.039111 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '21', '岁', '的', '赵', '雅', '芝', '，', '3... '冯', '程', '程', ',', '乘', '风', '破', '浪', '[SEP]']\n",
      "00:25:25.039407 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 52\n",
      "00:25:25.039838 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 8128, 2259, 4638, 6627, 7414, 5698, 8024, ...101, 4923, 4923, 117, 733, 7599, 4788, 3857, 102]\n",
      "00:25:25.040191 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 8128, 2259, 4638, 6627, 7414, 5698... 4923, 4923,  117,  733, 7599, 4788, 3857,  102])\n",
      "00:25:25.040630 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.041637 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:25.042864 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.044570 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 8128, 2259, 4638, 6627, 7414, 569...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013491\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 307\n",
      "00:25:25.047970 call        14     def __getitem__(self, idx):\n",
      "00:25:25.048056 line        15         if self.mode == \"test\":\n",
      "00:25:25.048100 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '林心如最美古装造型不是紫薇，而是这个造型'\n",
      "New var:....... text_b = '林心如,紫薇,古装'\n",
      "New var:....... label = 0\n",
      "00:25:25.048732 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.048921 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.049141 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.049515 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['林', '心', '如', '最', '美', '古', '装', '造', '型', '不', '是', '紫', '薇', '，', '而', '是', '这', '个', '造', '型']\n",
      "00:25:25.050194 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '心', '如', '最', '美', '古', '装', '造'... '薇', '，', '而', '是', '这', '个', '造', '型', '[SEP]']\n",
      "00:25:25.050454 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 22\n",
      "00:25:25.050705 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['林', '心', '如', ',', '紫', '薇', ',', '古', '装']\n",
      "00:25:25.051160 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '心', '如', '最', '美', '古', '装', '造'... '心', '如', ',', '紫', '薇', ',', '古', '装', '[SEP]']\n",
      "00:25:25.051547 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 10\n",
      "00:25:25.051805 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3360, 2552, 1963, 3297, 5401, 1367, 6163, ...552, 1963, 117, 5166, 5948, 117, 1367, 6163, 102]\n",
      "00:25:25.052074 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3360, 2552, 1963, 3297, 5401, 1367... 1963,  117, 5166, 5948,  117, 1367, 6163,  102])\n",
      "00:25:25.052350 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.052989 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:25.053867 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.054876 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3360, 2552, 1963, 3297, 5401, 136...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008938\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 308\n",
      "00:25:25.056941 call        14     def __getitem__(self, idx):\n",
      "00:25:25.057018 line        15         if self.mode == \"test\":\n",
      "00:25:25.057059 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '范爷携国内外巨星亮相戛纳国际，“八仙过海，各显神通”？'\n",
      "New var:....... text_b = '人尽皆知,如影随心,西游记,戛纳,战狼2,首映礼,戛纳电影节,金角大王,佩内洛普'\n",
      "New var:....... label = 0\n",
      "00:25:25.057723 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.057877 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.057969 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.058331 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['范', '爷', '携', '国', '内', '外', '巨', '星', '亮', '相... '过', '海', '，', '各', '显', '神', '通', '[UNK]', '？']\n",
      "00:25:25.059026 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '范', '爷', '携', '国', '内', '外', '巨', '星'..., '，', '各', '显', '神', '通', '[UNK]', '？', '[SEP]']\n",
      "00:25:25.059407 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:25.059804 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['人', '尽', '皆', '知', ',', '如', '影', '随', '心', ',...',', '金', '角', '大', '王', ',', '佩', '内', '洛', '普']\n",
      "00:25:25.061577 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '范', '爷', '携', '国', '内', '外', '巨', '星'... '角', '大', '王', ',', '佩', '内', '洛', '普', '[SEP]']\n",
      "00:25:25.063732 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 41\n",
      "00:25:25.064045 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5745, 4267, 3025, 1744, 1079, 1912, 2342, ...235, 1920, 4374, 117, 877, 1079, 3821, 3249, 102]\n",
      "00:25:25.065781 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5745, 4267, 3025, 1744, 1079, 1912... 1920, 4374,  117,  877, 1079, 3821, 3249,  102])\n",
      "00:25:25.066231 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.068308 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:25.071242 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.074415 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5745, 4267, 3025, 1744, 1079, 191... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.023019\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 309\n",
      "00:25:25.080027 call        14     def __getitem__(self, idx):\n",
      "00:25:25.080588 line        15         if self.mode == \"test\":\n",
      "00:25:25.080667 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '范冰冰高情商回应机场签名一事，原来粉丝认错爱豆的事情还不少！'\n",
      "New var:....... text_b = '外国友人,朴信惠,范冰冰,林更新,赵丽颖'\n",
      "New var:....... label = 0\n",
      "00:25:25.081670 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.082105 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.082260 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.082700 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['范', '冰', '冰', '高', '情', '商', '回', '应', '机', '场...'错', '爱', '豆', '的', '事', '情', '还', '不', '少', '！']\n",
      "00:25:25.084048 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '范', '冰', '冰', '高', '情', '商', '回', '应'... '豆', '的', '事', '情', '还', '不', '少', '！', '[SEP]']\n",
      "00:25:25.084809 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:25.085244 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['外', '国', '友', '人', ',', '朴', '信', '惠', ',', '范', '冰', '冰', ',', '林', '更', '新', ',', '赵', '丽', '颖']\n",
      "00:25:25.086627 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '范', '冰', '冰', '高', '情', '商', '回', '应'... ',', '林', '更', '新', ',', '赵', '丽', '颖', '[SEP]']\n",
      "00:25:25.087340 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:25:25.087816 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5745, 1102, 1102, 7770, 2658, 1555, 1726, ...117, 3360, 3291, 3173, 117, 6627, 714, 7577, 102]\n",
      "00:25:25.088095 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5745, 1102, 1102, 7770, 2658, 1555...3291, 3173,         117, 6627,  714, 7577,  102])\n",
      "00:25:25.088364 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.089134 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1])\n",
      "00:25:25.090684 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.093733 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5745, 1102, 1102, 7770, 2658, 155...1, 1, 1, 1, 1,        1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.018243\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 310\n",
      "00:25:25.098325 call        14     def __getitem__(self, idx):\n",
      "00:25:25.098453 line        15         if self.mode == \"test\":\n",
      "00:25:25.098546 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '阿娇怀孕已坐实？一袭绿裙，腰腹圆润，疑似怀孕'\n",
      "New var:....... text_b = '婚变,腰腹,电影,钟欣潼,阿娇'\n",
      "New var:....... label = 0\n",
      "00:25:25.099474 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.099629 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.099757 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.100175 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['阿', '娇', '怀', '孕', '已', '坐', '实', '？', '一', '袭...'，', '腰', '腹', '圆', '润', '，', '疑', '似', '怀', '孕']\n",
      "00:25:25.101583 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '阿', '娇', '怀', '孕', '已', '坐', '实', '？'... '腹', '圆', '润', '，', '疑', '似', '怀', '孕', '[SEP]']\n",
      "00:25:25.102043 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:25:25.102508 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['婚', '变', ',', '腰', '腹', ',', '电', '影', ',', '钟', '欣', '潼', ',', '阿', '娇']\n",
      "00:25:25.103734 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '阿', '娇', '怀', '孕', '已', '坐', '实', '？'... '影', ',', '钟', '欣', '潼', ',', '阿', '娇', '[SEP]']\n",
      "00:25:25.104157 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 16\n",
      "00:25:25.104561 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7350, 2019, 2577, 2097, 2347, 1777, 2141, ...512, 117, 7164, 3615, 4065, 117, 7350, 2019, 102]\n",
      "00:25:25.104852 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7350, 2019, 2577, 2097, 2347, 1777...7164, 3615, 4065,         117, 7350, 2019,  102])\n",
      "00:25:25.105619 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.106803 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:25.109523 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.112125 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7350, 2019, 2577, 2097, 2347, 177... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.017013\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 311\n",
      "00:25:25.115391 call        14     def __getitem__(self, idx):\n",
      "00:25:25.115508 line        15         if self.mode == \"test\":\n",
      "00:25:25.115558 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '作为湖北的家人，你知道从湖北走出的明星有哪些吗？'\n",
      "New var:....... text_b = '琅琊榜,丑女无敌,北京电影学院,中央戏剧学院,袁姗姗,北平无战事,宫锁心玉,少年杨家将,欢喜婆婆俏媳妇,美人心计,唐宫美人天下,李宗翰,金粉世家,宫锁珠帘,神雕侠侣'\n",
      "New var:....... label = 0\n",
      "00:25:25.116227 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.116538 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.116672 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.116978 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['作', '为', '湖', '北', '的', '家', '人', '，', '你', '知...'走', '出', '的', '明', '星', '有', '哪', '些', '吗', '？']\n",
      "00:25:25.117647 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '作', '为', '湖', '北', '的', '家', '人', '，'... '的', '明', '星', '有', '哪', '些', '吗', '？', '[SEP]']\n",
      "00:25:25.117794 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:25.118148 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['琅', '琊', '榜', ',', '丑', '女', '无', '敌', ',', '北...',', '宫', '锁', '珠', '帘', ',', '神', '雕', '侠', '侣']\n",
      "00:25:25.119923 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '作', '为', '湖', '北', '的', '家', '人', '，'... '锁', '珠', '帘', ',', '神', '雕', '侠', '侣', '[SEP]']\n",
      "00:25:25.120525 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 83\n",
      "00:25:25.121048 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 868, 711, 3959, 1266, 4638, 2157, 782, 802...7219, 4403, 2366, 117, 4868, 7425, 899, 901, 102]\n",
      "00:25:25.121403 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  868,  711, 3959, 1266, 4638, 2157...2366,  117, 4868, 7425,  899,  901,         102])\n",
      "00:25:25.121746 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.123365 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:25.124616 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.127006 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  868,  711, 3959, 1266, 4638, 215... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.016502\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 312\n",
      "00:25:25.131929 call        14     def __getitem__(self, idx):\n",
      "00:25:25.132014 line        15         if self.mode == \"test\":\n",
      "00:25:25.132058 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '在“重返未来”中，民乐“遇上”踢踏舞'\n",
      "New var:....... text_b = '民族音乐,卢中强,重返未来,民族乐器,民乐,踢踏舞'\n",
      "New var:....... label = 0\n",
      "00:25:25.132673 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.132790 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.132880 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.133150 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['在', '[UNK]', '重', '返', '未', '来', '[UNK]', '中',..., '乐', '[UNK]', '遇', '上', '[UNK]', '踢', '踏', '舞']\n",
      "00:25:25.133791 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '在', '[UNK]', '重', '返', '未', '来', '[UN...UNK]', '遇', '上', '[UNK]', '踢', '踏', '舞', '[SEP]']\n",
      "00:25:25.134173 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 20\n",
      "00:25:25.134429 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['民', '族', '音', '乐', ',', '卢', '中', '强', ',', '重...'族', '乐', '器', ',', '民', '乐', ',', '踢', '踏', '舞']\n",
      "00:25:25.135592 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '在', '[UNK]', '重', '返', '未', '来', '[UN... '器', ',', '民', '乐', ',', '踢', '踏', '舞', '[SEP]']\n",
      "00:25:25.136175 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 26\n",
      "00:25:25.136576 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1762, 100, 7028, 6819, 3313, 3341, 100, 70...1690, 117, 3696, 727, 117, 6677, 6672, 5659, 102]\n",
      "00:25:25.136864 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1762,  100, 7028, 6819, 3313, 3341...  117, 3696,  727,  117, 6677, 6672, 5659,  102])\n",
      "00:25:25.137289 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.138134 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:25.139192 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.140535 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1762,  100, 7028, 6819, 3313, 334... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011792\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 313\n",
      "00:25:25.143760 call        14     def __getitem__(self, idx):\n",
      "00:25:25.143847 line        15         if self.mode == \"test\":\n",
      "00:25:25.143890 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '仙剑难道注定要成为传说了吗？'\n",
      "New var:....... text_b = '仙剑,赵灵儿,歌哥,刘亦菲,赵丽颖'\n",
      "New var:....... label = 0\n",
      "00:25:25.144510 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.144777 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.144866 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.145001 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['仙', '剑', '难', '道', '注', '定', '要', '成', '为', '传', '说', '了', '吗', '？']\n",
      "00:25:25.145575 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '仙', '剑', '难', '道', '注', '定', '要', '成', '为', '传', '说', '了', '吗', '？', '[SEP]']\n",
      "00:25:25.146029 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 16\n",
      "00:25:25.146366 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['仙', '剑', ',', '赵', '灵', '儿', ',', '歌', '哥', ',', '刘', '亦', '菲', ',', '赵', '丽', '颖']\n",
      "00:25:25.147013 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '仙', '剑', '难', '道', '注', '定', '要', '成'... ',', '刘', '亦', '菲', ',', '赵', '丽', '颖', '[SEP]']\n",
      "00:25:25.147412 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:25.147674 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 803, 1187, 7410, 6887, 3800, 2137, 6206, 2... 117, 1155, 771, 5838, 117, 6627, 714, 7577, 102]\n",
      "00:25:25.147946 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  803, 1187, 7410, 6887, 3800, 2137... 1155,  771, 5838,  117, 6627,  714, 7577,  102])\n",
      "00:25:25.148226 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.148878 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:25.150091 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.151199 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  803, 1187, 7410, 6887, 3800, 213...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009938\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 314\n",
      "00:25:25.153737 call        14     def __getitem__(self, idx):\n",
      "00:25:25.153821 line        15         if self.mode == \"test\":\n",
      "00:25:25.153864 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '最新一期快乐大本营，吴昕怀孕，奚梦瑶被黑惨了！'\n",
      "New var:....... text_b = '杨迪,快乐大本营,吴昕,奚梦瑶,蔡少芬'\n",
      "New var:....... label = 0\n",
      "00:25:25.154504 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.154757 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.154848 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.155100 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['最', '新', '一', '期', '快', '乐', '大', '本', '营', '，...'孕', '，', '奚', '梦', '瑶', '被', '黑', '惨', '了', '！']\n",
      "00:25:25.155835 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '最', '新', '一', '期', '快', '乐', '大', '本'... '奚', '梦', '瑶', '被', '黑', '惨', '了', '！', '[SEP]']\n",
      "00:25:25.156206 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:25.156554 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['杨', '迪', ',', '快', '乐', '大', '本', '营', ',', '吴', '昕', ',', '奚', '梦', '瑶', ',', '蔡', '少', '芬']\n",
      "00:25:25.157420 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '最', '新', '一', '期', '快', '乐', '大', '本'... ',', '奚', '梦', '瑶', ',', '蔡', '少', '芬', '[SEP]']\n",
      "00:25:25.157760 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:25.158067 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3297, 3173, 671, 3309, 2571, 727, 1920, 33...17, 1949, 3457, 4457, 117, 5918, 2208, 5705, 102]\n",
      "00:25:25.158363 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3297, 3173,  671, 3309, 2571,  727... 1949, 3457, 4457,  117, 5918, 2208, 5705,  102])\n",
      "00:25:25.158665 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.159632 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:25.160448 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.162235 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3297, 3173,  671, 3309, 2571,  72... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010039\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 315\n",
      "00:25:25.163814 call        14     def __getitem__(self, idx):\n",
      "00:25:25.163903 line        15         if self.mode == \"test\":\n",
      "00:25:25.163946 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《乡村爱情》赵四关小平关思慧父女炮轰爱奇艺歧视河南人'\n",
      "New var:....... text_b = '河南人,乡村爱情,爱奇艺,关思慧,东北人'\n",
      "New var:....... label = 0\n",
      "00:25:25.164565 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.164785 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.164873 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.165122 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '乡', '村', '爱', '情', '》', '赵', '四', '关', '小...'炮', '轰', '爱', '奇', '艺', '歧', '视', '河', '南', '人']\n",
      "00:25:25.165910 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '乡', '村', '爱', '情', '》', '赵', '四'... '爱', '奇', '艺', '歧', '视', '河', '南', '人', '[SEP]']\n",
      "00:25:25.166161 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:25.166419 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['河', '南', '人', ',', '乡', '村', '爱', '情', ',', '爱', '奇', '艺', ',', '关', '思', '慧', ',', '东', '北', '人']\n",
      "00:25:25.167074 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '乡', '村', '爱', '情', '》', '赵', '四'... ',', '关', '思', '慧', ',', '东', '北', '人', '[SEP]']\n",
      "00:25:25.167387 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:25:25.167644 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 740, 3333, 4263, 2658, 518, 6627, 172... 117, 1068, 2590, 2716, 117, 691, 1266, 782, 102]\n",
      "00:25:25.167889 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517,  740, 3333, 4263, 2658,  518...2590, 2716,  117,  691, 1266,  782,         102])\n",
      "00:25:25.168148 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.168891 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:25:25.169680 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.170670 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517,  740, 3333, 4263, 2658,  51...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009369\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 316\n",
      "00:25:25.173216 call        14     def __getitem__(self, idx):\n",
      "00:25:25.173295 line        15         if self.mode == \"test\":\n",
      "00:25:25.173338 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张钧甯大赞“男友”张翰演戏有想法，但私底下很闷骚！'\n",
      "New var:....... text_b = '彭于晏,张钧甯,张翰,温暖的弦,撒狗粮'\n",
      "New var:....... label = 0\n",
      "00:25:25.173974 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.174078 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.174164 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.174474 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '钧', '甯', '大', '赞', '[UNK]', '男', '友', '[U...'法', '，', '但', '私', '底', '下', '很', '闷', '骚', '！']\n",
      "00:25:25.175219 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '钧', '甯', '大', '赞', '[UNK]', '男',... '但', '私', '底', '下', '很', '闷', '骚', '！', '[SEP]']\n",
      "00:25:25.175443 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:25:25.175650 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['彭', '于', '晏', ',', '张', '钧', '甯', ',', '张', '翰', ',', '温', '暖', '的', '弦', ',', '撒', '狗', '粮']\n",
      "00:25:25.176253 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '钧', '甯', '大', '赞', '[UNK]', '男',... '温', '暖', '的', '弦', ',', '撒', '狗', '粮', '[SEP]']\n",
      "00:25:25.176502 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:25.176829 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 7172, 4505, 1920, 6614, 100, 4511, 1...46, 3265, 4638, 2478, 117, 3054, 4318, 5117, 102]\n",
      "00:25:25.177092 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476, 7172, 4505, 1920, 6614,  100... 3265, 4638, 2478,  117, 3054, 4318, 5117,  102])\n",
      "00:25:25.177352 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.178121 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:25.178837 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.180050 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2476, 7172, 4505, 1920, 6614,  10... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009287\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 317\n",
      "00:25:25.182558 call        14     def __getitem__(self, idx):\n",
      "00:25:25.182695 line        15         if self.mode == \"test\":\n",
      "00:25:25.182768 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '林心如最出名的6部剧 角色多彩第一部每年假期都播！'\n",
      "New var:....... text_b = '美人心计,情深深雨濛濛,男才女貌,林心如,窦漪房,半生缘'\n",
      "New var:....... label = 0\n",
      "00:25:25.183403 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.183510 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.183596 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.183828 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['林', '心', '如', '最', '出', '名', '的', '6', '部', '剧...'第', '一', '部', '每', '年', '假', '期', '都', '播', '！']\n",
      "00:25:25.184551 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '心', '如', '最', '出', '名', '的', '6'... '部', '每', '年', '假', '期', '都', '播', '！', '[SEP]']\n",
      "00:25:25.184877 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:25.185154 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['美', '人', '心', '计', ',', '情', '深', '深', '雨', '濛...'心', '如', ',', '窦', '漪', '房', ',', '半', '生', '缘']\n",
      "00:25:25.185959 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '心', '如', '最', '出', '名', '的', '6'... ',', '窦', '漪', '房', ',', '半', '生', '缘', '[SEP]']\n",
      "00:25:25.186213 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 29\n",
      "00:25:25.186432 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3360, 2552, 1963, 3297, 1139, 1399, 4638, ...17, 4977, 4034, 2791, 117, 1288, 4495, 5357, 102]\n",
      "00:25:25.186674 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3360, 2552, 1963, 3297, 1139, 1399...       4034, 2791,  117, 1288, 4495, 5357,  102])\n",
      "00:25:25.186915 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.187725 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:25.188433 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.189611 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3360, 2552, 1963, 3297, 1139, 139...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009644\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 318\n",
      "00:25:25.192228 call        14     def __getitem__(self, idx):\n",
      "00:25:25.192312 line        15         if self.mode == \"test\":\n",
      "00:25:25.192355 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '看剧不如看衣服 张钧甯轻西装超美'\n",
      "New var:....... text_b = '西装,衬衫,外套,小西装,张钧甯'\n",
      "New var:....... label = 0\n",
      "00:25:25.192981 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.193177 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.193264 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.193463 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['看', '剧', '不', '如', '看', '衣', '服', '张', '钧', '甯', '轻', '西', '装', '超', '美']\n",
      "00:25:25.193995 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '看', '剧', '不', '如', '看', '衣', '服', '张', '钧', '甯', '轻', '西', '装', '超', '美', '[SEP]']\n",
      "00:25:25.194208 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 17\n",
      "00:25:25.194369 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['西', '装', ',', '衬', '衫', ',', '外', '套', ',', '小', '西', '装', ',', '张', '钧', '甯']\n",
      "00:25:25.194881 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '看', '剧', '不', '如', '看', '衣', '服', '张'... ',', '小', '西', '装', ',', '张', '钧', '甯', '[SEP]']\n",
      "00:25:25.195092 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 17\n",
      "00:25:25.195400 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4692, 1196, 679, 1963, 4692, 6132, 3302, 2...17, 2207, 6205, 6163, 117, 2476, 7172, 4505, 102]\n",
      "00:25:25.195665 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4692, 1196,  679, 1963, 4692, 6132... 2207, 6205, 6163,  117, 2476, 7172, 4505,  102])\n",
      "00:25:25.195926 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.196530 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:25.197231 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.198577 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4692, 1196,  679, 1963, 4692, 613...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008006\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 319\n",
      "00:25:25.200270 call        14     def __getitem__(self, idx):\n",
      "00:25:25.200349 line        15         if self.mode == \"test\":\n",
      "00:25:25.200391 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '这部剧朱茵愧对“天下第一美人” 网友：看走眼了，还不如张馨予'\n",
      "New var:....... text_b = '朱茵,沈璧君,大话西游,天下第一美人,张馨予,紫霞仙子,萧十一郎'\n",
      "New var:....... label = 0\n",
      "00:25:25.202029 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:25.202287 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:25.202476 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:25.202809 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['这', '部', '剧', '朱', '茵', '愧', '对', '[UNK]', '天'...'走', '眼', '了', '，', '还', '不', '如', '张', '馨', '予']\n",
      "00:25:25.203669 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '部', '剧', '朱', '茵', '愧', '对', '[U... '了', '，', '还', '不', '如', '张', '馨', '予', '[SEP]']\n",
      "00:25:25.203948 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:25.204222 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['朱', '茵', ',', '沈', '璧', '君', ',', '大', '话', '西...',', '紫', '霞', '仙', '子', ',', '萧', '十', '一', '郎']\n",
      "00:25:25.205214 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '部', '剧', '朱', '茵', '愧', '对', '[U... '霞', '仙', '子', ',', '萧', '十', '一', '郎', '[SEP]']\n",
      "00:25:25.205472 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 33\n",
      "00:25:25.205729 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6821, 6956, 1196, 3319, 5762, 2700, 2190, ...7459, 803, 2094, 117, 5854, 1282, 671, 6947, 102]\n",
      "00:25:25.206023 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6821, 6956, 1196, 3319, 5762, 2700...2094,  117, 5854,        1282,  671, 6947,  102])\n",
      "00:25:25.206301 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:25.207410 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:25.208197 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:25.209723 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6821, 6956, 1196, 3319, 5762, 270... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013045\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/IPython/core/\u001b[0m\u001b[1;38;5;231minteractiveshell.py\u001b[0m\u001b[38;5;145m, line 3325, in run_code\n\u001b[0m    \u001b[38;5;188m\u001b[0m\u001b[38;5;188mexec\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188mcode_obj\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself.user_global_ns\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself.user_ns\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[38;5;145mFile \u001b[0m\u001b[1;38;5;231m<ipython-input-55-aab6d4e4b2f6>\u001b[0m\u001b[38;5;145m, line 5, in <module>\n\u001b[0m\u001b[1;38;5;59m    1    \u001b[38;5;59m# 讓模型跑在 GPU 上並取得訓練集的分類準確率\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    2    \u001b[38;5;76mdevice\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;26mtorch.device\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188m\"cuda:0\" \u001b[0m\u001b[1;38;5;188mif\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;41mtorch.cuda.is_available\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188melse\u001b[0m\u001b[38;5;188m \"cpu\"\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    3    \u001b[38;5;188mprint\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188m\"device:\"\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;76mdevice\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    4    \u001b[1;38;5;43mmodel\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;178mmodel.to\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;76mdevice\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;188m--> 5    \u001b[1;38;5;44m_\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188macc\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mget_predictions\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;43mmodel\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;32mtrainloader\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mcompute_acc\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[1;38;5;188mTrue\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    6    \u001b[38;5;188mprint\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188m\"classification acc:\"\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188macc\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\u001b[38;5;76m     device = device(type='cpu')\n\u001b[0m\u001b[38;5;26m     torch.device = <class 'torch.device'>\n\u001b[0m\u001b[38;5;41m     torch.cuda.is_available = <function 'is_available' __init__.py:33>\n\u001b[0m\u001b[1;38;5;43m     model = BertForSequenceClassification(\n               (bert): BertModel(\n                 (embeddings): BertEmbeddings(\n                   (word_embeddings): Embedding(21128, 768, padding_idx=0\n              )\n                   (position_embeddings): Embedding(512, 768)\n                   (token_type_embeddings): Embedding(2, 768)\n                   (LayerNorm): BertLayerNorm()\n                   (dropout): Dropout(p=0.1, inplace=False)\n                 )\n                 (encoder): BertEncoder(\n                   (layer): ModuleList(\n                     (0): BertLayer(\n                       (attention): BertAttention(\n                         (self): BertSelfAttention(...\n\u001b[0m\u001b[38;5;178m     model.to = <method 'Module.to' of BertForSequenceClassification(\n                  (bert): BertModel(\n                    (embeddings): BertEmbeddings(\n                      (word_embeddings): Embedding(21128, 768, padding_idx=0\n                 )\n                      (position_embeddings): Embedding(512, 768)\n                      (token_type_embeddings): Embedding(2, 768)\n                      (LayerNorm): BertLayerNorm()\n                      (dropout): Dropout(p=0.1, inplace=False)\n                    )\n                    (encoder): BertEncoder(\n                      (layer): ModuleList(\n                        (0): BertLayer(\n                          (attention): BertAttention(\n                            (self): BertSe...\n\u001b[0m\u001b[1;38;5;44m     _ = tensor(0.9298)\n\u001b[0m\u001b[1;38;5;32m     trainloader = <torch.utils.data.dataloader.DataLoader object at 0x7fc15e0a\n                    2358>\n\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\n\u001b[38;5;145mFile \u001b[0m\u001b[1;38;5;231m<ipython-input-54-a83eda8b4de4>\u001b[0m\u001b[38;5;145m, line 19, in get_predictions\n\u001b[0m\u001b[1;38;5;59m    1    \u001b[1;38;5;188mdef\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mget_predictions\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;43mmodel\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;32mdataloader\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;40mcompute_acc\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[1;38;5;188mFalse\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m (...)\n\u001b[0m\u001b[1;38;5;59m    15   \u001b[38;5;188m            \u001b[0m\u001b[38;5;59m# 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    16   \u001b[38;5;188m            \u001b[0m\u001b[38;5;32mtokens_tensors\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;32msegments_tensors\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;184mmasks_tensors\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;44mdata\u001b[0m\u001b[1;38;5;188m[\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m3\u001b[0m\u001b[1;38;5;188m]\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    17   \u001b[38;5;188m            \u001b[0m\u001b[38;5;20moutputs\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;43mmodel\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188minput_ids\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;32mtokens_tensors\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \n\u001b[0m\u001b[0m\u001b[1;38;5;59m    18   \u001b[38;5;188m                            \u001b[0m\u001b[38;5;188mtoken_type_ids\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;32msegments_tensors\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \n\u001b[0m\u001b[0m\u001b[1;38;5;188m--> 19   \u001b[38;5;188m                            \u001b[0m\u001b[38;5;188mattention_mask\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[1;38;5;184mmasks_tensors\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    20   \u001b[38;5;188m            \n\u001b[0m\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\u001b[38;5;43m     model = BertForSequenceClassification(\n               (bert): BertModel(\n                 (embeddings): BertEmbeddings(\n                   (word_embeddings): Embedding(21128, 768, padding_idx=0\n              )\n                   (position_embeddings): Embedding(512, 768)\n                   (token_type_embeddings): Embedding(2, 768)\n                   (LayerNorm): BertLayerNorm()\n                   (dropout): Dropout(p=0.1, inplace=False)\n                 )\n                 (encoder): BertEncoder(\n                   (layer): ModuleList(\n                     (0): BertLayer(\n                       (attention): BertAttention(\n                         (self): BertSelfAttention(...\n\u001b[0m\u001b[38;5;32m     dataloader = <torch.utils.data.dataloader.DataLoader object at 0x7fc15e0a\n                   2358>\n\u001b[0m\u001b[38;5;40m     compute_acc = True\n\u001b[0m\u001b[38;5;32m     tokens_tensors = tensor([[ 101, 7506, 1744,  ...,    0,    0,    0],\n                              [ 101, 5299, 1745,  ...,    0,    0,    0],\n                              [ 101, 4696, 3633,  ...,    0,    0,    0],\n                              ...,\n                              [ 101, 3360, 2552,  ...,    0,    0,    0],\n                              [ 101, 4692, 1196,  ...,    0,    0,    0],\n                              [ 101, 6821, 6956,  ...,    0,    0,    0]])\n\u001b[0m\u001b[38;5;32m     segments_tensors = tensor([[0, 0, 0,  ..., 0, 0, 0],\n                                [0, 0, 0,  ..., 0, 0, 0],\n                                [0, 0, 0,  ..., 0, 0, 0],\n                                ...,\n                                [0, 0, 0,  ..., 0, 0, 0],\n                                [0, 0, 0,  ..., 0, 0, 0],\n                                [0, 0, 0,  ..., 0, 0, 0]])\n\u001b[0m\u001b[1;38;5;184m     masks_tensors = tensor([[1, 1, 1,  ..., 0, 0, 0],\n                             [1, 1, 1,  ..., 0, 0, 0],\n                             [1, 1, 1,  ..., 0, 0, 0],\n                             ...,\n                             [1, 1, 1,  ..., 0, 0, 0],\n                             [1, 1, 1,  ..., 0, 0, 0],\n                             [1, 1, 1,  ..., 0, 0, 0]])\n\u001b[0m\u001b[38;5;44m     data = (\n             tensor([[ 101, 7506, 1744,  ...,    0,    0,    0],\n                     [ 101, 5299, 1745,  ...,    0,    0,    0],\n                     [ 101, 4696, 3633,  ...,    0,    0,    0],\n                     ...,\n                     [ 101, 3360, 2552,  ...,    0,    0,    0],\n                     [ 101, 4692, 1196,  ...,    0,    0,    0],\n                     [ 101, 6821, 6956,  ...,    0,    0,    0]]), \n             tensor([[0, 0, 0,  ..., 0, 0, 0],\n                     [0, 0, 0,  ..., 0, 0, 0],\n                     [0, 0, 0,  ..., 0, 0, 0],\n                     ...,\n                     [0, 0, 0,  ..., 0, 0, 0],...\n\u001b[0m\u001b[38;5;20m     outputs = tensor([[ 8.8448e-01,  3.7995e-01,  1.2870e+00,  2.5557e-01,\n                 -5.6768e-01,\n                        -3.6111e-02,  8.8271e-02,  5.9909e-01,  1.8637e-01,\n                 -3.7868e-02],\n                       [ 5.4838e-01,  4.3819e-01,  4.5167e-01,  2.4767e-01,\n                 -4.3550e-01,\n                         1.4362e-01,  2.5046e-01,  8.7935e-01,  2.7808e-01,\n                  6.9621e-02],\n                       [ 6.5451e-01,  4.8014e-01,  6.1069e-01, -1.9161e-03,\n                 -6.7293e-01,\n                         3.6241e-01, -2.7624e-01,  9.0345e-01,  3.2011e-01,\n                 -2.8321e-02],\n                       [ 4.5486e-01,  6.7395e-01,  6.5322e-01,  4.62...\n\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\n\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;38;5;231mmodule.py\u001b[0m\u001b[38;5;145m, line 550, in __call__\n\u001b[0m\u001b[1;38;5;59m    540  \u001b[38;5;188m\u001b[0m\u001b[1;38;5;188mdef\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188m__call__\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;43mself\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[1;38;5;112minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[1;38;5;112mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m (...)\n\u001b[0m\u001b[1;38;5;59m    546  \u001b[38;5;188m            \u001b[0m\u001b[1;38;5;112minput\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    547  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188mif\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mtorch._C._get_tracing_state\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    548  \u001b[38;5;188m        \u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;42mself._slow_forward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[1;38;5;112minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[1;38;5;112mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    549  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188melse\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;188m--> 550  \u001b[38;5;188m        \u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;43mself.forward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[1;38;5;112minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[1;38;5;112mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    551  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188mfor\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mhook\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188min\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself._forward_hooks.values\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\u001b[38;5;43m     self = BertForSequenceClassification(\n              (bert): BertModel(\n                (embeddings): BertEmbeddings(\n                  (word_embeddings): Embedding(21128, 768, padding_idx=0\n             )\n                  (position_embeddings): Embedding(512, 768)\n                  (token_type_embeddings): Embedding(2, 768)\n                  (LayerNorm): BertLayerNorm()\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (encoder): BertEncoder(\n                  (layer): ModuleList(\n                    (0): BertLayer(\n                      (attention): BertAttention(\n                        (self): BertSelfAttention(...\n\u001b[0m\u001b[1;38;5;112m     input = ()\n\u001b[0m\u001b[1;38;5;112m     kwargs = {'input_ids': tensor([[ 101, 7506, 1744,  ...,    0,    0,  \n                 0],\n                                    [ 101, 5299, 1745,  ...,    0,    0,  \n                 0],\n                                    [ 101, 4696, 3633,  ...,    0,    0,  \n                 0],\n                                    ...,\n                                    [ 101, 3360, 2552,  ...,    0,    0,  \n                 0],\n                                    [ 101, 4692, 1196,  ...,    0,    0,  \n                 0],\n                                    [ 101, 6821, 6956,  ...,    0,    0,  \n                 0]]),\n               'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],...\n\u001b[0m\u001b[38;5;42m     self._slow_forward = <method 'Module._slow_forward' of BertForSequenceClassificat\n                           ion(\n                            (bert): BertModel(\n                              (embeddings): BertEmbeddings(\n                                (word_embeddings): Embedding(21128, 768, padding_idx=0\n                           )\n                                (position_embeddings): Embedding(512, 768)\n                                (token_type_embeddings): Embedding(2, 768)\n                                (LayerNorm): BertLayerNorm()\n                                (dropout): Dropout(p=0.1, inplace=False)\n                              )\n                              (encoder): BertEncoder(\n                                (layer): ModuleList(\n                                  (0): BertLayer(\n                                    (attention): BertAttention(\n                                      (se...\n\u001b[0m\u001b[1;38;5;43m     self.forward = <method 'BertForSequenceClassification.forward' of BertForSe\n                     quenceClassification(\n                      (bert): BertModel(\n                        (embeddings): BertEmbeddings(\n                          (word_embeddings): Embedding(21128, 768, padding_idx=0\n                     )\n                          (position_embeddings): Embedding(512, 768)\n                          (token_type_embeddings): Embedding(2, 768)\n                          (LayerNorm): BertLayerNorm()\n                          (dropout): Dropout(p=0.1, inplace=False)\n                        )\n                        (encoder): BertEncoder(\n                          (layer): ModuleList(\n                            (0): BertLayer(\n                              (attention): BertAttention...\n\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\n\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/\u001b[0m\u001b[1;38;5;231mmodeling.py\u001b[0m\u001b[38;5;145m, line 989, in forward\n\u001b[0m\u001b[1;38;5;59m    988  \u001b[38;5;188m\u001b[0m\u001b[1;38;5;188mdef\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mforward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;43mself\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;32minput_ids\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;32mtoken_type_ids\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[1;38;5;188mNone\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;184mattention_mask\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[1;38;5;188mNone\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;172mlabels\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[1;38;5;188mNone\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;188m--> 989  \u001b[38;5;188m    \u001b[0m\u001b[38;5;188m_\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mpooled_output\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;148mself.bert\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;32minput_ids\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;32mtoken_type_ids\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;184mattention_mask\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188moutput_all_encoded_layers\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[1;38;5;188mFalse\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    990  \u001b[38;5;188m    \u001b[0m\u001b[38;5;188mpooled_output\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;184mself.dropout\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188mpooled_output\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\u001b[38;5;43m     self = BertForSequenceClassification(\n              (bert): BertModel(\n                (embeddings): BertEmbeddings(\n                  (word_embeddings): Embedding(21128, 768, padding_idx=0\n             )\n                  (position_embeddings): Embedding(512, 768)\n                  (token_type_embeddings): Embedding(2, 768)\n                  (LayerNorm): BertLayerNorm()\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (encoder): BertEncoder(\n                  (layer): ModuleList(\n                    (0): BertLayer(\n                      (attention): BertAttention(\n                        (self): BertSelfAttention(...\n\u001b[0m\u001b[1;38;5;32m     input_ids = tensor([[ 101, 7506, 1744,  ...,    0,    0,    0],\n                         [ 101, 5299, 1745,  ...,    0,    0,    0],\n                         [ 101, 4696, 3633,  ...,    0,    0,    0],\n                         ...,\n                         [ 101, 3360, 2552,  ...,    0,    0,    0],\n                         [ 101, 4692, 1196,  ...,    0,    0,    0],\n                         [ 101, 6821, 6956,  ...,    0,    0,    0]])\n\u001b[0m\u001b[1;38;5;32m     token_type_ids = tensor([[0, 0, 0,  ..., 0, 0, 0],\n                              [0, 0, 0,  ..., 0, 0, 0],\n                              [0, 0, 0,  ..., 0, 0, 0],\n                              ...,\n                              [0, 0, 0,  ..., 0, 0, 0],\n                              [0, 0, 0,  ..., 0, 0, 0],\n                              [0, 0, 0,  ..., 0, 0, 0]])\n\u001b[0m\u001b[1;38;5;184m     attention_mask = tensor([[1, 1, 1,  ..., 0, 0, 0],\n                              [1, 1, 1,  ..., 0, 0, 0],\n                              [1, 1, 1,  ..., 0, 0, 0],\n                              ...,\n                              [1, 1, 1,  ..., 0, 0, 0],\n                              [1, 1, 1,  ..., 0, 0, 0],\n                              [1, 1, 1,  ..., 0, 0, 0]])\n\u001b[0m\u001b[38;5;172m     labels = None\n\u001b[0m\u001b[1;38;5;148m     self.bert = BertModel(\n                   (embeddings): BertEmbeddings(\n                     (word_embeddings): Embedding(21128, 768, padding_idx=0)\n                     (position_embeddings): Embedding(512, 768)\n                     (token_type_embeddings): Embedding(2, 768)\n                     (LayerNorm): BertLayerNorm()\n                     (dropout): Dropout(p=0.1, inplace=False)\n                   )\n                   (encoder): BertEncoder(\n                     (layer): ModuleList(\n                       (0): BertLayer(\n                         (attention): BertAttention(\n                           (self): BertSelfAttention(\n                             (query): Linear(in_features=768, out_features=76\n                  8, bias=True)...\n\u001b[0m\u001b[38;5;184m     self.dropout = Dropout(p=0.1, inplace=False)\n\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\n\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;38;5;231mmodule.py\u001b[0m\u001b[38;5;145m, line 550, in __call__\n\u001b[0m\u001b[1;38;5;59m    540  \u001b[38;5;188m\u001b[0m\u001b[1;38;5;188mdef\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188m__call__\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;148mself\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[1;38;5;112minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[1;38;5;44mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m (...)\n\u001b[0m\u001b[1;38;5;59m    546  \u001b[38;5;188m            \u001b[0m\u001b[1;38;5;112minput\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    547  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188mif\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mtorch._C._get_tracing_state\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    548  \u001b[38;5;188m        \u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;184mself._slow_forward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[1;38;5;112minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[1;38;5;44mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    549  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188melse\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;188m--> 550  \u001b[38;5;188m        \u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;41mself.forward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[1;38;5;112minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[1;38;5;44mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    551  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188mfor\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mhook\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188min\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself._forward_hooks.values\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\u001b[38;5;148m     self = BertModel(\n              (embeddings): BertEmbeddings(\n                (word_embeddings): Embedding(21128, 768, padding_idx=0)\n                (position_embeddings): Embedding(512, 768)\n                (token_type_embeddings): Embedding(2, 768)\n                (LayerNorm): BertLayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (encoder): BertEncoder(\n                (layer): ModuleList(\n                  (0): BertLayer(\n                    (attention): BertAttention(\n                      (self): BertSelfAttention(\n                        (query): Linear(in_features=768, out_features=76\n             8, bias=True)...\n\u001b[0m\u001b[1;38;5;112m     input = (\n              tensor([[ 101, 7506, 1744,  ...,    0,    0,    0],\n                      [ 101, 5299, 1745,  ...,    0,    0,    0],\n                      [ 101, 4696, 3633,  ...,    0,    0,    0],\n                      ...,\n                      [ 101, 3360, 2552,  ...,    0,    0,    0],\n                      [ 101, 4692, 1196,  ...,    0,    0,    0],\n                      [ 101, 6821, 6956,  ...,    0,    0,    0]]), \n              tensor([[0, 0, 0,  ..., 0, 0, 0],\n                      [0, 0, 0,  ..., 0, 0, 0],\n                      [0, 0, 0,  ..., 0, 0, 0],\n                      ...,\n                      [0, 0, 0,  ..., 0, 0, 0],...\n\u001b[0m\u001b[1;38;5;44m     kwargs = {'output_all_encoded_layers': False}\n\u001b[0m\u001b[38;5;184m     self._slow_forward = <method 'Module._slow_forward' of BertModel(\n                            (embeddings): BertEmbeddings(\n                              (word_embeddings): Embedding(21128, 768, padding_idx=0)\n                              (position_embeddings): Embedding(512, 768)\n                              (token_type_embeddings): Embedding(2, 768)\n                              (LayerNorm): BertLayerNorm()\n                              (dropout): Dropout(p=0.1, inplace=False)\n                            )\n                            (encoder): BertEncoder(\n                              (layer): ModuleList(\n                                (0): BertLayer(\n                                  (attention): BertAttention(\n                                    (self): BertSelfAttention(\n                                      (query): Linear(in_features=7...\n\u001b[0m\u001b[1;38;5;41m     self.forward = <method 'BertModel.forward' of BertModel(\n                      (embeddings): BertEmbeddings(\n                        (word_embeddings): Embedding(21128, 768, padding_idx=0)\n                        (position_embeddings): Embedding(512, 768)\n                        (token_type_embeddings): Embedding(2, 768)\n                        (LayerNorm): BertLayerNorm()\n                        (dropout): Dropout(p=0.1, inplace=False)\n                      )\n                      (encoder): BertEncoder(\n                        (layer): ModuleList(\n                          (0): BertLayer(\n                            (attention): BertAttention(\n                              (self): BertSelfAttention(\n                                (query): Linear(in_features=768,...\n\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\n\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/\u001b[0m\u001b[1;38;5;231mmodeling.py\u001b[0m\u001b[38;5;145m, line 733, in forward\n\u001b[0m\u001b[1;38;5;59m    709  \u001b[38;5;188m\u001b[0m\u001b[1;38;5;188mdef\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mforward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;148mself\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;32minput_ids\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;32mtoken_type_ids\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[1;38;5;188mNone\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;184mattention_mask\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[1;38;5;188mNone\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;44moutput_all_encoded_layers\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[1;38;5;188mTrue\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m (...)\n\u001b[0m\u001b[1;38;5;59m    729  \u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    730  \u001b[38;5;188m    \u001b[0m\u001b[38;5;43membedding_output\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;184mself.embeddings\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;32minput_ids\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;32mtoken_type_ids\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    731  \u001b[38;5;188m    \u001b[0m\u001b[38;5;188mencoded_layers\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;148mself.encoder\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;43membedding_output\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    732  \u001b[38;5;188m                                  \u001b[0m\u001b[38;5;40mextended_attention_mask\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;188m--> 733  \u001b[38;5;188m                                  \u001b[0m\u001b[1;38;5;44moutput_all_encoded_layers\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[1;38;5;44moutput_all_encoded_layers\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    734  \u001b[38;5;188m    \u001b[0m\u001b[38;5;188msequence_output\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mencoded_layers\u001b[0m\u001b[1;38;5;188m[\u001b[0m\u001b[1;38;5;188m-\u001b[0m\u001b[38;5;188m1\u001b[0m\u001b[1;38;5;188m]\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\u001b[38;5;148m     self = BertModel(\n              (embeddings): BertEmbeddings(\n                (word_embeddings): Embedding(21128, 768, padding_idx=0)\n                (position_embeddings): Embedding(512, 768)\n                (token_type_embeddings): Embedding(2, 768)\n                (LayerNorm): BertLayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (encoder): BertEncoder(\n                (layer): ModuleList(\n                  (0): BertLayer(\n                    (attention): BertAttention(\n                      (self): BertSelfAttention(\n                        (query): Linear(in_features=768, out_features=76\n             8, bias=True)...\n\u001b[0m\u001b[38;5;32m     input_ids = tensor([[ 101, 7506, 1744,  ...,    0,    0,    0],\n                         [ 101, 5299, 1745,  ...,    0,    0,    0],\n                         [ 101, 4696, 3633,  ...,    0,    0,    0],\n                         ...,\n                         [ 101, 3360, 2552,  ...,    0,    0,    0],\n                         [ 101, 4692, 1196,  ...,    0,    0,    0],\n                         [ 101, 6821, 6956,  ...,    0,    0,    0]])\n\u001b[0m\u001b[38;5;32m     token_type_ids = tensor([[0, 0, 0,  ..., 0, 0, 0],\n                              [0, 0, 0,  ..., 0, 0, 0],\n                              [0, 0, 0,  ..., 0, 0, 0],\n                              ...,\n                              [0, 0, 0,  ..., 0, 0, 0],\n                              [0, 0, 0,  ..., 0, 0, 0],\n                              [0, 0, 0,  ..., 0, 0, 0]])\n\u001b[0m\u001b[38;5;184m     attention_mask = tensor([[1, 1, 1,  ..., 0, 0, 0],\n                              [1, 1, 1,  ..., 0, 0, 0],\n                              [1, 1, 1,  ..., 0, 0, 0],\n                              ...,\n                              [1, 1, 1,  ..., 0, 0, 0],\n                              [1, 1, 1,  ..., 0, 0, 0],\n                              [1, 1, 1,  ..., 0, 0, 0]])\n\u001b[0m\u001b[1;38;5;44m     output_all_encoded_layers = False\n\u001b[0m\u001b[38;5;43m     embedding_output = tensor([[[ 0.0654,  0.0782, -0.2377,  ..., -0.0263, -0.2482,\n                          -0.0000],\n                                 [-0.0000, -0.5316,  0.5501,  ..., -0.0000,  0.2580,\n                           0.4366],\n                                 [ 0.2389, -0.2992, -0.1098,  ..., -0.7885, -0.0000,\n                          -0.3396],\n                                 ...,\n                                 [ 0.3279,  0.2815, -0.7743,  ...,  2.1764, -0.3332,\n                           0.2405],\n                                 [ 0.0000,  0.4282, -1.0950,  ...,  0.0000,  0.5362,\n                           0.1182],\n                                 [-0.0000,  0.1157, -0.6062,  ...,  1.7547, -0.0000,\n                           0.2400]],\n                        \n                                [[ 0.0654,  0.0782, -0.2377,  ..., -0.0000, -0.248..\n                         .\n\u001b[0m\u001b[38;5;184m     self.embeddings = BertEmbeddings(\n                         (word_embeddings): Embedding(21128, 768, padding_idx=0)\n                         (position_embeddings): Embedding(512, 768)\n                         (token_type_embeddings): Embedding(2, 768)\n                         (LayerNorm): BertLayerNorm()\n                         (dropout): Dropout(p=0.1, inplace=False)\n                       )\n\u001b[0m\u001b[38;5;148m     self.encoder = BertEncoder(\n                      (layer): ModuleList(\n                        (0): BertLayer(\n                          (attention): BertAttention(\n                            (self): BertSelfAttention(\n                              (query): Linear(in_features=768, out_features=768,\n                      bias=True)\n                              (key): Linear(in_features=768, out_features=768, b\n                     ias=True)\n                              (value): Linear(in_features=768, out_features=768,\n                      bias=True)\n                              (dropout): Dropout(p=0.1, inplace=False)\n                            )\n                            (output): BertSelfOutput(\n                              (dense): Linear(in_features=768, out_features=768,\n                      bias=...\n\u001b[0m\u001b[38;5;40m     extended_attention_mask = tensor([[[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                                , -10000.]]],\n                               \n                               \n                                       [[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                                , -10000.]]],\n                               \n                               \n                                       [[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                                , -10000.]]],\n                               \n                               \n                                       ...,\n                               \n                               \n                                       [[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                                , -10000.]]],\n                               \n                               \n                                       [[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                                , -10000.]]],\n                               \n                               \n                                       [[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                                , -10000.]]]])\n\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\n\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;38;5;231mmodule.py\u001b[0m\u001b[38;5;145m, line 550, in __call__\n\u001b[0m\u001b[1;38;5;59m    540  \u001b[38;5;188m\u001b[0m\u001b[1;38;5;188mdef\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188m__call__\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;148mself\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[1;38;5;184minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[1;38;5;20mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m (...)\n\u001b[0m\u001b[1;38;5;59m    546  \u001b[38;5;188m            \u001b[0m\u001b[1;38;5;184minput\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    547  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188mif\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mtorch._C._get_tracing_state\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    548  \u001b[38;5;188m        \u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;26mself._slow_forward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[1;38;5;184minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[1;38;5;20mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    549  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188melse\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;188m--> 550  \u001b[38;5;188m        \u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;112mself.forward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[1;38;5;184minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[1;38;5;20mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    551  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188mfor\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mhook\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188min\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself._forward_hooks.values\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\u001b[38;5;148m     self = BertEncoder(\n              (layer): ModuleList(\n                (0): BertLayer(\n                  (attention): BertAttention(\n                    (self): BertSelfAttention(\n                      (query): Linear(in_features=768, out_features=768,\n              bias=True)\n                      (key): Linear(in_features=768, out_features=768, b\n             ias=True)\n                      (value): Linear(in_features=768, out_features=768,\n              bias=True)\n                      (dropout): Dropout(p=0.1, inplace=False)\n                    )\n                    (output): BertSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768,\n              bias=...\n\u001b[0m\u001b[1;38;5;184m     input = (\n              tensor([[[ 0.0654,  0.0782, -0.2377,  ..., -0.0263, -0.2482\n              , -0.0000],\n                       [-0.0000, -0.5316,  0.5501,  ..., -0.0000,  0.2580\n              ,  0.4366],\n                       [ 0.2389, -0.2992, -0.1098,  ..., -0.7885, -0.0000\n              , -0.3396],\n                       ...,\n                       [ 0.3279,  0.2815, -0.7743,  ...,  2.1764, -0.3332\n              ,  0.2405],\n                       [ 0.0000,  0.4282, -1.0950,  ...,  0.0000,  0.5362\n              ,  0.1182],\n                       [-0.0000,  0.1157, -0.6062,  ...,  1.7547, -0.0000\n              ,  0.2400]],\n              \n                      [[ 0.0654,  0.0782, -0.2377,  ..., -0.0...\n\u001b[0m\u001b[1;38;5;20m     kwargs = {'output_all_encoded_layers': False}\n\u001b[0m\u001b[38;5;26m     self._slow_forward = <method 'Module._slow_forward' of BertEncoder(\n                            (layer): ModuleList(\n                              (0): BertLayer(\n                                (attention): BertAttention(\n                                  (self): BertSelfAttention(\n                                    (query): Linear(in_features=768, out_features=768,\n                            bias=True)\n                                    (key): Linear(in_features=768, out_features=768, b\n                           ias=True)\n                                    (value): Linear(in_features=768, out_features=768,\n                            bias=True)\n                                    (dropout): Dropout(p=0.1, inplace=False)\n                                  )\n                                  (output): BertSelfOutput(\n                                    (dense): Linear(in_fea...\n\u001b[0m\u001b[1;38;5;112m     self.forward = <method 'BertEncoder.forward' of BertEncoder(\n                      (layer): ModuleList(\n                        (0): BertLayer(\n                          (attention): BertAttention(\n                            (self): BertSelfAttention(\n                              (query): Linear(in_features=768, out_features=768,\n                      bias=True)\n                              (key): Linear(in_features=768, out_features=768, b\n                     ias=True)\n                              (value): Linear(in_features=768, out_features=768,\n                      bias=True)\n                              (dropout): Dropout(p=0.1, inplace=False)\n                            )\n                            (output): BertSelfOutput(\n                              (dense): Linear(in_feat...\n\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\n\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/\u001b[0m\u001b[1;38;5;231mmodeling.py\u001b[0m\u001b[38;5;145m, line 406, in forward\n\u001b[0m\u001b[1;38;5;59m    403  \u001b[38;5;188m\u001b[0m\u001b[1;38;5;188mdef\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mforward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;148mself\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;42mhidden_states\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;40mattention_mask\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;44moutput_all_encoded_layers\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[1;38;5;188mTrue\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    404  \u001b[38;5;188m    \u001b[0m\u001b[38;5;40mall_encoder_layers\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m[\u001b[0m\u001b[1;38;5;188m]\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    405  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188mfor\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;148mlayer_module\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188min\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;42mself.layer\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;188m--> 406  \u001b[38;5;188m        \u001b[0m\u001b[1;38;5;42mhidden_states\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;148mlayer_module\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;42mhidden_states\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;40mattention_mask\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    407  \u001b[38;5;188m        \u001b[0m\u001b[1;38;5;188mif\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;44moutput_all_encoded_layers\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\u001b[38;5;148m     self = BertEncoder(\n              (layer): ModuleList(\n                (0): BertLayer(\n                  (attention): BertAttention(\n                    (self): BertSelfAttention(\n                      (query): Linear(in_features=768, out_features=768,\n              bias=True)\n                      (key): Linear(in_features=768, out_features=768, b\n             ias=True)\n                      (value): Linear(in_features=768, out_features=768,\n              bias=True)\n                      (dropout): Dropout(p=0.1, inplace=False)\n                    )\n                    (output): BertSelfOutput(\n                      (dense): Linear(in_features=768, out_features=768,\n              bias=...\n\u001b[0m\u001b[1;38;5;42m     hidden_states = tensor([[[ 6.3718e-01, -2.9755e-01, -2.2887e-01,  ...,  9.19\n                      26e-01,\n                                3.9297e-01, -4.1448e-01],\n                              [ 6.8257e-02, -8.0330e-01,  6.3477e-02,  ...,  8.47\n                      29e-01,\n                                5.9033e-01, -4.9242e-01],\n                              [ 1.5515e-01,  5.4363e-02, -1.2653e+00,  ...,  5.69\n                      11e-01,\n                                1.3152e+00, -5.8674e-01],\n                              ...,\n                              [ 2.4433e-01,  1.4039e-01, -8.3382e-01,  ...,  2.39\n                      97e-01,\n                               -5.5139e-01, -4.0470e-01],\n                              [ 1.0313e-01, -7.5655e-01, -5.9051e-01,  ..., -3.33\n                      01e-01...\n\u001b[0m\u001b[1;38;5;40m     attention_mask = tensor([[[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                       , -10000.]]],\n                      \n                      \n                              [[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                       , -10000.]]],\n                      \n                      \n                              [[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                       , -10000.]]],\n                      \n                      \n                              ...,\n                      \n                      \n                              [[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                       , -10000.]]],\n                      \n                      \n                              [[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                       , -10000.]]],\n                      \n                      \n                              [[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                       , -10000.]]]])\n\u001b[0m\u001b[38;5;44m     output_all_encoded_layers = False\n\u001b[0m\u001b[38;5;40m     all_encoder_layers = []\n\u001b[0m\u001b[1;38;5;148m     layer_module = BertLayer(\n                      (attention): BertAttention(\n                        (self): BertSelfAttention(\n                          (query): Linear(in_features=768, out_features=768, bia\n                     s=True)\n                          (key): Linear(in_features=768, out_features=768, bias=\n                     True)\n                          (value): Linear(in_features=768, out_features=768, bia\n                     s=True)\n                          (dropout): Dropout(p=0.1, inplace=False)\n                        )\n                        (output): BertSelfOutput(\n                          (dense): Linear(in_features=768, out_features=768, bia\n                     s=True)\n                          (LayerNorm): BertLayerNorm()\n                          (dropout): Dropout(p=0.1, inplace=...\n\u001b[0m\u001b[38;5;42m     self.layer = ModuleList(\n                    (0): BertLayer(\n                      (attention): BertAttention(\n                        (self): BertSelfAttention(\n                          (query): Linear(in_features=768, out_features=768, b\n                   ias=True)\n                          (key): Linear(in_features=768, out_features=768, bia\n                   s=True)\n                          (value): Linear(in_features=768, out_features=768, b\n                   ias=True)\n                          (dropout): Dropout(p=0.1, inplace=False)\n                        )\n                        (output): BertSelfOutput(\n                          (dense): Linear(in_features=768, out_features=768, b\n                   ias=True)\n                          (LayerNorm): BertLayerNorm()...\n\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\n\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;38;5;231mmodule.py\u001b[0m\u001b[38;5;145m, line 550, in __call__\n\u001b[0m\u001b[1;38;5;59m    540  \u001b[38;5;188m\u001b[0m\u001b[1;38;5;188mdef\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188m__call__\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;148mself\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[1;38;5;26minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[1;38;5;44mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m (...)\n\u001b[0m\u001b[1;38;5;59m    546  \u001b[38;5;188m            \u001b[0m\u001b[1;38;5;26minput\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    547  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188mif\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mtorch._C._get_tracing_state\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    548  \u001b[38;5;188m        \u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;41mself._slow_forward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[1;38;5;26minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[1;38;5;44mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    549  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188melse\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;188m--> 550  \u001b[38;5;188m        \u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;148mself.forward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[1;38;5;26minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[1;38;5;44mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    551  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188mfor\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mhook\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188min\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself._forward_hooks.values\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\u001b[38;5;148m     self = BertLayer(\n              (attention): BertAttention(\n                (self): BertSelfAttention(\n                  (query): Linear(in_features=768, out_features=768, bia\n             s=True)\n                  (key): Linear(in_features=768, out_features=768, bias=\n             True)\n                  (value): Linear(in_features=768, out_features=768, bia\n             s=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bia\n             s=True)\n                  (LayerNorm): BertLayerNorm()\n                  (dropout): Dropout(p=0.1, inplace=...\n\u001b[0m\u001b[1;38;5;26m     input = (\n              tensor([[[ 6.3718e-01, -2.9755e-01, -2.2887e-01,  ...,  9.1\n              926e-01,\n                         3.9297e-01, -4.1448e-01],\n                       [ 6.8257e-02, -8.0330e-01,  6.3477e-02,  ...,  8.4\n              729e-01,\n                         5.9033e-01, -4.9242e-01],\n                       [ 1.5515e-01,  5.4363e-02, -1.2653e+00,  ...,  5.6\n              911e-01,\n                         1.3152e+00, -5.8674e-01],\n                       ...,\n                       [ 2.4433e-01,  1.4039e-01, -8.3382e-01,  ...,  2.3\n              997e-01,\n                        -5.5139e-01, -4.0470e-01],\n                       [ 1.0313e-01, -7.5655e-01, -5.9051e-01,  ...,...\n\u001b[0m\u001b[1;38;5;44m     kwargs = {}\n\u001b[0m\u001b[38;5;41m     self._slow_forward = <method 'Module._slow_forward' of BertLayer(\n                            (attention): BertAttention(\n                              (self): BertSelfAttention(\n                                (query): Linear(in_features=768, out_features=768, bia\n                           s=True)\n                                (key): Linear(in_features=768, out_features=768, bias=\n                           True)\n                                (value): Linear(in_features=768, out_features=768, bia\n                           s=True)\n                                (dropout): Dropout(p=0.1, inplace=False)\n                              )\n                              (output): BertSelfOutput(\n                                (dense): Linear(in_features=768, out_features=768, bia\n                           s=True)\n                                (LayerNorm): BertLayerNorm()...\n\u001b[0m\u001b[1;38;5;148m     self.forward = <method 'BertLayer.forward' of BertLayer(\n                      (attention): BertAttention(\n                        (self): BertSelfAttention(\n                          (query): Linear(in_features=768, out_features=768, bia\n                     s=True)\n                          (key): Linear(in_features=768, out_features=768, bias=\n                     True)\n                          (value): Linear(in_features=768, out_features=768, bia\n                     s=True)\n                          (dropout): Dropout(p=0.1, inplace=False)\n                        )\n                        (output): BertSelfOutput(\n                          (dense): Linear(in_features=768, out_features=768, bia\n                     s=True)\n                          (LayerNorm): BertLayerNorm()\n                          (dr...\n\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\n\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/\u001b[0m\u001b[1;38;5;231mmodeling.py\u001b[0m\u001b[38;5;145m, line 392, in forward\n\u001b[0m\u001b[1;38;5;59m    390  \u001b[38;5;188m\u001b[0m\u001b[1;38;5;188mdef\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mforward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;148mself\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;42mhidden_states\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;40mattention_mask\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    391  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;112mattention_output\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;32mself.attention\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;42mhidden_states\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;40mattention_mask\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;188m--> 392  \u001b[38;5;188m    \u001b[0m\u001b[38;5;188mintermediate_output\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;112mself.intermediate\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;112mattention_output\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    393  \u001b[38;5;188m    \u001b[0m\u001b[38;5;188mlayer_output\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;32mself.output\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188mintermediate_output\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;112mattention_output\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\u001b[38;5;148m     self = BertLayer(\n              (attention): BertAttention(\n                (self): BertSelfAttention(\n                  (query): Linear(in_features=768, out_features=768, bia\n             s=True)\n                  (key): Linear(in_features=768, out_features=768, bias=\n             True)\n                  (value): Linear(in_features=768, out_features=768, bia\n             s=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bia\n             s=True)\n                  (LayerNorm): BertLayerNorm()\n                  (dropout): Dropout(p=0.1, inplace=...\n\u001b[0m\u001b[38;5;42m     hidden_states = tensor([[[ 6.3718e-01, -2.9755e-01, -2.2887e-01,  ...,  9.19\n                      26e-01,\n                                3.9297e-01, -4.1448e-01],\n                              [ 6.8257e-02, -8.0330e-01,  6.3477e-02,  ...,  8.47\n                      29e-01,\n                                5.9033e-01, -4.9242e-01],\n                              [ 1.5515e-01,  5.4363e-02, -1.2653e+00,  ...,  5.69\n                      11e-01,\n                                1.3152e+00, -5.8674e-01],\n                              ...,\n                              [ 2.4433e-01,  1.4039e-01, -8.3382e-01,  ...,  2.39\n                      97e-01,\n                               -5.5139e-01, -4.0470e-01],\n                              [ 1.0313e-01, -7.5655e-01, -5.9051e-01,  ..., -3.33\n                      01e-01...\n\u001b[0m\u001b[38;5;40m     attention_mask = tensor([[[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                       , -10000.]]],\n                      \n                      \n                              [[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                       , -10000.]]],\n                      \n                      \n                              [[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                       , -10000.]]],\n                      \n                      \n                              ...,\n                      \n                      \n                              [[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                       , -10000.]]],\n                      \n                      \n                              [[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                       , -10000.]]],\n                      \n                      \n                              [[[    -0.,     -0.,     -0.,  ..., -10000., -10000.\n                       , -10000.]]]])\n\u001b[0m\u001b[1;38;5;112m     attention_output = tensor([[[ 1.1472e+00, -3.1780e-01, -6.8597e-01,  ...,  1.05\n                         10e+00,\n                                   3.5034e-01, -6.8905e-01],\n                                 [ 4.5880e-01, -9.5087e-01, -2.2360e-01,  ...,  7.87\n                         68e-01,\n                                   7.0068e-01, -6.0346e-01],\n                                 [ 5.2295e-01,  1.6633e-02, -1.7326e+00,  ...,  3.50\n                         87e-01,\n                                   1.8198e+00, -7.1184e-01],\n                                 ...,\n                                 [ 2.7932e-01,  8.3819e-02, -6.6686e-01,  ...,  1.89\n                         37e-01,\n                                  -1.9095e-01, -3.8282e-01],\n                                 [ 2.0040e-01, -3.4717e-01, -5.2290e-01,  ..., -1.44\n                         92e-01...\n\u001b[0m\u001b[38;5;32m     self.attention = BertAttention(\n                        (self): BertSelfAttention(\n                          (query): Linear(in_features=768, out_features=768, bias=\n                       True)\n                          (key): Linear(in_features=768, out_features=768, bias=Tr\n                       ue)\n                          (value): Linear(in_features=768, out_features=768, bias=\n                       True)\n                          (dropout): Dropout(p=0.1, inplace=False)\n                        )\n                        (output): BertSelfOutput(\n                          (dense): Linear(in_features=768, out_features=768, bias=\n                       True)\n                          (LayerNorm): BertLayerNorm()\n                          (dropout): Dropout(p=0.1, inplace=False)\n                        )\n                      )\n\u001b[0m\u001b[1;38;5;112m     self.intermediate = BertIntermediate(\n                           (dense): Linear(in_features=768, out_features=3072, bias=T\n                          rue)\n                         )\n\u001b[0m\u001b[38;5;32m     self.output = BertOutput(\n                     (dense): Linear(in_features=3072, out_features=768, bias=T\n                    rue)\n                     (LayerNorm): BertLayerNorm()\n                     (dropout): Dropout(p=0.1, inplace=False)\n                   )\n\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\n\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;38;5;231mmodule.py\u001b[0m\u001b[38;5;145m, line 550, in __call__\n\u001b[0m\u001b[1;38;5;59m    540  \u001b[38;5;188m\u001b[0m\u001b[1;38;5;188mdef\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188m__call__\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;112mself\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[1;38;5;76minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[1;38;5;20mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m (...)\n\u001b[0m\u001b[1;38;5;59m    546  \u001b[38;5;188m            \u001b[0m\u001b[1;38;5;76minput\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    547  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188mif\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mtorch._C._get_tracing_state\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    548  \u001b[38;5;188m        \u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;172mself._slow_forward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[1;38;5;76minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[1;38;5;20mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    549  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188melse\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;188m--> 550  \u001b[38;5;188m        \u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;56mself.forward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[1;38;5;76minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[1;38;5;20mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    551  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188mfor\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mhook\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188min\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself._forward_hooks.values\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\u001b[38;5;112m     self = BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=T\n             rue)\n            )\n\u001b[0m\u001b[1;38;5;76m     input = (\n              tensor([[[ 1.1472e+00, -3.1780e-01, -6.8597e-01,  ...,  1.0\n              510e+00,\n                         3.5034e-01, -6.8905e-01],\n                       [ 4.5880e-01, -9.5087e-01, -2.2360e-01,  ...,  7.8\n              768e-01,\n                         7.0068e-01, -6.0346e-01],\n                       [ 5.2295e-01,  1.6633e-02, -1.7326e+00,  ...,  3.5\n              087e-01,\n                         1.8198e+00, -7.1184e-01],\n                       ...,\n                       [ 2.7932e-01,  8.3819e-02, -6.6686e-01,  ...,  1.8\n              937e-01,\n                        -1.9095e-01, -3.8282e-01],\n                       [ 2.0040e-01, -3.4717e-01, -5.2290e-01,  ...,...\n\u001b[0m\u001b[1;38;5;20m     kwargs = {}\n\u001b[0m\u001b[38;5;172m     self._slow_forward = <method 'Module._slow_forward' of BertIntermediate(\n                            (dense): Linear(in_features=768, out_features=3072, bias=T\n                           rue)\n                          ) module.py:521>\n\u001b[0m\u001b[1;38;5;56m     self.forward = <method 'BertIntermediate.forward' of BertIntermediate(\n                      (dense): Linear(in_features=768, out_features=3072, bias=T\n                     rue)\n                    ) modeling.py:363>\n\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\n\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/\u001b[0m\u001b[1;38;5;231mmodeling.py\u001b[0m\u001b[38;5;145m, line 365, in forward\n\u001b[0m\u001b[1;38;5;59m    363  \u001b[38;5;188m\u001b[0m\u001b[1;38;5;188mdef\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mforward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;112mself\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;166mhidden_states\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    364  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;166mhidden_states\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;43mself.dense\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;166mhidden_states\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;188m--> 365  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;166mhidden_states\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;184mself.intermediate_act_fn\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;166mhidden_states\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    366  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188mreturn\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;166mhidden_states\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\u001b[38;5;112m     self = BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=T\n             rue)\n            )\n\u001b[0m\u001b[1;38;5;166m     hidden_states = tensor([[[-1.2541, -1.0404, -1.6908,  ..., -1.3791, -1.4585,\n                       -2.5114],\n                              [-1.0610, -1.9483, -1.2200,  ..., -0.1421, -2.4976,\n                       -1.2685],\n                              [-2.4276, -1.6593, -1.3430,  ...,  0.0487, -2.2074,\n                       -1.4065],\n                              ...,\n                              [-1.1773, -0.7174, -0.8320,  ..., -0.1304, -1.0031,\n                       -0.1444],\n                              [-1.3185, -0.8409, -0.4471,  ..., -0.4518, -1.0443,\n                       -1.4319],\n                              [-1.3008, -1.2552, -0.0529,  ..., -0.1717, -0.3281,\n                       -1.3946]],\n                     \n                             [[-1.3728, -1.4712, -0.7931,  ..., -1.1037, -1.875..\n                      .\n\u001b[0m\u001b[38;5;43m     self.dense = Linear(in_features=768, out_features=3072, bias=True)\n\u001b[0m\u001b[1;38;5;184m     self.intermediate_act_fn = <function 'gelu' modeling.py:118>\n\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\n\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/\u001b[0m\u001b[1;38;5;231mmodeling.py\u001b[0m\u001b[38;5;145m, line 124, in gelu\n\u001b[0m\u001b[1;38;5;59m    118  \u001b[1;38;5;188mdef\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mgelu\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;166mx\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m:\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m (...)\n\u001b[0m\u001b[1;38;5;59m    120  \u001b[38;5;188m        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    121  \u001b[38;5;188m        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    122  \u001b[38;5;188m        Also see https://arxiv.org/abs/1606.08415\n\u001b[0m\u001b[0m\u001b[1;38;5;59m    123  \u001b[38;5;188m    \"\"\"\n\u001b[0m\u001b[0m\u001b[1;38;5;188m--> 124  \u001b[38;5;188m    \u001b[0m\u001b[1;38;5;188mreturn\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;166mx\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[38;5;188m 0.5 \u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188m1.0 \u001b[0m\u001b[1;38;5;188m+\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mtorch.erf\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;166mx\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m/\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mmath.sqrt\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188m2.0\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\u001b[1;38;5;166m     x = tensor([[[-1.2541, -1.0404, -1.6908,  ..., -1.3791, -1.4585,\n           -2.5114],\n                  [-1.0610, -1.9483, -1.2200,  ..., -0.1421, -2.4976,\n           -1.2685],\n                  [-2.4276, -1.6593, -1.3430,  ...,  0.0487, -2.2074,\n           -1.4065],\n                  ...,\n                  [-1.1773, -0.7174, -0.8320,  ..., -0.1304, -1.0031,\n           -0.1444],\n                  [-1.3185, -0.8409, -0.4471,  ..., -0.4518, -1.0443,\n           -1.4319],\n                  [-1.3008, -1.2552, -0.0529,  ..., -0.1717, -0.3281,\n           -1.3946]],\n         \n                 [[-1.3728, -1.4712, -0.7931,  ..., -1.1037, -1.875..\n          .\n\u001b[0m\u001b[38;5;102m    ..................................................\u001b[0m\n\n---- (full traceback above) ----\n\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/IPython/core/\u001b[0m\u001b[1;38;5;231minteractiveshell.py\u001b[0m\u001b[38;5;145m, line 3325, in run_code\n\u001b[0m    \u001b[38;5;188m\u001b[0m\u001b[38;5;188mexec\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188mcode_obj\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself.user_global_ns\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself.user_ns\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[38;5;145mFile \u001b[0m\u001b[1;38;5;231m<ipython-input-55-aab6d4e4b2f6>\u001b[0m\u001b[38;5;145m, line 5, in <module>\n\u001b[0m    \u001b[38;5;188m_\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188macc\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mget_predictions\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188mmodel\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mtrainloader\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mcompute_acc\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[1;38;5;188mTrue\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[38;5;145mFile \u001b[0m\u001b[1;38;5;231m<ipython-input-54-a83eda8b4de4>\u001b[0m\u001b[38;5;145m, line 19, in get_predictions\n\u001b[0m    \u001b[38;5;188m\u001b[0m\u001b[38;5;188mattention_mask\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188mmasks_tensors\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;38;5;231mmodule.py\u001b[0m\u001b[38;5;145m, line 550, in __call__\n\u001b[0m    \u001b[38;5;188m\u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself.forward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[38;5;188minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[38;5;188mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/\u001b[0m\u001b[1;38;5;231mmodeling.py\u001b[0m\u001b[38;5;145m, line 989, in forward\n\u001b[0m    \u001b[38;5;188m\u001b[0m\u001b[38;5;188m_\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mpooled_output\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself.bert\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188minput_ids\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mtoken_type_ids\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mattention_mask\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188moutput_all_encoded_layers\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[1;38;5;188mFalse\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;38;5;231mmodule.py\u001b[0m\u001b[38;5;145m, line 550, in __call__\n\u001b[0m    \u001b[38;5;188m\u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself.forward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[38;5;188minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[38;5;188mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/\u001b[0m\u001b[1;38;5;231mmodeling.py\u001b[0m\u001b[38;5;145m, line 733, in forward\n\u001b[0m    \u001b[38;5;188m\u001b[0m\u001b[38;5;188moutput_all_encoded_layers\u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188moutput_all_encoded_layers\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;38;5;231mmodule.py\u001b[0m\u001b[38;5;145m, line 550, in __call__\n\u001b[0m    \u001b[38;5;188m\u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself.forward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[38;5;188minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[38;5;188mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/\u001b[0m\u001b[1;38;5;231mmodeling.py\u001b[0m\u001b[38;5;145m, line 406, in forward\n\u001b[0m    \u001b[38;5;188m\u001b[0m\u001b[38;5;188mhidden_states\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mlayer_module\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188mhidden_states\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mattention_mask\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;38;5;231mmodule.py\u001b[0m\u001b[38;5;145m, line 550, in __call__\n\u001b[0m    \u001b[38;5;188m\u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself.forward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[38;5;188minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[38;5;188mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/\u001b[0m\u001b[1;38;5;231mmodeling.py\u001b[0m\u001b[38;5;145m, line 392, in forward\n\u001b[0m    \u001b[38;5;188m\u001b[0m\u001b[38;5;188mintermediate_output\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself.intermediate\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188mattention_output\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;38;5;231mmodule.py\u001b[0m\u001b[38;5;145m, line 550, in __call__\n\u001b[0m    \u001b[38;5;188m\u001b[0m\u001b[38;5;188mresult\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself.forward\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[38;5;188minput\u001b[0m\u001b[1;38;5;188m,\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m**\u001b[0m\u001b[38;5;188mkwargs\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/\u001b[0m\u001b[1;38;5;231mmodeling.py\u001b[0m\u001b[38;5;145m, line 365, in forward\n\u001b[0m    \u001b[38;5;188m\u001b[0m\u001b[38;5;188mhidden_states\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m=\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mself.intermediate_act_fn\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188mhidden_states\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\u001b[38;5;145mFile /Library/anaconda3/lib/python3.7/site-packages/pytorch_pretrained_bert/\u001b[0m\u001b[1;38;5;231mmodeling.py\u001b[0m\u001b[38;5;145m, line 124, in gelu\n\u001b[0m    \u001b[38;5;188m\u001b[0m\u001b[1;38;5;188mreturn\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mx\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[38;5;188m 0.5 \u001b[0m\u001b[1;38;5;188m*\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188m1.0 \u001b[0m\u001b[1;38;5;188m+\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mtorch.erf\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188mx\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[1;38;5;188m/\u001b[0m\u001b[38;5;188m \u001b[0m\u001b[38;5;188mmath.sqrt\u001b[0m\u001b[1;38;5;188m(\u001b[0m\u001b[38;5;188m2.0\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[1;38;5;188m)\u001b[0m\u001b[38;5;188m\n\u001b[0m\n\u001b[1;38;5;160mKeyboardInterrupt\u001b[0m\u001b[1;38;5;160m\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 0\n",
      "00:25:53.614712 call        14     def __getitem__(self, idx):\n",
      "00:25:53.615002 line        15         if self.mode == \"test\":\n",
      "00:25:53.615056 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '古力娜扎再次成为焦点，这一身招摇大方，掳获了网友们的心'\n",
      "New var:....... text_b = '古力娜扎,粉丝'\n",
      "New var:....... label = 0\n",
      "00:25:53.615658 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.615750 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.615883 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.616070 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['古', '力', '娜', '扎', '再', '次', '成', '为', '焦', '点...'方', '，', '掳', '获', '了', '网', '友', '们', '的', '心']\n",
      "00:25:53.616801 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '古', '力', '娜', '扎', '再', '次', '成', '为'... '掳', '获', '了', '网', '友', '们', '的', '心', '[SEP]']\n",
      "00:25:53.617833 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:53.618382 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['古', '力', '娜', '扎', ',', '粉', '丝']\n",
      "00:25:53.618719 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '古', '力', '娜', '扎', '再', '次', '成', '为'...EP]', '古', '力', '娜', '扎', ',', '粉', '丝', '[SEP]']\n",
      "00:25:53.618982 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 8\n",
      "00:25:53.619214 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1367, 1213, 2025, 2799, 1086, 3613, 2768, ...102, 1367, 1213, 2025, 2799, 117, 5106, 692, 102]\n",
      "00:25:53.619442 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1367, 1213, 2025, 2799, 1086, 3613...1213, 2025, 2799,  117, 5106,  692,         102])\n",
      "00:25:53.619789 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.620381 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0,        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.621008 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.621785 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1367, 1213, 2025, 2799, 1086, 361... 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008993\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 1\n",
      "00:25:53.623747 call        14     def __getitem__(self, idx):\n",
      "00:25:53.623830 line        15         if self.mode == \"test\":\n",
      "00:25:53.623868 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '包贝尔带娇妻外出就餐被拍，大家把注意力放在了第3张！'\n",
      "New var:....... text_b = '娇妻,娇妻外出就餐,包贝尔'\n",
      "New var:....... label = 0\n",
      "00:25:53.624568 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.624685 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.624780 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.625069 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['包', '贝', '尔', '带', '娇', '妻', '外', '出', '就', '餐...'注', '意', '力', '放', '在', '了', '第', '3', '张', '！']\n",
      "00:25:53.625879 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '包', '贝', '尔', '带', '娇', '妻', '外', '出'... '力', '放', '在', '了', '第', '3', '张', '！', '[SEP]']\n",
      "00:25:53.626300 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:53.626506 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['娇', '妻', ',', '娇', '妻', '外', '出', '就', '餐', ',', '包', '贝', '尔']\n",
      "00:25:53.626947 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '包', '贝', '尔', '带', '娇', '妻', '外', '出'... '外', '出', '就', '餐', ',', '包', '贝', '尔', '[SEP]']\n",
      "00:25:53.627214 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "00:25:53.627407 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1259, 6564, 2209, 2372, 2019, 1988, 1912, ...12, 1139, 2218, 7623, 117, 1259, 6564, 2209, 102]\n",
      "00:25:53.627613 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1259, 6564, 2209, 2372, 2019, 1988...2218,        7623,  117, 1259, 6564, 2209,  102])\n",
      "00:25:53.627890 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.628448 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.629002 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.630017 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1259, 6564, 2209, 2372, 2019, 198... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008083\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 2\n",
      "00:25:53.631862 call        14     def __getitem__(self, idx):\n",
      "00:25:53.631934 line        15         if self.mode == \"test\":\n",
      "00:25:53.631972 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '娱乐圈娶了豪门的5位男星，事业开挂，最后一位想离婚门都没有'\n",
      "New var:....... text_b = '豪门,迟重瑞,周立波,吕良伟,石贞善'\n",
      "New var:....... label = 0\n",
      "00:25:53.632561 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.632738 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.632825 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.633085 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['娱', '乐', '圈', '娶', '了', '豪', '门', '的', '5', '位...'后', '一', '位', '想', '离', '婚', '门', '都', '没', '有']\n",
      "00:25:53.633760 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '娶', '了', '豪', '门', '的'... '位', '想', '离', '婚', '门', '都', '没', '有', '[SEP]']\n",
      "00:25:53.633892 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:53.634178 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['豪', '门', ',', '迟', '重', '瑞', ',', '周', '立', '波', ',', '吕', '良', '伟', ',', '石', '贞', '善']\n",
      "00:25:53.634825 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '娶', '了', '豪', '门', '的'... ',', '吕', '良', '伟', ',', '石', '贞', '善', '[SEP]']\n",
      "00:25:53.635484 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:25:53.636182 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2031, 727, 1750, 2034, 749, 6498, 7305, 46...117, 1406, 5679, 836, 117, 4767, 6565, 1587, 102]\n",
      "00:25:53.636895 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2031,  727, 1750, 2034,  749, 6498...5679,  836,  117, 4767, 6565,        1587,  102])\n",
      "00:25:53.637299 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.638750 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:25:53.639789 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.641324 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2031,  727, 1750, 2034,  749, 649...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011721\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 3\n",
      "00:25:53.643624 call        14     def __getitem__(self, idx):\n",
      "00:25:53.643706 line        15         if self.mode == \"test\":\n",
      "00:25:53.643745 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '陈学冬参加节目被爆童年照，果然胖子都是潜力股啊！'\n",
      "New var:....... text_b = '薛之谦,陈学冬,谭维维,跨界歌王'\n",
      "New var:....... label = 0\n",
      "00:25:53.644344 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.644446 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.644620 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.644948 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['陈', '学', '冬', '参', '加', '节', '目', '被', '爆', '童...'然', '胖', '子', '都', '是', '潜', '力', '股', '啊', '！']\n",
      "00:25:53.645621 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '学', '冬', '参', '加', '节', '目', '被'... '子', '都', '是', '潜', '力', '股', '啊', '！', '[SEP]']\n",
      "00:25:53.646247 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:53.646629 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['薛', '之', '谦', ',', '陈', '学', '冬', ',', '谭', '维', '维', ',', '跨', '界', '歌', '王']\n",
      "00:25:53.647126 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '学', '冬', '参', '加', '节', '目', '被'... '谭', '维', '维', ',', '跨', '界', '歌', '王', '[SEP]']\n",
      "00:25:53.647309 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 17\n",
      "00:25:53.647555 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7357, 2110, 1100, 1346, 1217, 5688, 4680, ...78, 5335, 5335, 117, 6659, 4518, 3625, 4374, 102]\n",
      "00:25:53.648162 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7357, 2110, 1100, 1346, 1217, 5688...       5335,  117, 6659, 4518, 3625, 4374,  102])\n",
      "00:25:53.649015 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.650233 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.651217 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.652684 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7357, 2110, 1100, 1346, 1217, 568... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.014055\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 4\n",
      "00:25:53.657730 call        14     def __getitem__(self, idx):\n",
      "00:25:53.657828 line        15         if self.mode == \"test\":\n",
      "00:25:53.657930 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '福斯进军华语迷你剧开山之作定檔，“东方华尔街”吴镇宇张孝全'\n",
      "New var:....... text_b = '金融圈,吴镇宇,王牌逗王牌,桃姐,张孝全,东方华尔街,拆弹专家,女朋友·男朋友,刘德华'\n",
      "New var:....... label = 0\n",
      "00:25:53.658669 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.658842 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.659133 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.659409 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['福', '斯', '进', '军', '华', '语', '迷', '你', '剧', '开... '尔', '街', '[UNK]', '吴', '镇', '宇', '张', '孝', '全']\n",
      "00:25:53.660065 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '福', '斯', '进', '军', '华', '语', '迷', '你'..., '[UNK]', '吴', '镇', '宇', '张', '孝', '全', '[SEP]']\n",
      "00:25:53.660313 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:53.660626 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['金', '融', '圈', ',', '吴', '镇', '宇', ',', '王', '牌...'朋', '友', '·', '男', '朋', '友', ',', '刘', '德', '华']\n",
      "00:25:53.661658 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '福', '斯', '进', '军', '华', '语', '迷', '你'... '·', '男', '朋', '友', ',', '刘', '德', '华', '[SEP]']\n",
      "00:25:53.662200 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 44\n",
      "00:25:53.663124 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4886, 3172, 6822, 1092, 1290, 6427, 6837, ...85, 4511, 3301, 1351, 117, 1155, 2548, 1290, 102]\n",
      "00:25:53.663620 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4886, 3172, 6822, 1092, 1290, 6427...3301, 1351,  117, 1155,        2548, 1290,  102])\n",
      "00:25:53.664032 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.665453 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:25:53.666221 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.667696 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4886, 3172, 6822, 1092, 1290, 642...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.012828\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 5\n",
      "00:25:53.670589 call        14     def __getitem__(self, idx):\n",
      "00:25:53.670662 line        15         if self.mode == \"test\":\n",
      "00:25:53.670700 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '马伊琍就是马伊琍，蕾丝也能穿出女王范，文章还是乖乖做小男人吧'\n",
      "New var:....... text_b = '乖乖做小男人,小男人,马伊琍,蕾丝,穿出女王范'\n",
      "New var:....... label = 0\n",
      "00:25:53.671288 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.671382 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.671550 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.671883 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['马', '伊', '琍', '就', '是', '马', '伊', '琍', '，', '蕾...'章', '还', '是', '乖', '乖', '做', '小', '男', '人', '吧']\n",
      "00:25:53.672648 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '马', '伊', '琍', '就', '是', '马', '伊', '琍'... '是', '乖', '乖', '做', '小', '男', '人', '吧', '[SEP]']\n",
      "00:25:53.673010 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:53.673233 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['乖', '乖', '做', '小', '男', '人', ',', '小', '男', '人...'琍', ',', '蕾', '丝', ',', '穿', '出', '女', '王', '范']\n",
      "00:25:53.673854 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '马', '伊', '琍', '就', '是', '马', '伊', '琍'... '蕾', '丝', ',', '穿', '出', '女', '王', '范', '[SEP]']\n",
      "00:25:53.674062 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "00:25:53.674259 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7716, 823, 4419, 2218, 3221, 7716, 823, 44...945, 692, 117, 4959, 1139, 1957, 4374, 5745, 102]\n",
      "00:25:53.674468 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7716,  823, 4419, 2218, 3221, 7716...  692,  117, 4959, 1139, 1957, 4374, 5745,  102])\n",
      "00:25:53.674687 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.675684 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.676514 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.677658 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7716,  823, 4419, 2218, 3221, 771...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009400\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 6\n",
      "00:25:53.680025 call        14     def __getitem__(self, idx):\n",
      "00:25:53.680096 line        15         if self.mode == \"test\":\n",
      "00:25:53.680134 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '这就是街舞 收官：石头秀肌肉，抖动胸肌！易烊千玺坐地，尖叫！'\n",
      "New var:....... text_b = '街舞,胸肌,烊千'\n",
      "New var:....... label = 0\n",
      "00:25:53.680743 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.680846 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.681039 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.681371 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['这', '就', '是', '街', '舞', '收', '官', '：', '石', '头...'易', '烊', '千', '玺', '坐', '地', '，', '尖', '叫', '！']\n",
      "00:25:53.682100 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '就', '是', '街', '舞', '收', '官', '：'... '千', '玺', '坐', '地', '，', '尖', '叫', '！', '[SEP]']\n",
      "00:25:53.682296 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:53.682419 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['街', '舞', ',', '胸', '肌', ',', '烊', '千']\n",
      "00:25:53.682719 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '就', '是', '街', '舞', '收', '官', '：'... '街', '舞', ',', '胸', '肌', ',', '烊', '千', '[SEP]']\n",
      "00:25:53.682979 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 9\n",
      "00:25:53.683275 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6821, 2218, 3221, 6125, 5659, 3119, 2135, ...125, 5659, 117, 5541, 5491, 117, 4165, 1283, 102]\n",
      "00:25:53.683625 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6821, 2218, 3221, 6125, 5659, 3119... 117, 5541, 5491,         117, 4165, 1283,  102])\n",
      "00:25:53.684137 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.684871 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.685472 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.686729 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6821, 2218, 3221, 6125, 5659, 311... 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009033\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 7\n",
      "00:25:53.689101 call        14     def __getitem__(self, idx):\n",
      "00:25:53.689189 line        15         if self.mode == \"test\":\n",
      "00:25:53.689228 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张歆艺现身上海出席红毯活动 网友：好V好比刚路过的飞机场！'\n",
      "New var:....... text_b = '红毯活动,红毯,张歆艺,东方IC,东方ic'\n",
      "New var:....... label = 0\n",
      "00:25:53.689814 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.689909 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.690089 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.690278 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '歆', '艺', '现', '身', '上', '海', '出', '席', '红...'好', '比', '刚', '路', '过', '的', '飞', '机', '场', '！']\n",
      "00:25:53.690989 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '歆', '艺', '现', '身', '上', '海', '出'... '刚', '路', '过', '的', '飞', '机', '场', '！', '[SEP]']\n",
      "00:25:53.691309 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:25:53.691497 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['红', '毯', '活', '动', ',', '红', '毯', ',', '张', '歆', '艺', ',', '东', '方', 'ic', ',', '东', '方', 'ic']\n",
      "00:25:53.692040 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '歆', '艺', '现', '身', '上', '海', '出'...,', '东', '方', 'ic', ',', '东', '方', 'ic', '[SEP]']\n",
      "00:25:53.692279 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:53.692513 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 3622, 5686, 4385, 6716, 677, 3862, 1... 117, 691, 3175, 8577, 117, 691, 3175, 8577, 102]\n",
      "00:25:53.692735 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476, 3622, 5686, 4385, 6716,  677...3175, 8577,  117,  691, 3175,        8577,  102])\n",
      "00:25:53.692954 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.693681 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:25:53.694287 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.695287 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2476, 3622, 5686, 4385, 6716,  67...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008824\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 8\n",
      "00:25:53.697963 call        14     def __getitem__(self, idx):\n",
      "00:25:53.698045 line        15         if self.mode == \"test\":\n",
      "00:25:53.698103 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '李小璐数月首曝光！女儿做爱心餐庆祝母亲节，贾乃亮却不在现场？'\n",
      "New var:....... text_b = '甜馨,母亲节,李小璐,粉丝,贾乃亮'\n",
      "New var:....... label = 0\n",
      "00:25:53.698894 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.699161 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.699319 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.699508 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['李', '小', '璐', '数', '月', '首', '曝', '光', '！', '女...'，', '贾', '乃', '亮', '却', '不', '在', '现', '场', '？']\n",
      "00:25:53.700241 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '小', '璐', '数', '月', '首', '曝', '光'... '乃', '亮', '却', '不', '在', '现', '场', '？', '[SEP]']\n",
      "00:25:53.700434 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:53.700617 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['甜', '馨', ',', '母', '亲', '节', ',', '李', '小', '璐', ',', '粉', '丝', ',', '贾', '乃', '亮']\n",
      "00:25:53.701111 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '李', '小', '璐', '数', '月', '首', '曝', '光'... '璐', ',', '粉', '丝', ',', '贾', '乃', '亮', '[SEP]']\n",
      "00:25:53.701433 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:53.701623 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3330, 2207, 4466, 3144, 3299, 7674, 3284, ..., 4466, 117, 5106, 692, 117, 6593, 718, 778, 102]\n",
      "00:25:53.701827 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3330, 2207, 4466, 3144, 3299, 7674...5106,  692,  117, 6593,  718,         778,  102])\n",
      "00:25:53.702038 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.702693 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:25:53.703432 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.704573 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3330, 2207, 4466, 3144, 3299, 767...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008920\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 9\n",
      "00:25:53.706925 call        14     def __getitem__(self, idx):\n",
      "00:25:53.707006 line        15         if self.mode == \"test\":\n",
      "00:25:53.707046 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '你觉得独孤曼陀丑？那是没看过她演的虞姬！'\n",
      "New var:....... text_b = '安以轩,宇文护,虞姬,李依晓,陇西郡,独孤天下,楚汉传奇,新洛神'\n",
      "New var:....... label = 0\n",
      "00:25:53.707616 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.707711 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.707872 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.708061 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['你', '觉', '得', '独', '孤', '曼', '陀', '丑', '？', '那', '是', '没', '看', '过', '她', '演', '的', '虞', '姬', '！']\n",
      "00:25:53.708622 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '你', '觉', '得', '独', '孤', '曼', '陀', '丑'... '看', '过', '她', '演', '的', '虞', '姬', '！', '[SEP]']\n",
      "00:25:53.708841 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 22\n",
      "00:25:53.709313 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['安', '以', '轩', ',', '宇', '文', '护', ',', '虞', '姬...'下', ',', '楚', '汉', '传', '奇', ',', '新', '洛', '神']\n",
      "00:25:53.710087 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '你', '觉', '得', '独', '孤', '曼', '陀', '丑'... '楚', '汉', '传', '奇', ',', '新', '洛', '神', '[SEP]']\n",
      "00:25:53.710381 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 33\n",
      "00:25:53.710604 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 872, 6230, 2533, 4324, 2109, 3294, 7351, 6...504, 3727, 837, 1936, 117, 3173, 3821, 4868, 102]\n",
      "00:25:53.710824 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  872, 6230, 2533, 4324, 2109, 3294...        837, 1936,  117, 3173, 3821, 4868,  102])\n",
      "00:25:53.711042 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.711793 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.712432 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.713479 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  872, 6230, 2533, 4324, 2109, 329...1, 1, 1,        1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008862\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 10\n",
      "00:25:53.715818 call        14     def __getitem__(self, idx):\n",
      "00:25:53.715890 line        15         if self.mode == \"test\":\n",
      "00:25:53.715929 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = 'Tik杰西达邦和Chakrit联手演绎《傲世双雄》，这部泰剧不狗血哦'\n",
      "New var:....... text_b = 'Falconer,Tik,杰西达邦,雷人,傲世双雄,Mat,Chakrit'\n",
      "New var:....... label = 0\n",
      "00:25:53.716517 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.716613 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.716688 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.716901 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['ti', '##k', '杰', '西', '达', '邦', '和', 'ch', '##...'》', '，', '这', '部', '泰', '剧', '不', '狗', '血', '哦']\n",
      "00:25:53.717639 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'ti', '##k', '杰', '西', '达', '邦', '和', ... '这', '部', '泰', '剧', '不', '狗', '血', '哦', '[SEP]']\n",
      "00:25:53.717878 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:53.718062 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['fa', '##lc', '##one', '##r', ',', 'ti', '##k',...', 'ma', '##t', ',', 'ch', '##ak', '##ri', '##t']\n",
      "00:25:53.718734 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'ti', '##k', '杰', '西', '达', '邦', '和', ...'##t', ',', 'ch', '##ak', '##ri', '##t', '[SEP]']\n",
      "00:25:53.718935 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 29\n",
      "00:25:53.719196 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 9654, 8197, 3345, 6205, 6809, 6930, 1469, ...17, 9622, 8165, 117, 9537, 9896, 8641, 8165, 102]\n",
      "00:25:53.719415 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  9654,  8197,  3345,  6205,  6809...  117,  9537,  9896,  8641,  8165,          102])\n",
      "00:25:53.719633 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.720358 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.721030 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.722247 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  9654,  8197,  3345,  6205,  680... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008781\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 11\n",
      "00:25:53.724628 call        14     def __getitem__(self, idx):\n",
      "00:25:53.724696 line        15         if self.mode == \"test\":\n",
      "00:25:53.724733 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '小男孩模仿薛之谦唱歌，薛之谦一脸错愕，台下尖叫！声音简直一样'\n",
      "New var:....... text_b = '薛之谦'\n",
      "New var:....... label = 0\n",
      "00:25:53.725231 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.725322 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.725393 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.725570 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['小', '男', '孩', '模', '仿', '薛', '之', '谦', '唱', '歌...'下', '尖', '叫', '！', '声', '音', '简', '直', '一', '样']\n",
      "00:25:53.726301 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '小', '男', '孩', '模', '仿', '薛', '之', '谦'... '叫', '！', '声', '音', '简', '直', '一', '样', '[SEP]']\n",
      "00:25:53.726575 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:53.726833 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['薛', '之', '谦']\n",
      "00:25:53.727097 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '小', '男', '孩', '模', '仿', '薛', '之', '谦'..., '直', '一', '样', '[SEP]', '薛', '之', '谦', '[SEP]']\n",
      "00:25:53.727289 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 4\n",
      "00:25:53.727474 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2207, 4511, 2111, 3563, 820, 5955, 722, 64...5042, 4684, 671, 3416, 102, 5955, 722, 6472, 102]\n",
      "00:25:53.727674 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2207, 4511, 2111, 3563,  820, 5955... 4684,  671, 3416,  102, 5955,  722, 6472,  102])\n",
      "00:25:53.727879 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.728393 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0,        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1])\n",
      "00:25:53.729011 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.729926 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2207, 4511, 2111, 3563,  820, 595... 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006950\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 12\n",
      "00:25:53.731608 call        14     def __getitem__(self, idx):\n",
      "00:25:53.731676 line        15         if self.mode == \"test\":\n",
      "00:25:53.731713 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '上官燕被秋若枫毁了清白，可是她觉得伤害她最深的是她父亲上官云'\n",
      "New var:....... text_b = '上官云,上官燕'\n",
      "New var:....... label = 0\n",
      "00:25:53.732243 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.732335 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.732407 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.732586 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['上', '官', '燕', '被', '秋', '若', '枫', '毁', '了', '清...'最', '深', '的', '是', '她', '父', '亲', '上', '官', '云']\n",
      "00:25:53.733297 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '上', '官', '燕', '被', '秋', '若', '枫', '毁'... '的', '是', '她', '父', '亲', '上', '官', '云', '[SEP]']\n",
      "00:25:53.733529 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:53.733810 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['上', '官', '云', ',', '上', '官', '燕']\n",
      "00:25:53.734181 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '上', '官', '燕', '被', '秋', '若', '枫', '毁'...EP]', '上', '官', '云', ',', '上', '官', '燕', '[SEP]']\n",
      "00:25:53.734386 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 8\n",
      "00:25:53.734580 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 677, 2135, 4242, 6158, 4904, 5735, 3367, 3..., 102, 677, 2135, 756, 117, 677, 2135, 4242, 102]\n",
      "00:25:53.734784 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  677, 2135, 4242, 6158, 4904, 5735...2135,  756,  117,         677, 2135, 4242,  102])\n",
      "00:25:53.734994 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.735531 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.736233 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.737096 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  677, 2135, 4242, 6158, 4904, 573... 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007333\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 13\n",
      "00:25:53.738970 call        14     def __getitem__(self, idx):\n",
      "00:25:53.739037 line        15         if self.mode == \"test\":\n",
      "00:25:53.739074 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '会唠嗑儿的直男赢了！说你娶到刘亦菲我也信……'\n",
      "New var:....... text_b = '范冰冰,刘亦菲'\n",
      "New var:....... label = 0\n",
      "00:25:53.739564 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.739655 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.739726 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.739942 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['会', '唠', '嗑', '儿', '的', '直', '男', '赢', '了', '！..., '刘', '亦', '菲', '我', '也', '信', '[UNK]', '[UNK]']\n",
      "00:25:53.740520 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '会', '唠', '嗑', '儿', '的', '直', '男', '赢'...', '菲', '我', '也', '信', '[UNK]', '[UNK]', '[SEP]']\n",
      "00:25:53.740707 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:25:53.740888 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['范', '冰', '冰', ',', '刘', '亦', '菲']\n",
      "00:25:53.741317 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '会', '唠', '嗑', '儿', '的', '直', '男', '赢'...EP]', '范', '冰', '冰', ',', '刘', '亦', '菲', '[SEP]']\n",
      "00:25:53.741535 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 8\n",
      "00:25:53.741730 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 833, 1541, 1622, 1036, 4638, 4684, 4511, 6...102, 5745, 1102, 1102, 117, 1155, 771, 5838, 102]\n",
      "00:25:53.741985 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  833, 1541, 1622, 1036, 4638, 4684... 5745, 1102, 1102,  117, 1155,  771, 5838,  102])\n",
      "00:25:53.742190 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.742679 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0, 0, 0,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.743183 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.744061 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  833, 1541, 1622, 1036, 4638, 468...0, 0,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006794\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 14\n",
      "00:25:53.745796 call        14     def __getitem__(self, idx):\n",
      "00:25:53.745874 line        15         if self.mode == \"test\":\n",
      "00:25:53.745914 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '漫威新电影复仇者联盟3无限战争豪夺票房王座'\n",
      "New var:....... text_b = '复仇者联盟3 ：无限之战,星际异攻队,漫威电影宇宙,复仇者联盟2 ：奥创纪元,IMAX,漫威,电影,漫威影业'\n",
      "New var:....... label = 0\n",
      "00:25:53.746516 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.746787 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.746987 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.747356 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['漫', '威', '新', '电', '影', '复', '仇', '者', '联', '盟...'无', '限', '战', '争', '豪', '夺', '票', '房', '王', '座']\n",
      "00:25:53.748037 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '漫', '威', '新', '电', '影', '复', '仇', '者'... '战', '争', '豪', '夺', '票', '房', '王', '座', '[SEP]']\n",
      "00:25:53.748313 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:25:53.748503 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['复', '仇', '者', '联', '盟', '3', '：', '无', '限', '之...'漫', '威', ',', '电', '影', ',', '漫', '威', '影', '业']\n",
      "00:25:53.749570 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '漫', '威', '新', '电', '影', '复', '仇', '者'... ',', '电', '影', ',', '漫', '威', '影', '业', '[SEP]']\n",
      "00:25:53.749874 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 50\n",
      "00:25:53.750107 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4035, 2014, 3173, 4510, 2512, 1908, 790, 5...117, 4510, 2512, 117, 4035, 2014, 2512, 689, 102]\n",
      "00:25:53.750386 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  4035,  2014,  3173,  4510,  2512...  117,  4035,  2014,         2512,   689,   102])\n",
      "00:25:53.750615 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.751377 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:25:53.752202 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.753532 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  4035,  2014,  3173,  4510,  251...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010586\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 15\n",
      "00:25:53.756414 call        14     def __getitem__(self, idx):\n",
      "00:25:53.756488 line        15         if self.mode == \"test\":\n",
      "00:25:53.756527 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《上海女子图鉴》女主上海十年男友比戚薇少，每个大有来头超有钱'\n",
      "New var:....... text_b = '女强人,刘孜,女主角,北京女子图鉴,罗海燕,戚薇,上海女子图鉴'\n",
      "New var:....... label = 0\n",
      "00:25:53.757097 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.757321 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.757399 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.757584 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '上', '海', '女', '子', '图', '鉴', '》', '女', '主...'，', '每', '个', '大', '有', '来', '头', '超', '有', '钱']\n",
      "00:25:53.758340 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '上', '海', '女', '子', '图', '鉴', '》'... '个', '大', '有', '来', '头', '超', '有', '钱', '[SEP]']\n",
      "00:25:53.758530 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:53.758713 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['女', '强', '人', ',', '刘', '孜', ',', '女', '主', '角...',', '戚', '薇', ',', '上', '海', '女', '子', '图', '鉴']\n",
      "00:25:53.759445 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '上', '海', '女', '子', '图', '鉴', '》'... '薇', ',', '上', '海', '女', '子', '图', '鉴', '[SEP]']\n",
      "00:25:53.759763 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 32\n",
      "00:25:53.759958 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 677, 3862, 1957, 2094, 1745, 7063, 51...948, 117, 677, 3862, 1957, 2094, 1745, 7063, 102]\n",
      "00:25:53.760168 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517,  677, 3862, 1957, 2094, 1745... 677, 3862, 1957,        2094, 1745, 7063,  102])\n",
      "00:25:53.760383 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.761071 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.761756 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.763004 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517,  677, 3862, 1957, 2094, 174... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009480\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 16\n",
      "00:25:53.765948 call        14     def __getitem__(self, idx):\n",
      "00:25:53.766050 line        15         if self.mode == \"test\":\n",
      "00:25:53.766099 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '跟王菲学的“新潮流”? 谢霆锋穿着“鸳鸯袜”喊话网友自寻亮点！'\n",
      "New var:....... text_b = '新潮流,王菲,张柏芝,谢霆锋,牛仔裤'\n",
      "New var:....... label = 0\n",
      "00:25:53.767000 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.767137 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.767411 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.767807 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['跟', '王', '菲', '学', '的', '[UNK]', '新', '潮', '流'...K]', '喊', '话', '网', '友', '自', '寻', '亮', '点', '！']\n",
      "00:25:53.768715 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '跟', '王', '菲', '学', '的', '[UNK]', '新',... '话', '网', '友', '自', '寻', '亮', '点', '！', '[SEP]']\n",
      "00:25:53.768955 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:53.769152 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['新', '潮', '流', ',', '王', '菲', ',', '张', '柏', '芝', ',', '谢', '霆', '锋', ',', '牛', '仔', '裤']\n",
      "00:25:53.769710 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '跟', '王', '菲', '学', '的', '[UNK]', '新',... ',', '谢', '霆', '锋', ',', '牛', '仔', '裤', '[SEP]']\n",
      "00:25:53.769909 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:25:53.770099 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6656, 4374, 5838, 2110, 4638, 100, 3173, 4...117, 6468, 7447, 7226, 117, 4281, 798, 6175, 102]\n",
      "00:25:53.770406 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6656, 4374, 5838, 2110, 4638,  100...7447, 7226,  117, 4281,         798, 6175,  102])\n",
      "00:25:53.770655 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.771270 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:25:53.771924 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.773025 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6656, 4374, 5838, 2110, 4638,  10...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009228\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 17\n",
      "00:25:53.775206 call        14     def __getitem__(self, idx):\n",
      "00:25:53.775278 line        15         if self.mode == \"test\":\n",
      "00:25:53.775316 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '4岁成名，比张一山杨紫还红，被父母败光千万身家，如今长成这样'\n",
      "New var:....... text_b = '演艺圈,笑林小子,朱延平,新乌龙院,哪吒大战美猴王,释小龙,郝劭文,张一山'\n",
      "New var:....... label = 0\n",
      "00:25:53.775880 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.775971 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.776043 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.776278 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['4', '岁', '成', '名', '，', '比', '张', '一', '山', '杨...'万', '身', '家', '，', '如', '今', '长', '成', '这', '样']\n",
      "00:25:53.777004 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '4', '岁', '成', '名', '，', '比', '张', '一'... '家', '，', '如', '今', '长', '成', '这', '样', '[SEP]']\n",
      "00:25:53.777194 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:53.777443 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['演', '艺', '圈', ',', '笑', '林', '小', '子', ',', '朱...'小', '龙', ',', '郝', '劭', '文', ',', '张', '一', '山']\n",
      "00:25:53.778297 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '4', '岁', '成', '名', '，', '比', '张', '一'... ',', '郝', '劭', '文', ',', '张', '一', '山', '[SEP]']\n",
      "00:25:53.778517 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 38\n",
      "00:25:53.778724 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 125, 2259, 2768, 1399, 8024, 3683, 2476, 6...117, 6950, 1224, 3152, 117, 2476, 671, 2255, 102]\n",
      "00:25:53.778948 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  125, 2259, 2768, 1399, 8024, 3683... 6950, 1224, 3152,  117, 2476,  671, 2255,  102])\n",
      "00:25:53.779219 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.780039 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.780775 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.782093 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  125, 2259, 2768, 1399, 8024, 368... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009651\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 18\n",
      "00:25:53.784891 call        14     def __getitem__(self, idx):\n",
      "00:25:53.784965 line        15         if self.mode == \"test\":\n",
      "00:25:53.785003 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '张杰外网发文回应《101》风波：解释无果那就沉默'\n",
      "New var:....... text_b = '张杰,粉丝,创造101'\n",
      "New var:....... label = 0\n",
      "00:25:53.785580 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.785786 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.785864 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.786041 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['张', '杰', '外', '网', '发', '文', '回', '应', '《', '1...'波', '：', '解', '释', '无', '果', '那', '就', '沉', '默']\n",
      "00:25:53.786641 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '杰', '外', '网', '发', '文', '回', '应'... '解', '释', '无', '果', '那', '就', '沉', '默', '[SEP]']\n",
      "00:25:53.786830 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 24\n",
      "00:25:53.787012 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['张', '杰', ',', '粉', '丝', ',', '创', '造', '101']\n",
      "00:25:53.787387 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '张', '杰', '外', '网', '发', '文', '回', '应'...杰', ',', '粉', '丝', ',', '创', '造', '101', '[SEP]']\n",
      "00:25:53.787618 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 10\n",
      "00:25:53.787867 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2476, 3345, 1912, 5381, 1355, 3152, 1726, ...3345, 117, 5106, 692, 117, 1158, 6863, 8359, 102]\n",
      "00:25:53.788067 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2476, 3345, 1912, 5381, 1355, 3152...  117, 5106,  692,  117, 1158, 6863, 8359,  102])\n",
      "00:25:53.788271 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.788773 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.789375 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.790398 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2476, 3345, 1912, 5381, 1355, 315...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007213\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 19\n",
      "00:25:53.792131 call        14     def __getitem__(self, idx):\n",
      "00:25:53.792199 line        15         if self.mode == \"test\":\n",
      "00:25:53.792236 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '娱乐圈又一气质女神出柜了？她还是柳岩韩雪的闺蜜！'\n",
      "New var:....... text_b = '吴卓林,祝你幸福,毛林林,兰陵王,柳岩,韩雪'\n",
      "New var:....... label = 0\n",
      "00:25:53.792719 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.792907 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.792989 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.793176 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['娱', '乐', '圈', '又', '一', '气', '质', '女', '神', '出...'还', '是', '柳', '岩', '韩', '雪', '的', '闺', '蜜', '！']\n",
      "00:25:53.793798 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '又', '一', '气', '质', '女'... '柳', '岩', '韩', '雪', '的', '闺', '蜜', '！', '[SEP]']\n",
      "00:25:53.794039 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:53.794221 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['吴', '卓', '林', ',', '祝', '你', '幸', '福', ',', '毛...',', '兰', '陵', '王', ',', '柳', '岩', ',', '韩', '雪']\n",
      "00:25:53.794800 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '又', '一', '气', '质', '女'... '陵', '王', ',', '柳', '岩', ',', '韩', '雪', '[SEP]']\n",
      "00:25:53.795061 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:25:53.795254 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2031, 727, 1750, 1348, 671, 3698, 6574, 19...377, 4374, 117, 3394, 2272, 117, 7506, 7434, 102]\n",
      "00:25:53.795458 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2031,  727, 1750, 1348,  671, 3698... 117, 3394, 2272,  117, 7506, 7434,         102])\n",
      "00:25:53.795667 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.796266 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:25:53.796903 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.798016 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2031,  727, 1750, 1348,  671, 369...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008039\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 20\n",
      "00:25:53.800198 call        14     def __getitem__(self, idx):\n",
      "00:25:53.800266 line        15         if self.mode == \"test\":\n",
      "00:25:53.800302 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = 'Jasper胖得下巴和脖子连成线，这样的小小春反而更加呆萌可爱'\n",
      "New var:....... text_b = '连成线,下巴'\n",
      "New var:....... label = 0\n",
      "00:25:53.800768 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.800858 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.800928 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.801145 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['j', '##as', '##per', '胖', '得', '下', '巴', '和', ...'小', '春', '反', '而', '更', '加', '呆', '萌', '可', '爱']\n",
      "00:25:53.801840 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'j', '##as', '##per', '胖', '得', '下', '... '反', '而', '更', '加', '呆', '萌', '可', '爱', '[SEP]']\n",
      "00:25:53.802029 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:25:53.802210 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['连', '成', '线', ',', '下', '巴']\n",
      "00:25:53.802586 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'j', '##as', '##per', '胖', '得', '下', '..., '[SEP]', '连', '成', '线', ',', '下', '巴', '[SEP]']\n",
      "00:25:53.802788 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 7\n",
      "00:25:53.802976 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 152, 8576, 9063, 5523, 2533, 678, 2349, 14...4263, 102, 6825, 2768, 5296, 117, 678, 2349, 102]\n",
      "00:25:53.803211 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  152, 8576, 9063, 5523, 2533,  678...6825, 2768, 5296,  117,  678, 2349,         102])\n",
      "00:25:53.803414 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.803932 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0,        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.804447 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.805397 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  152, 8576, 9063, 5523, 2533,  67... 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006983\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 21\n",
      "00:25:53.807208 call        14     def __getitem__(self, idx):\n",
      "00:25:53.807274 line        15         if self.mode == \"test\":\n",
      "00:25:53.807310 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '黄子韬：我演哭戏不用眼药水'\n",
      "New var:....... text_b = '黄子韬,眼药水,杨采钰,女主角,大秘密'\n",
      "New var:....... label = 0\n",
      "00:25:53.807777 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.807977 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.808050 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.808224 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['黄', '子', '韬', '：', '我', '演', '哭', '戏', '不', '用', '眼', '药', '水']\n",
      "00:25:53.808648 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '黄', '子', '韬', '：', '我', '演', '哭', '戏', '不', '用', '眼', '药', '水', '[SEP]']\n",
      "00:25:53.808828 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "00:25:53.809003 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['黄', '子', '韬', ',', '眼', '药', '水', ',', '杨', '采', '钰', ',', '女', '主', '角', ',', '大', '秘', '密']\n",
      "00:25:53.809527 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '黄', '子', '韬', '：', '我', '演', '哭', '戏'... ',', '女', '主', '角', ',', '大', '秘', '密', '[SEP]']\n",
      "00:25:53.809715 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:53.809899 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7942, 2094, 7507, 8038, 2769, 4028, 1526, ...117, 1957, 712, 6235, 117, 1920, 4908, 2166, 102]\n",
      "00:25:53.810195 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7942, 2094, 7507, 8038, 2769, 4028... 1957,  712, 6235,  117, 1920, 4908, 2166,  102])\n",
      "00:25:53.810397 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.810957 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.811459 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.812474 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7942, 2094, 7507, 8038, 2769, 402...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007006\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 22\n",
      "00:25:53.814244 call        14     def __getitem__(self, idx):\n",
      "00:25:53.814314 line        15         if self.mode == \"test\":\n",
      "00:25:53.814351 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '吴尊的肌肉是怎么练出来的？'\n",
      "New var:....... text_b = 'Fitness,飞轮海,吴尊,健身中心,文莱'\n",
      "New var:....... label = 0\n",
      "00:25:53.814872 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.815035 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.815112 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.815288 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['吴', '尊', '的', '肌', '肉', '是', '怎', '么', '练', '出', '来', '的', '？']\n",
      "00:25:53.815718 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '吴', '尊', '的', '肌', '肉', '是', '怎', '么', '练', '出', '来', '的', '？', '[SEP]']\n",
      "00:25:53.815901 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "00:25:53.816087 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['fit', '##ness', ',', '飞', '轮', '海', ',', '吴', '尊', ',', '健', '身', '中', '心', ',', '文', '莱']\n",
      "00:25:53.816649 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '吴', '尊', '的', '肌', '肉', '是', '怎', '么'... ',', '健', '身', '中', '心', ',', '文', '莱', '[SEP]']\n",
      "00:25:53.816870 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:53.817172 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1426, 2203, 4638, 5491, 5489, 3221, 2582, ... 117, 978, 6716, 704, 2552, 117, 3152, 5812, 102]\n",
      "00:25:53.817384 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  1426,  2203,  4638,  5491,  5489...  704,  2552,   117,         3152,  5812,   102])\n",
      "00:25:53.817690 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.818596 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.819165 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.819840 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  1426,  2203,  4638,  5491,  548...1,        1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010903\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 23\n",
      "00:25:53.825186 call        14     def __getitem__(self, idx):\n",
      "00:25:53.825266 line        15         if self.mode == \"test\":\n",
      "00:25:53.825304 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '韩瑜：出席活动，网友：真羡慕这个肌肉男！女神身材太好了'\n",
      "New var:....... text_b = '女神身材太,真羡慕,肌肉男,韩瑜,女神'\n",
      "New var:....... label = 0\n",
      "00:25:53.825967 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.826125 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.826449 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.827192 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['韩', '瑜', '：', '出', '席', '活', '动', '，', '网', '友...'肉', '男', '！', '女', '神', '身', '材', '太', '好', '了']\n",
      "00:25:53.828036 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '瑜', '：', '出', '席', '活', '动', '，'... '！', '女', '神', '身', '材', '太', '好', '了', '[SEP]']\n",
      "00:25:53.828354 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:53.828631 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['女', '神', '身', '材', '太', ',', '真', '羡', '慕', ',', '肌', '肉', '男', ',', '韩', '瑜', ',', '女', '神']\n",
      "00:25:53.829222 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '韩', '瑜', '：', '出', '席', '活', '动', '，'... '肉', '男', ',', '韩', '瑜', ',', '女', '神', '[SEP]']\n",
      "00:25:53.829580 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 20\n",
      "00:25:53.829819 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7506, 4447, 8038, 1139, 2375, 3833, 1220, ...489, 4511, 117, 7506, 4447, 117, 1957, 4868, 102]\n",
      "00:25:53.830079 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7506, 4447, 8038, 1139, 2375, 3833... 117, 7506, 4447,  117, 1957, 4868,         102])\n",
      "00:25:53.830450 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.831138 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:25:53.832219 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.833345 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7506, 4447, 8038, 1139, 2375, 383...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010863\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 24\n",
      "00:25:53.836086 call        14     def __getitem__(self, idx):\n",
      "00:25:53.836166 line        15         if self.mode == \"test\":\n",
      "00:25:53.836204 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '陈都灵、沈月出演女主角'\n",
      "New var:....... text_b = '韩东,流星花园,七月与安生,沈月,七月,邹廷威,陈都灵,左耳'\n",
      "New var:....... label = 0\n",
      "00:25:53.836784 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.837002 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.837195 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.837418 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['陈', '都', '灵', '、', '沈', '月', '出', '演', '女', '主', '角']\n",
      "00:25:53.837870 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '都', '灵', '、', '沈', '月', '出', '演', '女', '主', '角', '[SEP]']\n",
      "00:25:53.838099 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 13\n",
      "00:25:53.838321 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['韩', '东', ',', '流', '星', '花', '园', ',', '七', '月...'邹', '廷', '威', ',', '陈', '都', '灵', ',', '左', '耳']\n",
      "00:25:53.839075 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '都', '灵', '、', '沈', '月', '出', '演'... '威', ',', '陈', '都', '灵', ',', '左', '耳', '[SEP]']\n",
      "00:25:53.839423 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 31\n",
      "00:25:53.839661 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7357, 6963, 4130, 510, 3755, 3299, 1139, 4...014, 117, 7357, 6963, 4130, 117, 2340, 5455, 102]\n",
      "00:25:53.839909 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7357, 6963, 4130,  510, 3755, 3299...  117, 7357, 6963, 4130,  117, 2340, 5455,  102])\n",
      "00:25:53.840166 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.840816 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.841580 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.842853 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7357, 6963, 4130,  510, 3755, 329... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009197\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 25\n",
      "00:25:53.845311 call        14     def __getitem__(self, idx):\n",
      "00:25:53.845379 line        15         if self.mode == \"test\":\n",
      "00:25:53.845417 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '笑笑被曝携手七号西卡探险 七号Miss深夜发文帮助澄清'\n",
      "New var:....... text_b = '微博,Miss,探险,德云色'\n",
      "New var:....... label = 0\n",
      "00:25:53.845982 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.846075 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.846150 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.846491 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['笑', '笑', '被', '曝', '携', '手', '七', '号', '西', '卡..., 'miss', '深', '夜', '发', '文', '帮', '助', '澄', '清']\n",
      "00:25:53.847172 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '笑', '笑', '被', '曝', '携', '手', '七', '号'... '深', '夜', '发', '文', '帮', '助', '澄', '清', '[SEP]']\n",
      "00:25:53.847407 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:53.847558 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['微', '博', ',', 'miss', ',', '探', '险', ',', '德', '云', '色']\n",
      "00:25:53.847992 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '笑', '笑', '被', '曝', '携', '手', '七', '号'...iss', ',', '探', '险', ',', '德', '云', '色', '[SEP]']\n",
      "00:25:53.848230 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 12\n",
      "00:25:53.848465 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5010, 5010, 6158, 3284, 3025, 2797, 673, 1...9368, 117, 2968, 7372, 117, 2548, 756, 5682, 102]\n",
      "00:25:53.848911 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5010, 5010, 6158, 3284, 3025, 2797...2968, 7372,  117, 2548,  756, 5682,         102])\n",
      "00:25:53.849210 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.849829 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0,        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.850345 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.851446 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5010, 5010, 6158, 3284, 3025, 279... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008160\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 26\n",
      "00:25:53.853501 call        14     def __getitem__(self, idx):\n",
      "00:25:53.853684 line        15         if self.mode == \"test\":\n",
      "00:25:53.853724 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '沈梦辰穿短裤秀美腿现身机场 和周觅、沈凌等人赴巴厘岛参加婚礼'\n",
      "New var:....... text_b = '付辛博,沈梦辰,巴厘岛,沈凌,周觅'\n",
      "New var:....... label = 0\n",
      "00:25:53.854281 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.854373 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.854446 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.854674 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['沈', '梦', '辰', '穿', '短', '裤', '秀', '美', '腿', '现...'等', '人', '赴', '巴', '厘', '岛', '参', '加', '婚', '礼']\n",
      "00:25:53.855429 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '沈', '梦', '辰', '穿', '短', '裤', '秀', '美'... '赴', '巴', '厘', '岛', '参', '加', '婚', '礼', '[SEP]']\n",
      "00:25:53.855662 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:53.856000 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['付', '辛', '博', ',', '沈', '梦', '辰', ',', '巴', '厘', '岛', ',', '沈', '凌', ',', '周', '觅']\n",
      "00:25:53.856546 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '沈', '梦', '辰', '穿', '短', '裤', '秀', '美'... '厘', '岛', ',', '沈', '凌', ',', '周', '觅', '[SEP]']\n",
      "00:25:53.856786 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:53.857022 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3755, 3457, 6801, 4959, 4764, 6175, 4899, ...330, 2270, 117, 3755, 1119, 117, 1453, 6227, 102]\n",
      "00:25:53.857273 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3755, 3457, 6801, 4959, 4764, 6175... 117, 3755, 1119,  117, 1453, 6227,         102])\n",
      "00:25:53.857530 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.858326 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:25:53.859125 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.860265 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3755, 3457, 6801, 4959, 4764, 617...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009212\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 27\n",
      "00:25:53.862747 call        14     def __getitem__(self, idx):\n",
      "00:25:53.862817 line        15         if self.mode == \"test\":\n",
      "00:25:53.862854 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '女星穿搭很重要，一不小心变槽点：盘点女星的扑街丑照'\n",
      "New var:....... text_b = '邓紫棋,马思纯,吴昕,快乐大本营,穿搭,刘亦菲'\n",
      "New var:....... label = 0\n",
      "00:25:53.863589 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.863970 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.864232 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.864648 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['女', '星', '穿', '搭', '很', '重', '要', '，', '一', '不...'：', '盘', '点', '女', '星', '的', '扑', '街', '丑', '照']\n",
      "00:25:53.865527 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '星', '穿', '搭', '很', '重', '要', '，'... '点', '女', '星', '的', '扑', '街', '丑', '照', '[SEP]']\n",
      "00:25:53.866280 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:25:53.866704 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['邓', '紫', '棋', ',', '马', '思', '纯', ',', '吴', '昕...'大', '本', '营', ',', '穿', '搭', ',', '刘', '亦', '菲']\n",
      "00:25:53.867398 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '星', '穿', '搭', '很', '重', '要', '，'... '营', ',', '穿', '搭', ',', '刘', '亦', '菲', '[SEP]']\n",
      "00:25:53.867656 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "00:25:53.867899 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1957, 3215, 4959, 3022, 2523, 7028, 6206, ...5852, 117, 4959, 3022, 117, 1155, 771, 5838, 102]\n",
      "00:25:53.868166 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1957, 3215, 4959, 3022, 2523, 7028...4959, 3022,  117, 1155,         771, 5838,  102])\n",
      "00:25:53.868664 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.869370 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1])\n",
      "00:25:53.870089 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.872305 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1957, 3215, 4959, 3022, 2523, 702...1, 1, 1, 1, 1, 1, 1,        1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.013163\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 28\n",
      "00:25:53.875967 call        14     def __getitem__(self, idx):\n",
      "00:25:53.876061 line        15         if self.mode == \"test\":\n",
      "00:25:53.876103 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '刘浩强：演员、导演、编剧、策划人'\n",
      "New var:....... text_b = '演员,编剧,电影,婚礼,山乡溢彩,叮当遇险记,克隆出租,刘浩强'\n",
      "New var:....... label = 0\n",
      "00:25:53.876716 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.876833 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.876938 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.877539 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['刘', '浩', '强', '：', '演', '员', '、', '导', '演', '、', '编', '剧', '、', '策', '划', '人']\n",
      "00:25:53.878708 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刘', '浩', '强', '：', '演', '员', '、', '导', '演', '、', '编', '剧', '、', '策', '划', '人', '[SEP]']\n",
      "00:25:53.879115 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 18\n",
      "00:25:53.879354 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['演', '员', ',', '编', '剧', ',', '电', '影', ',', '婚...'记', ',', '克', '隆', '出', '租', ',', '刘', '浩', '强']\n",
      "00:25:53.880444 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '刘', '浩', '强', '：', '演', '员', '、', '导'... '克', '隆', '出', '租', ',', '刘', '浩', '强', '[SEP]']\n",
      "00:25:53.880951 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 32\n",
      "00:25:53.881361 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1155, 3856, 2487, 8038, 4028, 1447, 510, 2...46, 7384, 1139, 4909, 117, 1155, 3856, 2487, 102]\n",
      "00:25:53.881556 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1155, 3856, 2487, 8038, 4028, 1447...1139, 4909,  117, 1155, 3856,        2487,  102])\n",
      "00:25:53.882198 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.883228 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:25:53.883985 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.885179 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1155, 3856, 2487, 8038, 4028, 144...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.011862\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 29\n",
      "00:25:53.887858 call        14     def __getitem__(self, idx):\n",
      "00:25:53.887931 line        15         if self.mode == \"test\":\n",
      "00:25:53.887968 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '唐嫣首部穿越剧开播，《天意》配角比主角演技还好，实力圈粉！'\n",
      "New var:....... text_b = '张丹峰,秦始皇,唐嫣,天意,剧中饰演,乔振宇'\n",
      "New var:....... label = 0\n",
      "00:25:53.888546 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.888788 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.888873 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.889105 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['唐', '嫣', '首', '部', '穿', '越', '剧', '开', '播', '，...'演', '技', '还', '好', '，', '实', '力', '圈', '粉', '！']\n",
      "00:25:53.889914 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '唐', '嫣', '首', '部', '穿', '越', '剧', '开'... '还', '好', '，', '实', '力', '圈', '粉', '！', '[SEP]']\n",
      "00:25:53.890150 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:53.890381 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['张', '丹', '峰', ',', '秦', '始', '皇', ',', '唐', '嫣...'意', ',', '剧', '中', '饰', '演', ',', '乔', '振', '宇']\n",
      "00:25:53.891011 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '唐', '嫣', '首', '部', '穿', '越', '剧', '开'... '剧', '中', '饰', '演', ',', '乔', '振', '宇', '[SEP]']\n",
      "00:25:53.891397 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:25:53.891668 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1538, 2073, 7674, 6956, 4959, 6632, 1196, ...1196, 704, 7652, 4028, 117, 730, 2920, 2126, 102]\n",
      "00:25:53.891843 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1538, 2073, 7674, 6956, 4959, 6632...7652,        4028,  117,  730, 2920, 2126,  102])\n",
      "00:25:53.892150 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.892974 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:25:53.893946 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.895123 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1538, 2073, 7674, 6956, 4959, 663...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010144\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 30\n",
      "00:25:53.898040 call        14     def __getitem__(self, idx):\n",
      "00:25:53.898117 line        15         if self.mode == \"test\":\n",
      "00:25:53.898155 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '时尚扫街·“暮光女”克里斯汀·斯图尔特，个性短裙下车大摆Pose'\n",
      "New var:....... text_b = '克里斯汀,MTV电影奖最佳女演员奖,暮光之城,奇幻爱情电影,暮光之城：新月,东方IC,暮光之城：月食,届MTV'\n",
      "New var:....... label = 0\n",
      "00:25:53.898729 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.898970 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.899174 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.899401 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['时', '尚', '扫', '街', '·', '[UNK]', '暮', '光', '女'..., '个', '性', '短', '裙', '下', '车', '大', '摆', 'pose']\n",
      "00:25:53.900194 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '时', '尚', '扫', '街', '·', '[UNK]', '暮',...', '短', '裙', '下', '车', '大', '摆', 'pose', '[SEP]']\n",
      "00:25:53.900441 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:53.900685 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['克', '里', '斯', '汀', ',', 'mtv', '电', '影', '奖', ...', '光', '之', '城', '：', '月', '食', ',', '届', 'mtv']\n",
      "00:25:53.901828 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '时', '尚', '扫', '街', '·', '[UNK]', '暮',...之', '城', '：', '月', '食', ',', '届', 'mtv', '[SEP]']\n",
      "00:25:53.902244 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 51\n",
      "00:25:53.902494 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3198, 2213, 2812, 6125, 185, 100, 3272, 10...2, 1814, 8038, 3299, 7608, 117, 2237, 11529, 102]\n",
      "00:25:53.902765 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  3198,  2213,  2812,  6125,   185... 3299,  7608,   117,  2237,        11529,   102])\n",
      "00:25:53.903040 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.904092 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.905168 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.906679 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  3198,  2213,  2812,  6125,   18...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.012045\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 31\n",
      "00:25:53.910118 call        14     def __getitem__(self, idx):\n",
      "00:25:53.910191 line        15         if self.mode == \"test\":\n",
      "00:25:53.910229 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '被消失五年，这是近几年最让人不寒而栗的国产佳作'\n",
      "New var:....... text_b = '李玩,中国式,烈日灼心,鸟人,时间简史,曹保平,爱因斯坦,李米的猜想,追凶者也,周迅'\n",
      "New var:....... label = 0\n",
      "00:25:53.910803 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.910992 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.911070 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.911305 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['被', '消', '失', '五', '年', '，', '这', '是', '近', '几...'人', '不', '寒', '而', '栗', '的', '国', '产', '佳', '作']\n",
      "00:25:53.911913 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '被', '消', '失', '五', '年', '，', '这', '是'... '寒', '而', '栗', '的', '国', '产', '佳', '作', '[SEP]']\n",
      "00:25:53.912101 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:53.912283 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['李', '玩', ',', '中', '国', '式', ',', '烈', '日', '灼...'猜', '想', ',', '追', '凶', '者', '也', ',', '周', '迅']\n",
      "00:25:53.913200 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '被', '消', '失', '五', '年', '，', '这', '是'... ',', '追', '凶', '者', '也', ',', '周', '迅', '[SEP]']\n",
      "00:25:53.913502 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 43\n",
      "00:25:53.913698 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6158, 3867, 1927, 758, 2399, 8024, 6821, 3...117, 6841, 1136, 5442, 738, 117, 1453, 6813, 102]\n",
      "00:25:53.913912 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6158, 3867, 1927,  758, 2399, 8024... 6841, 1136, 5442,  738,  117, 1453, 6813,  102])\n",
      "00:25:53.914128 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.914833 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.915540 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.916872 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6158, 3867, 1927,  758, 2399, 802... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009630\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 32\n",
      "00:25:53.919786 call        14     def __getitem__(self, idx):\n",
      "00:25:53.919865 line        15         if self.mode == \"test\":\n",
      "00:25:53.919903 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '她出演了韩国的综艺节目，她的美被人发现'\n",
      "New var:....... text_b = '塞尔维亚语,综艺节目,Angelina,圣彼得堡,俄罗斯'\n",
      "New var:....... label = 0\n",
      "00:25:53.920501 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.920598 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.920747 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.920926 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['她', '出', '演', '了', '韩', '国', '的', '综', '艺', '节', '目', '，', '她', '的', '美', '被', '人', '发', '现']\n",
      "00:25:53.921472 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '她', '出', '演', '了', '韩', '国', '的', '综'... '，', '她', '的', '美', '被', '人', '发', '现', '[SEP]']\n",
      "00:25:53.921658 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:25:53.921904 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['塞', '尔', '维', '亚', '语', ',', '综', '艺', '节', '目...na', ',', '圣', '彼', '得', '堡', ',', '俄', '罗', '斯']\n",
      "00:25:53.922524 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '她', '出', '演', '了', '韩', '国', '的', '综'... '圣', '彼', '得', '堡', ',', '俄', '罗', '斯', '[SEP]']\n",
      "00:25:53.922722 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 23\n",
      "00:25:53.922912 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1961, 1139, 4028, 749, 7506, 1744, 4638, 5...760, 2516, 2533, 1836, 117, 915, 5384, 3172, 102]\n",
      "00:25:53.923119 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1961, 1139, 4028,  749, 7506, 1744... 2516, 2533, 1836,  117,  915, 5384, 3172,  102])\n",
      "00:25:53.923330 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.923941 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.924630 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.925533 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1961, 1139, 4028,  749, 7506, 174... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007699\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 33\n",
      "00:25:53.927516 call        14     def __getitem__(self, idx):\n",
      "00:25:53.927585 line        15         if self.mode == \"test\":\n",
      "00:25:53.927623 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = 'TVB艺人吴家乐拍新剧遭人排挤，好友李珊珊安慰：遇神杀神！'\n",
      "New var:....... text_b = '吴家乐,包青天再起风云,花蝴蝶,谭俊彦,姚子羚,城寨英雄,李珊珊,胡定欣'\n",
      "New var:....... label = 0\n",
      "00:25:53.928199 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.928292 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.928367 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.928548 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['tvb', '艺', '人', '吴', '家', '乐', '拍', '新', '剧', ...'珊', '珊', '安', '慰', '：', '遇', '神', '杀', '神', '！']\n",
      "00:25:53.929246 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'tvb', '艺', '人', '吴', '家', '乐', '拍', '... '安', '慰', '：', '遇', '神', '杀', '神', '！', '[SEP]']\n",
      "00:25:53.929436 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:53.929685 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['吴', '家', '乐', ',', '包', '青', '天', '再', '起', '风...'英', '雄', ',', '李', '珊', '珊', ',', '胡', '定', '欣']\n",
      "00:25:53.930510 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'tvb', '艺', '人', '吴', '家', '乐', '拍', '... ',', '李', '珊', '珊', ',', '胡', '定', '欣', '[SEP]']\n",
      "00:25:53.930707 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 37\n",
      "00:25:53.930900 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 9312, 5686, 782, 1426, 2157, 727, 2864, 31...17, 3330, 4396, 4396, 117, 5529, 2137, 3615, 102]\n",
      "00:25:53.931113 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 9312, 5686,  782, 1426, 2157,  727...4396,        4396,  117, 5529, 2137, 3615,  102])\n",
      "00:25:53.931330 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.932199 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.932911 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.934187 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 9312, 5686,  782, 1426, 2157,  72... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009449\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 34\n",
      "00:25:53.937016 call        14     def __getitem__(self, idx):\n",
      "00:25:53.937139 line        15         if self.mode == \"test\":\n",
      "00:25:53.937176 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '言承旭带娃俨然慈父，端小碗耐心喂饭超宠溺，网友喊话林志玲'\n",
      "New var:....... text_b = '放开我北鼻,言承旭,经纪人,真人秀,林志玲'\n",
      "New var:....... label = 0\n",
      "00:25:53.937717 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.937935 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.938028 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.938257 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['言', '承', '旭', '带', '娃', '俨', '然', '慈', '父', '，...'宠', '溺', '，', '网', '友', '喊', '话', '林', '志', '玲']\n",
      "00:25:53.938986 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '言', '承', '旭', '带', '娃', '俨', '然', '慈'... '，', '网', '友', '喊', '话', '林', '志', '玲', '[SEP]']\n",
      "00:25:53.939188 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:25:53.939387 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['放', '开', '我', '北', '鼻', ',', '言', '承', '旭', ',...'纪', '人', ',', '真', '人', '秀', ',', '林', '志', '玲']\n",
      "00:25:53.939952 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '言', '承', '旭', '带', '娃', '俨', '然', '慈'... ',', '真', '人', '秀', ',', '林', '志', '玲', '[SEP]']\n",
      "00:25:53.940277 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 22\n",
      "00:25:53.940506 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6241, 2824, 3195, 2372, 2015, 929, 4197, 2...117, 4696, 782, 4899, 117, 3360, 2562, 4386, 102]\n",
      "00:25:53.940724 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6241, 2824, 3195, 2372, 2015,  929... 782, 4899,  117,        3360, 2562, 4386,  102])\n",
      "00:25:53.940941 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.941551 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:25:53.942161 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.943265 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6241, 2824, 3195, 2372, 2015,  92...1, 1, 1, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008313\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 35\n",
      "00:25:53.945337 call        14     def __getitem__(self, idx):\n",
      "00:25:53.945474 line        15         if self.mode == \"test\":\n",
      "00:25:53.945513 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '一部反应人性的电影，母女逃难被全村人欺负，让人心焦'\n",
      "New var:....... text_b = '全村人,电影,母女'\n",
      "New var:....... label = 0\n",
      "00:25:53.945990 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.946082 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.946153 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.946332 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['一', '部', '反', '应', '人', '性', '的', '电', '影', '，...'全', '村', '人', '欺', '负', '，', '让', '人', '心', '焦']\n",
      "00:25:53.947003 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '一', '部', '反', '应', '人', '性', '的', '电'... '人', '欺', '负', '，', '让', '人', '心', '焦', '[SEP]']\n",
      "00:25:53.947190 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:25:53.947372 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['全', '村', '人', ',', '电', '影', ',', '母', '女']\n",
      "00:25:53.947797 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '一', '部', '反', '应', '人', '性', '的', '电'... '村', '人', ',', '电', '影', ',', '母', '女', '[SEP]']\n",
      "00:25:53.947989 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 10\n",
      "00:25:53.948176 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 671, 6956, 1353, 2418, 782, 2595, 4638, 45...3333, 782, 117, 4510, 2512, 117, 3678, 1957, 102]\n",
      "00:25:53.948374 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  671, 6956, 1353, 2418,  782, 2595... 117, 4510, 2512,  117, 3678, 1957,         102])\n",
      "00:25:53.948622 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.949133 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0,        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.949647 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.950572 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  671, 6956, 1353, 2418,  782, 259... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006866\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 36\n",
      "00:25:53.952233 call        14     def __getitem__(self, idx):\n",
      "00:25:53.952305 line        15         if self.mode == \"test\":\n",
      "00:25:53.952343 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '女明星的高跟鞋谁最奇特？'\n",
      "New var:....... text_b = '女明星,景甜,冉莹颖,尚雯婕,欧阳娜娜,邹市明,女人味,范冰冰,袁姗姗,驴蹄鞋,恨天高,连衣裙'\n",
      "New var:....... label = 0\n",
      "00:25:53.952824 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.953056 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.953142 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.953333 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['女', '明', '星', '的', '高', '跟', '鞋', '谁', '最', '奇', '特', '？']\n",
      "00:25:53.953751 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '明', '星', '的', '高', '跟', '鞋', '谁', '最', '奇', '特', '？', '[SEP]']\n",
      "00:25:53.953936 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 14\n",
      "00:25:53.954113 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['女', '明', '星', ',', '景', '甜', ',', '冉', '莹', '颖...'蹄', '鞋', ',', '恨', '天', '高', ',', '连', '衣', '裙']\n",
      "00:25:53.955109 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '女', '明', '星', '的', '高', '跟', '鞋', '谁'... ',', '恨', '天', '高', ',', '连', '衣', '裙', '[SEP]']\n",
      "00:25:53.955411 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 48\n",
      "00:25:53.955607 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1957, 3209, 3215, 4638, 7770, 6656, 7490, ...17, 2616, 1921, 7770, 117, 6825, 6132, 6170, 102]\n",
      "00:25:53.955819 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1957, 3209, 3215, 4638, 7770, 6656...1921, 7770,  117, 6825, 6132,        6170,  102])\n",
      "00:25:53.956001 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.956645 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.957367 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.958610 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1957, 3209, 3215, 4638, 7770, 665... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008805\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 37\n",
      "00:25:53.961067 call        14     def __getitem__(self, idx):\n",
      "00:25:53.961135 line        15         if self.mode == \"test\":\n",
      "00:25:53.961171 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '鹿晗出行澳门被网友偶遇，身旁同行的女孩亮了，有点眼熟'\n",
      "New var:....... text_b = '鹿晗,澳门,粉丝'\n",
      "New var:....... label = 0\n",
      "00:25:53.961734 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.961841 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.961922 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.962203 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['鹿', '晗', '出', '行', '澳', '门', '被', '网', '友', '偶...'的', '女', '孩', '亮', '了', '，', '有', '点', '眼', '熟']\n",
      "00:25:53.962961 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '鹿', '晗', '出', '行', '澳', '门', '被', '网'... '孩', '亮', '了', '，', '有', '点', '眼', '熟', '[SEP]']\n",
      "00:25:53.963616 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:53.963887 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['鹿', '晗', ',', '澳', '门', ',', '粉', '丝']\n",
      "00:25:53.964272 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '鹿', '晗', '出', '行', '澳', '门', '被', '网'... '鹿', '晗', ',', '澳', '门', ',', '粉', '丝', '[SEP]']\n",
      "00:25:53.964473 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 9\n",
      "00:25:53.964664 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7922, 3240, 1139, 6121, 4078, 7305, 6158, ...7922, 3240, 117, 4078, 7305, 117, 5106, 692, 102]\n",
      "00:25:53.964872 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7922, 3240, 1139, 6121, 4078, 7305... 117, 4078, 7305,  117, 5106,  692,         102])\n",
      "00:25:53.965151 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.965775 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0,        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.966370 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.967244 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7922, 3240, 1139, 6121, 4078, 730... 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008019\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 38\n",
      "00:25:53.969117 call        14     def __getitem__(self, idx):\n",
      "00:25:53.969187 line        15         if self.mode == \"test\":\n",
      "00:25:53.969226 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = 'Jasper为应采儿庆祝母亲节用这招，网友：心都化了'\n",
      "New var:....... text_b = '爸爸去哪儿,社交网站,母亲节,陈小春,爸爸去哪儿5,应采儿,Jasper'\n",
      "New var:....... label = 0\n",
      "00:25:53.969839 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.969932 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.970007 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.970218 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['j', '##as', '##per', '为', '应', '采', '儿', '庆', ...'这', '招', '，', '网', '友', '：', '心', '都', '化', '了']\n",
      "00:25:53.970978 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'j', '##as', '##per', '为', '应', '采', '... '，', '网', '友', '：', '心', '都', '化', '了', '[SEP]']\n",
      "00:25:53.971170 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:53.971354 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['爸', '爸', '去', '哪', '儿', ',', '社', '交', '网', '站...', ',', '应', '采', '儿', ',', 'j', '##as', '##per']\n",
      "00:25:53.972125 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', 'j', '##as', '##per', '为', '应', '采', '...应', '采', '儿', ',', 'j', '##as', '##per', '[SEP]']\n",
      "00:25:53.972324 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 34\n",
      "00:25:53.972556 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 152, 8576, 9063, 711, 2418, 7023, 1036, 24...117, 2418, 7023, 1036, 117, 152, 8576, 9063, 102]\n",
      "00:25:53.972833 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  152, 8576, 9063,  711, 2418, 7023... 2418, 7023, 1036,  117,  152, 8576, 9063,  102])\n",
      "00:25:53.973051 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.973712 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.974427 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.975723 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  152, 8576, 9063,  711, 2418, 702...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009007\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 39\n",
      "00:25:53.978152 call        14     def __getitem__(self, idx):\n",
      "00:25:53.978220 line        15         if self.mode == \"test\":\n",
      "00:25:53.978258 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '娱乐圈中男扮女装的男星们，有的变身萌妹，有的辣眼睛！'\n",
      "New var:....... text_b = '美人痣,男星,娱乐圈,马天宇,萌妹子'\n",
      "New var:....... label = 0\n",
      "00:25:53.978756 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.978847 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.978918 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.979153 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['娱', '乐', '圈', '中', '男', '扮', '女', '装', '的', '男...'身', '萌', '妹', '，', '有', '的', '辣', '眼', '睛', '！']\n",
      "00:25:53.979800 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '中', '男', '扮', '女', '装'... '妹', '，', '有', '的', '辣', '眼', '睛', '！', '[SEP]']\n",
      "00:25:53.979988 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 28\n",
      "00:25:53.980168 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['美', '人', '痣', ',', '男', '星', ',', '娱', '乐', '圈', ',', '马', '天', '宇', ',', '萌', '妹', '子']\n",
      "00:25:53.980745 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '娱', '乐', '圈', '中', '男', '扮', '女', '装'... ',', '马', '天', '宇', ',', '萌', '妹', '子', '[SEP]']\n",
      "00:25:53.980938 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:25:53.981166 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2031, 727, 1750, 704, 4511, 2815, 1957, 61...17, 7716, 1921, 2126, 117, 5846, 1987, 2094, 102]\n",
      "00:25:53.981369 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2031,  727, 1750,  704, 4511, 2815... 7716, 1921, 2126,  117, 5846, 1987, 2094,  102])\n",
      "00:25:53.981576 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.982281 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.982954 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.983954 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2031,  727, 1750,  704, 4511, 281... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007886\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 40\n",
      "00:25:53.986068 call        14     def __getitem__(self, idx):\n",
      "00:25:53.986139 line        15         if self.mode == \"test\":\n",
      "00:25:53.986177 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '高颜值王源也会选错配件，贵气风衣却搭地摊包包，效果一言难尽'\n",
      "New var:....... text_b = '牛仔服,邮差包,王源,Salvatore,大主宰,Ferragamo'\n",
      "New var:....... label = 0\n",
      "00:25:53.986662 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.986753 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.986824 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.987002 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['高', '颜', '值', '王', '源', '也', '会', '选', '错', '配...'摊', '包', '包', '，', '效', '果', '一', '言', '难', '尽']\n",
      "00:25:53.987707 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '高', '颜', '值', '王', '源', '也', '会', '选'... '包', '，', '效', '果', '一', '言', '难', '尽', '[SEP]']\n",
      "00:25:53.987931 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:53.988112 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['牛', '仔', '服', ',', '邮', '差', '包', ',', '王', '源...salvatore', ',', '大', '主', '宰', ',', 'ferragamo']\n",
      "00:25:53.988819 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '高', '颜', '值', '王', '源', '也', '会', '选'...', ',', '大', '主', '宰', ',', 'ferragamo', '[SEP]']\n",
      "00:25:53.989034 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 19\n",
      "00:25:53.989234 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7770, 7582, 966, 4374, 3975, 738, 833, 684...117, 10078, 117, 1920, 712, 2153, 117, 9992, 102]\n",
      "00:25:53.989447 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  7770,  7582,   966,  4374,  3975...  117,  1920,   712,  2153,   117,  9992,   102])\n",
      "00:25:53.989662 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.990313 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1])\n",
      "00:25:53.990986 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:53.992035 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  7770,  7582,   966,  4374,  397...1, 1, 1, 1, 1, 1, 1, 1,        1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008066\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 41\n",
      "00:25:53.994161 call        14     def __getitem__(self, idx):\n",
      "00:25:53.994229 line        15         if self.mode == \"test\":\n",
      "00:25:53.994266 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '王菲私人小号疑似曝光，分享生活日常还晒与谢霆锋合影'\n",
      "New var:....... text_b = '分享生活日常,王菲,谢霆锋,谢霆锋合影王菲私人小号,王菲私人小号疑似曝光'\n",
      "New var:....... label = 0\n",
      "00:25:53.994731 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:53.994821 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:53.994893 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:53.995126 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['王', '菲', '私', '人', '小', '号', '疑', '似', '曝', '光...'日', '常', '还', '晒', '与', '谢', '霆', '锋', '合', '影']\n",
      "00:25:53.995823 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '菲', '私', '人', '小', '号', '疑', '似'... '还', '晒', '与', '谢', '霆', '锋', '合', '影', '[SEP]']\n",
      "00:25:53.996011 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:25:53.996194 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['分', '享', '生', '活', '日', '常', ',', '王', '菲', ',...'王', '菲', '私', '人', '小', '号', '疑', '似', '曝', '光']\n",
      "00:25:53.997004 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '王', '菲', '私', '人', '小', '号', '疑', '似'... '私', '人', '小', '号', '疑', '似', '曝', '光', '[SEP]']\n",
      "00:25:53.997240 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 37\n",
      "00:25:53.997541 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4374, 5838, 4900, 782, 2207, 1384, 4542, 8...900, 782, 2207, 1384, 4542, 849, 3284, 1045, 102]\n",
      "00:25:53.997807 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4374, 5838, 4900,  782, 2207, 1384...2207, 1384, 4542,         849, 3284, 1045,  102])\n",
      "00:25:53.998229 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:53.998945 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:53.999790 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.000979 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4374, 5838, 4900,  782, 2207, 138... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009039\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 42\n",
      "00:25:54.003230 call        14     def __getitem__(self, idx):\n",
      "00:25:54.003301 line        15         if self.mode == \"test\":\n",
      "00:25:54.003338 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '很快和鹿晗结婚？关晓彤的回答太甜蜜了！'\n",
      "New var:....... text_b = '鹿晗,关晓彤,粉丝,董子健,男明星'\n",
      "New var:....... label = 0\n",
      "00:25:54.003892 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.003985 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.004060 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.004382 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['很', '快', '和', '鹿', '晗', '结', '婚', '？', '关', '晓', '彤', '的', '回', '答', '太', '甜', '蜜', '了', '！']\n",
      "00:25:54.004932 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '很', '快', '和', '鹿', '晗', '结', '婚', '？'... '的', '回', '答', '太', '甜', '蜜', '了', '！', '[SEP]']\n",
      "00:25:54.005119 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:25:54.005301 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['鹿', '晗', ',', '关', '晓', '彤', ',', '粉', '丝', ',', '董', '子', '健', ',', '男', '明', '星']\n",
      "00:25:54.005798 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '很', '快', '和', '鹿', '晗', '结', '婚', '？'... ',', '董', '子', '健', ',', '男', '明', '星', '[SEP]']\n",
      "00:25:54.005990 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:54.006180 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2523, 2571, 1469, 7922, 3240, 5310, 2042, ...117, 5869, 2094, 978, 117, 4511, 3209, 3215, 102]\n",
      "00:25:54.006417 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2523, 2571, 1469, 7922, 3240, 5310...2094,  978,  117, 4511,        3209, 3215,  102])\n",
      "00:25:54.006689 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.007274 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.007816 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.008892 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2523, 2571, 1469, 7922, 3240, 531... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007480\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 43\n",
      "00:25:54.010738 call        14     def __getitem__(self, idx):\n",
      "00:25:54.010806 line        15         if self.mode == \"test\":\n",
      "00:25:54.010842 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '唐嫣身穿时尚现身机场，网友：正值颜值最巅峰'\n",
      "New var:....... text_b = '唐嫣,颜值,东方IC,机场,东方ic'\n",
      "New var:....... label = 0\n",
      "00:25:54.011335 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.011495 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.011568 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.011788 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['唐', '嫣', '身', '穿', '时', '尚', '现', '身', '机', '场...'网', '友', '：', '正', '值', '颜', '值', '最', '巅', '峰']\n",
      "00:25:54.012354 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '唐', '嫣', '身', '穿', '时', '尚', '现', '身'... '：', '正', '值', '颜', '值', '最', '巅', '峰', '[SEP]']\n",
      "00:25:54.012538 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 23\n",
      "00:25:54.012686 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['唐', '嫣', ',', '颜', '值', ',', '东', '方', 'ic', ',', '机', '场', ',', '东', '方', 'ic']\n",
      "00:25:54.013153 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '唐', '嫣', '身', '穿', '时', '尚', '现', '身'...ic', ',', '机', '场', ',', '东', '方', 'ic', '[SEP]']\n",
      "00:25:54.013345 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 17\n",
      "00:25:54.013534 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1538, 2073, 6716, 4959, 3198, 2213, 4385, ...8577, 117, 3322, 1767, 117, 691, 3175, 8577, 102]\n",
      "00:25:54.013865 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1538, 2073, 6716, 4959, 3198, 2213...3322, 1767,  117,         691, 3175, 8577,  102])\n",
      "00:25:54.014101 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.014651 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.015267 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.016237 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1538, 2073, 6716, 4959, 3198, 221... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007297\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 44\n",
      "00:25:54.018062 call        14     def __getitem__(self, idx):\n",
      "00:25:54.018129 line        15         if self.mode == \"test\":\n",
      "00:25:54.018166 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '这款小型SUV比宝骏510智能，难怪市场下滑也能连续6个月过万辆'\n",
      "New var:....... text_b = '极限挑战,远景X3,真质良品SUV,优等生,起跑线,吉利汽车'\n",
      "New var:....... label = 0\n",
      "00:25:54.018641 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.018828 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.018907 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.019183 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['这', '款', '小', '型', 'suv', '比', '宝', '骏', '510'...'也', '能', '连', '续', '6', '个', '月', '过', '万', '辆']\n",
      "00:25:54.019834 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '款', '小', '型', 'suv', '比', '宝', '... '连', '续', '6', '个', '月', '过', '万', '辆', '[SEP]']\n",
      "00:25:54.020503 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:25:54.020623 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['极', '限', '挑', '战', ',', '远', '景', 'x3', ',', '...'生', ',', '起', '跑', '线', ',', '吉', '利', '汽', '车']\n",
      "00:25:54.021281 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '款', '小', '型', 'suv', '比', '宝', '... '起', '跑', '线', ',', '吉', '利', '汽', '车', '[SEP]']\n",
      "00:25:54.021604 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 28\n",
      "00:25:54.021859 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6821, 3621, 2207, 1798, 8540, 3683, 2140, ...29, 6651, 5296, 117, 1395, 1164, 3749, 6756, 102]\n",
      "00:25:54.024313 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  6821,  3621,  2207,  1798,  8540... 5296,   117,  1395,  1164,  3749,  6756,   102])\n",
      "00:25:54.024824 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.025598 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.026837 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.028272 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  6821,  3621,  2207,  1798,  854...       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.012972\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 45\n",
      "00:25:54.031070 call        14     def __getitem__(self, idx):\n",
      "00:25:54.031143 line        15         if self.mode == \"test\":\n",
      "00:25:54.031181 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '后宫才华服饰比美，贵妃创意被皇后夺取，现代女子帮贵妃扭转乾坤'\n",
      "New var:....... text_b = '皇后,后宫,服饰'\n",
      "New var:....... label = 0\n",
      "00:25:54.031774 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.031869 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.032240 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.032484 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['后', '宫', '才', '华', '服', '饰', '比', '美', '，', '贵...'代', '女', '子', '帮', '贵', '妃', '扭', '转', '乾', '坤']\n",
      "00:25:54.033272 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '后', '宫', '才', '华', '服', '饰', '比', '美'... '子', '帮', '贵', '妃', '扭', '转', '乾', '坤', '[SEP]']\n",
      "00:25:54.033509 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:54.033736 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['皇', '后', ',', '后', '宫', ',', '服', '饰']\n",
      "00:25:54.034173 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '后', '宫', '才', '华', '服', '饰', '比', '美'... '皇', '后', ',', '后', '宫', ',', '服', '饰', '[SEP]']\n",
      "00:25:54.034539 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 9\n",
      "00:25:54.034782 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1400, 2151, 2798, 1290, 3302, 7652, 3683, ...640, 1400, 117, 1400, 2151, 117, 3302, 7652, 102]\n",
      "00:25:54.035035 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1400, 2151, 2798, 1290, 3302, 7652... 117, 1400,        2151,  117, 3302, 7652,  102])\n",
      "00:25:54.035295 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.035932 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.036784 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.038003 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1400, 2151, 2798, 1290, 3302, 765... 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009391\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 46\n",
      "00:25:54.040492 call        14     def __getitem__(self, idx):\n",
      "00:25:54.040565 line        15         if self.mode == \"test\":\n",
      "00:25:54.040603 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '杨幂旗下9位小花旦，我猜你只认识迪丽热巴，能叫出名字，算你牛'\n",
      "New var:....... text_b = '铁粉杨幂旗下,迪丽热巴,我猜,幂幂,花旦'\n",
      "New var:....... label = 0\n",
      "00:25:54.041197 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.041299 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.041511 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.041967 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['杨', '幂', '旗', '下', '9', '位', '小', '花', '旦', '，...'，', '能', '叫', '出', '名', '字', '，', '算', '你', '牛']\n",
      "00:25:54.042765 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '幂', '旗', '下', '9', '位', '小', '花'... '叫', '出', '名', '字', '，', '算', '你', '牛', '[SEP]']\n",
      "00:25:54.043031 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:54.043264 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['铁', '粉', '杨', '幂', '旗', '下', ',', '迪', '丽', '热', '巴', ',', '我', '猜', ',', '幂', '幂', ',', '花', '旦']\n",
      "00:25:54.043859 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '杨', '幂', '旗', '下', '9', '位', '小', '花'... '我', '猜', ',', '幂', '幂', ',', '花', '旦', '[SEP]']\n",
      "00:25:54.044212 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 21\n",
      "00:25:54.044451 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3342, 2386, 3186, 678, 130, 855, 2207, 570...769, 4339, 117, 2386, 2386, 117, 5709, 3190, 102]\n",
      "00:25:54.044704 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3342, 2386, 3186,  678,  130,  855... 117, 2386,        2386,  117, 5709, 3190,  102])\n",
      "00:25:54.044964 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.045669 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1])\n",
      "00:25:54.046636 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.048036 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3342, 2386, 3186,  678,  130,  85...1, 1, 1, 1, 1,        1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010170\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 47\n",
      "00:25:54.050694 call        14     def __getitem__(self, idx):\n",
      "00:25:54.050766 line        15         if self.mode == \"test\":\n",
      "00:25:54.050803 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '舒淇出席某活动 网友：看到她的腿，就不在乎她的嘴！'\n",
      "New var:....... text_b = '舒淇出席,舒淇'\n",
      "New var:....... label = 0\n",
      "00:25:54.051377 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.051600 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.051678 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.051931 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['舒', '淇', '出', '席', '某', '活', '动', '网', '友', '：...'腿', '，', '就', '不', '在', '乎', '她', '的', '嘴', '！']\n",
      "00:25:54.052610 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '舒', '淇', '出', '席', '某', '活', '动', '网'... '就', '不', '在', '乎', '她', '的', '嘴', '！', '[SEP]']\n",
      "00:25:54.052844 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:54.053073 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['舒', '淇', '出', '席', ',', '舒', '淇']\n",
      "00:25:54.053449 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '舒', '淇', '出', '席', '某', '活', '动', '网'...EP]', '舒', '淇', '出', '席', ',', '舒', '淇', '[SEP]']\n",
      "00:25:54.053799 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 8\n",
      "00:25:54.054034 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5653, 3899, 1139, 2375, 3378, 3833, 1220, ...02, 5653, 3899, 1139, 2375, 117, 5653, 3899, 102]\n",
      "00:25:54.054279 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5653, 3899, 1139, 2375, 3378, 3833... 5653, 3899, 1139, 2375,  117, 5653, 3899,  102])\n",
      "00:25:54.054534 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.055127 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0,        0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.055811 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.056915 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5653, 3899, 1139, 2375, 3378, 383...       0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008390\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 48\n",
      "00:25:54.059119 call        14     def __getitem__(self, idx):\n",
      "00:25:54.059194 line        15         if self.mode == \"test\":\n",
      "00:25:54.059232 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '推五部将军为背景的古言情小说，第二部有点像女版的琅琊榜'\n",
      "New var:....... text_b = '将军有喜,琅琊榜,将军叼回个小娇娘,男主,女主,穿越文,大将军,将军策嫡女权谋'\n",
      "New var:....... label = 0\n",
      "00:25:54.059791 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.059883 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.059958 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.060189 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['推', '五', '部', '将', '军', '为', '背', '景', '的', '古...'部', '有', '点', '像', '女', '版', '的', '琅', '琊', '榜']\n",
      "00:25:54.060911 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '推', '五', '部', '将', '军', '为', '背', '景'... '点', '像', '女', '版', '的', '琅', '琊', '榜', '[SEP]']\n",
      "00:25:54.061290 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 29\n",
      "00:25:54.061547 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['将', '军', '有', '喜', ',', '琅', '琊', '榜', ',', '将...'将', '军', ',', '将', '军', '策', '嫡', '女', '权', '谋']\n",
      "00:25:54.062471 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '推', '五', '部', '将', '军', '为', '背', '景'... ',', '将', '军', '策', '嫡', '女', '权', '谋', '[SEP]']\n",
      "00:25:54.062721 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 40\n",
      "00:25:54.062962 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2972, 758, 6956, 2199, 1092, 711, 5520, 32...7, 2199, 1092, 5032, 2072, 1957, 3326, 6450, 102]\n",
      "00:25:54.063225 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2972,  758, 6956, 2199, 1092,  711... 2199, 1092, 5032, 2072, 1957, 3326, 6450,  102])\n",
      "00:25:54.063602 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.064405 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.065308 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.066842 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2972,  758, 6956, 2199, 1092,  71... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010602\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 49\n",
      "00:25:54.069751 call        14     def __getitem__(self, idx):\n",
      "00:25:54.069820 line        15         if self.mode == \"test\":\n",
      "00:25:54.069857 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '朱亚文出身于什么样的家庭？'\n",
      "New var:....... text_b = '父辈的旗帜,父傲,艺人,朱亚文,娱乐圈,闯关东'\n",
      "New var:....... label = 0\n",
      "00:25:54.070383 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.070474 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.070544 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.070705 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['朱', '亚', '文', '出', '身', '于', '什', '么', '样', '的', '家', '庭', '？']\n",
      "00:25:54.071120 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '朱', '亚', '文', '出', '身', '于', '什', '么', '样', '的', '家', '庭', '？', '[SEP]']\n",
      "00:25:54.071306 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "00:25:54.071481 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['父', '辈', '的', '旗', '帜', ',', '父', '傲', ',', '艺...'亚', '文', ',', '娱', '乐', '圈', ',', '闯', '关', '东']\n",
      "00:25:54.072073 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '朱', '亚', '文', '出', '身', '于', '什', '么'... ',', '娱', '乐', '圈', ',', '闯', '关', '东', '[SEP]']\n",
      "00:25:54.072326 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 24\n",
      "00:25:54.072515 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3319, 762, 3152, 1139, 6716, 754, 784, 720... 117, 2031, 727, 1750, 117, 7310, 1068, 691, 102]\n",
      "00:25:54.072713 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3319,  762, 3152, 1139, 6716,  754... 727, 1750,  117, 7310,        1068,  691,  102])\n",
      "00:25:54.072917 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.073490 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.074114 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.075130 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3319,  762, 3152, 1139, 6716,  75... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007381\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 50\n",
      "00:25:54.077238 call        14     def __getitem__(self, idx):\n",
      "00:25:54.077312 line        15         if self.mode == \"test\":\n",
      "00:25:54.077350 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '林心如曾是多少人的梦中情人，现在她当妈了，非常受宝宝喜欢'\n",
      "New var:....... text_b = '霍建华,林心如,小孩子'\n",
      "New var:....... label = 0\n",
      "00:25:54.077920 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.078014 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.078189 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.078372 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['林', '心', '如', '曾', '是', '多', '少', '人', '的', '梦...'妈', '了', '，', '非', '常', '受', '宝', '宝', '喜', '欢']\n",
      "00:25:54.079067 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '心', '如', '曾', '是', '多', '少', '人'... '，', '非', '常', '受', '宝', '宝', '喜', '欢', '[SEP]']\n",
      "00:25:54.079256 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:25:54.079506 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['霍', '建', '华', ',', '林', '心', '如', ',', '小', '孩', '子']\n",
      "00:25:54.079905 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '林', '心', '如', '曾', '是', '多', '少', '人'... ',', '林', '心', '如', ',', '小', '孩', '子', '[SEP]']\n",
      "00:25:54.080097 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 12\n",
      "00:25:54.080288 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 3360, 2552, 1963, 3295, 3221, 1914, 2208, ...17, 3360, 2552, 1963, 117, 2207, 2111, 2094, 102]\n",
      "00:25:54.080487 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 3360, 2552, 1963, 3295, 3221, 1914...2552,        1963,  117, 2207, 2111, 2094,  102])\n",
      "00:25:54.080692 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.081296 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.081960 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.082951 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 3360, 2552, 1963, 3295, 3221, 191... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007648\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 51\n",
      "00:25:54.084917 call        14     def __getitem__(self, idx):\n",
      "00:25:54.084990 line        15         if self.mode == \"test\":\n",
      "00:25:54.085028 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '陈乔恩携爱犬登封面 放慢脚步享受友爱生活'\n",
      "New var:....... text_b = '爱犬,陈乔恩'\n",
      "New var:....... label = 0\n",
      "00:25:54.085583 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.085678 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.085756 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.085933 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['陈', '乔', '恩', '携', '爱', '犬', '登', '封', '面', '放', '慢', '脚', '步', '享', '受', '友', '爱', '生', '活']\n",
      "00:25:54.086470 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '乔', '恩', '携', '爱', '犬', '登', '封'... '脚', '步', '享', '受', '友', '爱', '生', '活', '[SEP]']\n",
      "00:25:54.086653 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:25:54.086831 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['爱', '犬', ',', '陈', '乔', '恩']\n",
      "00:25:54.087229 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '陈', '乔', '恩', '携', '爱', '犬', '登', '封'..., '[SEP]', '爱', '犬', ',', '陈', '乔', '恩', '[SEP]']\n",
      "00:25:54.087424 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 7\n",
      "00:25:54.087608 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 7357, 730, 2617, 3025, 4263, 4305, 4633, 2...3833, 102, 4263, 4305, 117, 7357, 730, 2617, 102]\n",
      "00:25:54.087801 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 7357,  730, 2617, 3025, 4263, 4305...4263, 4305,  117,        7357,  730, 2617,  102])\n",
      "00:25:54.088002 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.088460 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,        1, 1, 1, 1])\n",
      "00:25:54.088974 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.089876 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 7357,  730, 2617, 3025, 4263, 430...0, 0, 0, 1, 1, 1,        1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006130\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 52\n",
      "00:25:54.091078 call        14     def __getitem__(self, idx):\n",
      "00:25:54.091142 line        15         if self.mode == \"test\":\n",
      "00:25:54.091178 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '周丽淇出席某活动，网友：她长得很有辨识度看着舒服，保养得很好'\n",
      "New var:....... text_b = '周丽淇,辨识度'\n",
      "New var:....... label = 0\n",
      "00:25:54.091737 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.091823 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.091892 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.092108 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['周', '丽', '淇', '出', '席', '某', '活', '动', '，', '网...'看', '着', '舒', '服', '，', '保', '养', '得', '很', '好']\n",
      "00:25:54.092972 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '周', '丽', '淇', '出', '席', '某', '活', '动'... '舒', '服', '，', '保', '养', '得', '很', '好', '[SEP]']\n",
      "00:25:54.093179 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 32\n",
      "00:25:54.093369 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['周', '丽', '淇', ',', '辨', '识', '度']\n",
      "00:25:54.093709 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '周', '丽', '淇', '出', '席', '某', '活', '动'...EP]', '周', '丽', '淇', ',', '辨', '识', '度', '[SEP]']\n",
      "00:25:54.093905 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 8\n",
      "00:25:54.094095 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 1453, 714, 3899, 1139, 2375, 3378, 3833, 1...102, 1453, 714, 3899, 117, 6795, 6399, 2428, 102]\n",
      "00:25:54.094371 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 1453,  714, 3899, 1139, 2375, 3378... 714, 3899,  117,        6795, 6399, 2428,  102])\n",
      "00:25:54.094648 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.095190 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.095733 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.096792 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 1453,  714, 3899, 1139, 2375, 337... 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007497\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 53\n",
      "00:25:54.098608 call        14     def __getitem__(self, idx):\n",
      "00:25:54.098680 line        15         if self.mode == \"test\":\n",
      "00:25:54.098718 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '维密超模后台化妆照，展示魅力天使容颜和独特的 风采！'\n",
      "New var:....... text_b = '安布罗,时装周,安布罗休,奥尔德里奇,维密'\n",
      "New var:....... label = 0\n",
      "00:25:54.099342 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.099512 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.099659 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.099839 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['维', '密', '超', '模', '后', '台', '化', '妆', '照', '，...'使', '容', '颜', '和', '独', '特', '的', '风', '采', '！']\n",
      "00:25:54.100499 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '维', '密', '超', '模', '后', '台', '化', '妆'... '颜', '和', '独', '特', '的', '风', '采', '！', '[SEP]']\n",
      "00:25:54.100690 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:25:54.100872 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['安', '布', '罗', ',', '时', '装', '周', ',', '安', '布...'休', ',', '奥', '尔', '德', '里', '奇', ',', '维', '密']\n",
      "00:25:54.101436 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '维', '密', '超', '模', '后', '台', '化', '妆'... '奥', '尔', '德', '里', '奇', ',', '维', '密', '[SEP]']\n",
      "00:25:54.101698 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 22\n",
      "00:25:54.101892 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5335, 2166, 6631, 3563, 1400, 1378, 1265, ...52, 2209, 2548, 7027, 1936, 117, 5335, 2166, 102]\n",
      "00:25:54.102101 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5335, 2166, 6631, 3563, 1400, 1378...2548, 7027, 1936,  117, 5335, 2166,         102])\n",
      "00:25:54.102320 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.102919 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:25:54.103524 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.104569 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5335, 2166, 6631, 3563, 1400, 137...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007897\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 54\n",
      "00:25:54.106543 call        14     def __getitem__(self, idx):\n",
      "00:25:54.106618 line        15         if self.mode == \"test\":\n",
      "00:25:54.106742 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '拿到多位业界大佬投资，她如何在内容和商业教育领域立下壁垒'\n",
      "New var:....... text_b = '毛大庆,吴婷,我有嘉宾,嘉宾派,嘉宾大学,科大讯飞'\n",
      "New var:....... label = 0\n",
      "00:25:54.107421 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.107520 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.107608 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.107764 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['拿', '到', '多', '位', '业', '界', '大', '佬', '投', '资...'商', '业', '教', '育', '领', '域', '立', '下', '壁', '垒']\n",
      "00:25:54.108611 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '拿', '到', '多', '位', '业', '界', '大', '佬'... '教', '育', '领', '域', '立', '下', '壁', '垒', '[SEP]']\n",
      "00:25:54.109033 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 30\n",
      "00:25:54.109281 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['毛', '大', '庆', ',', '吴', '婷', ',', '我', '有', '嘉...',', '嘉', '宾', '大', '学', ',', '科', '大', '讯', '飞']\n",
      "00:25:54.109944 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '拿', '到', '多', '位', '业', '界', '大', '佬'... '宾', '大', '学', ',', '科', '大', '讯', '飞', '[SEP]']\n",
      "00:25:54.110096 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 26\n",
      "00:25:54.110334 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2897, 1168, 1914, 855, 689, 4518, 1920, 87...61, 1920, 2110, 117, 4906, 1920, 6380, 7607, 102]\n",
      "00:25:54.110563 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2897, 1168, 1914,  855,  689, 4518... 1920, 2110,  117, 4906, 1920, 6380, 7607,  102])\n",
      "00:25:54.110823 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.111849 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.112743 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.114185 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2897, 1168, 1914,  855,  689, 451...1, 1,        1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010089\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 55\n",
      "00:25:54.116663 call        14     def __getitem__(self, idx):\n",
      "00:25:54.116733 line        15         if self.mode == \"test\":\n",
      "00:25:54.116770 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '贾斯汀比伯和薛之谦可比吗？'\n",
      "New var:....... text_b = 'Justin Bieber,Sorry,What Do You Mean ?,I ’ ll Show You,福布斯,薛之谦,贾斯汀比伯,Purpose,侃爷,FEAR'\n",
      "New var:....... label = 0\n",
      "00:25:54.117347 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.117440 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.117593 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.117767 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['贾', '斯', '汀', '比', '伯', '和', '薛', '之', '谦', '可', '比', '吗', '？']\n",
      "00:25:54.118206 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '贾', '斯', '汀', '比', '伯', '和', '薛', '之', '谦', '可', '比', '吗', '？', '[SEP]']\n",
      "00:25:54.118387 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "00:25:54.118612 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['justin', 'bi', '##e', '##ber', ',', 'sorry', '...##rp', '##ose', ',', '侃', '爷', ',', 'fe', '##ar']\n",
      "00:25:54.119773 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '贾', '斯', '汀', '比', '伯', '和', '薛', '之'...#ose', ',', '侃', '爷', ',', 'fe', '##ar', '[SEP]']\n",
      "00:25:54.119974 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 44\n",
      "00:25:54.120139 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6593, 3172, 3722, 3683, 843, 1469, 5955, 7...80, 10936, 117, 887, 4267, 117, 12605, 8458, 102]\n",
      "00:25:54.120402 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([  101,  6593,  3172,  3722,  3683,   843...  117,   887,  4267,   117, 12605,  8458,   102])\n",
      "00:25:54.120638 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.121391 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.122069 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.123315 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([  101,  6593,  3172,  3722,  3683,   84...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.009007\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 56\n",
      "00:25:54.125699 call        14     def __getitem__(self, idx):\n",
      "00:25:54.125799 line        15         if self.mode == \"test\":\n",
      "00:25:54.125837 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《犬夜叉》里的玲是不是人？'\n",
      "New var:....... text_b = '犬夜叉,杀生丸,日暮戈薇,大结局,天生牙,四魂之玉'\n",
      "New var:....... label = 0\n",
      "00:25:54.126403 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.126498 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.126573 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.126879 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '犬', '夜', '叉', '》', '里', '的', '玲', '是', '不', '是', '人', '？']\n",
      "00:25:54.127319 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '犬', '夜', '叉', '》', '里', '的', '玲', '是', '不', '是', '人', '？', '[SEP]']\n",
      "00:25:54.127592 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 15\n",
      "00:25:54.127818 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['犬', '夜', '叉', ',', '杀', '生', '丸', ',', '日', '暮...'局', ',', '天', '生', '牙', ',', '四', '魂', '之', '玉']\n",
      "00:25:54.128492 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '犬', '夜', '叉', '》', '里', '的', '玲'... '天', '生', '牙', ',', '四', '魂', '之', '玉', '[SEP]']\n",
      "00:25:54.128696 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 26\n",
      "00:25:54.128890 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 4305, 1915, 1349, 518, 7027, 4638, 43...921, 4495, 4280, 117, 1724, 7789, 722, 4373, 102]\n",
      "00:25:54.129163 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 4305, 1915, 1349,  518, 7027...4280,  117,        1724, 7789,  722, 4373,  102])\n",
      "00:25:54.129377 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.129965 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.130552 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.131557 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 4305, 1915, 1349,  518, 702... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007732\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 57\n",
      "00:25:54.133465 call        14     def __getitem__(self, idx):\n",
      "00:25:54.133537 line        15         if self.mode == \"test\":\n",
      "00:25:54.133575 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '片场偶遇刘涛新剧造型 网友：太有年代感了'\n",
      "New var:....... text_b = '刘涛,片场,新剧'\n",
      "New var:....... label = 0\n",
      "00:25:54.134162 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.134331 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.134416 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.134621 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['片', '场', '偶', '遇', '刘', '涛', '新', '剧', '造', '型', '网', '友', '：', '太', '有', '年', '代', '感', '了']\n",
      "00:25:54.135182 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '片', '场', '偶', '遇', '刘', '涛', '新', '剧'... '友', '：', '太', '有', '年', '代', '感', '了', '[SEP]']\n",
      "00:25:54.135369 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 21\n",
      "00:25:54.135550 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['刘', '涛', ',', '片', '场', ',', '新', '剧']\n",
      "00:25:54.135894 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '片', '场', '偶', '遇', '刘', '涛', '新', '剧'... '刘', '涛', ',', '片', '场', ',', '新', '剧', '[SEP]']\n",
      "00:25:54.136082 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 9\n",
      "00:25:54.136267 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4275, 1767, 981, 6878, 1155, 3875, 3173, 1...155, 3875, 117, 4275, 1767, 117, 3173, 1196, 102]\n",
      "00:25:54.136606 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4275, 1767,  981, 6878, 1155, 3875... 117,        4275, 1767,  117, 3173, 1196,  102])\n",
      "00:25:54.136831 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.137313 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...0, 0, 0, 0, 0, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:25:54.137786 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.138537 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4275, 1767,  981, 6878, 1155, 387...0, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.006633\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 58\n",
      "00:25:54.140125 call        14     def __getitem__(self, idx):\n",
      "00:25:54.140192 line        15         if self.mode == \"test\":\n",
      "00:25:54.140229 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '虽然不被大多数人知道，但她是个难得一见的好演员'\n",
      "New var:....... text_b = '万箭穿心,成泰燊,霸王别姬,李宝莉,戏曲,密阳,体验派,颜丙燕,香港大营救'\n",
      "New var:....... label = 0\n",
      "00:25:54.140716 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.140809 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.140881 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.141130 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['虽', '然', '不', '被', '大', '多', '数', '人', '知', '道...'是', '个', '难', '得', '一', '见', '的', '好', '演', '员']\n",
      "00:25:54.141736 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '虽', '然', '不', '被', '大', '多', '数', '人'... '难', '得', '一', '见', '的', '好', '演', '员', '[SEP]']\n",
      "00:25:54.141992 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 25\n",
      "00:25:54.142176 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['万', '箭', '穿', '心', ',', '成', '泰', '燊', ',', '霸...',', '颜', '丙', '燕', ',', '香', '港', '大', '营', '救']\n",
      "00:25:54.143011 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '虽', '然', '不', '被', '大', '多', '数', '人'... '丙', '燕', ',', '香', '港', '大', '营', '救', '[SEP]']\n",
      "00:25:54.143280 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 38\n",
      "00:25:54.143537 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6006, 4197, 679, 6158, 1920, 1914, 3144, 7...88, 4242, 117, 7676, 3949, 1920, 5852, 3131, 102]\n",
      "00:25:54.143763 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6006, 4197,  679, 6158, 1920, 1914... 117, 7676, 3949, 1920,        5852, 3131,  102])\n",
      "00:25:54.143983 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.144725 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.145397 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.146580 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6006, 4197,  679, 6158, 1920, 191... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008889\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 59\n",
      "00:25:54.149043 call        14     def __getitem__(self, idx):\n",
      "00:25:54.149114 line        15         if self.mode == \"test\":\n",
      "00:25:54.149151 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '蒋欣生日“两美”送祝福 但与这两位却没互动'\n",
      "New var:....... text_b = '蒋欣,杨紫,欢乐颂,王子文'\n",
      "New var:....... label = 0\n",
      "00:25:54.149667 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.149760 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.149834 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.150090 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['蒋', '欣', '生', '日', '[UNK]', '两', '美', '[UNK]',...'福', '但', '与', '这', '两', '位', '却', '没', '互', '动']\n",
      "00:25:54.150658 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '蒋', '欣', '生', '日', '[UNK]', '两', '美',... '与', '这', '两', '位', '却', '没', '互', '动', '[SEP]']\n",
      "00:25:54.150852 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 22\n",
      "00:25:54.151037 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['蒋', '欣', ',', '杨', '紫', ',', '欢', '乐', '颂', ',', '王', '子', '文']\n",
      "00:25:54.151470 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '蒋', '欣', '生', '日', '[UNK]', '两', '美',... ',', '欢', '乐', '颂', ',', '王', '子', '文', '[SEP]']\n",
      "00:25:54.151665 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "00:25:54.151880 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 5882, 3615, 4495, 3189, 100, 697, 5401, 10...117, 3614, 727, 7563, 117, 4374, 2094, 3152, 102]\n",
      "00:25:54.152147 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 5882, 3615, 4495, 3189,  100,  697... 3614,  727, 7563,  117, 4374, 2094, 3152,  102])\n",
      "00:25:54.152575 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.153213 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1,        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.153733 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.154668 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 5882, 3615, 4495, 3189,  100,  69... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007425\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 60\n",
      "00:25:54.156500 call        14     def __getitem__(self, idx):\n",
      "00:25:54.156571 line        15         if self.mode == \"test\":\n",
      "00:25:54.156610 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '演《破冰者》被称“社会我逗姐” 潘之琳：罗晋大师兄成表演明灯'\n",
      "New var:....... text_b = '破冰者,大师兄,真爱的谎言之破冰者,娘要嫁人,谭逗逗,职场是个技术活,罗晋,老房有喜,潘之琳'\n",
      "New var:....... label = 0\n",
      "00:25:54.157188 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.157419 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.157498 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.157681 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['演', '《', '破', '冰', '者', '》', '被', '称', '[UNK]'...'罗', '晋', '大', '师', '兄', '成', '表', '演', '明', '灯']\n",
      "00:25:54.158408 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '演', '《', '破', '冰', '者', '》', '被', '称'... '大', '师', '兄', '成', '表', '演', '明', '灯', '[SEP]']\n",
      "00:25:54.158641 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:54.158847 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['破', '冰', '者', ',', '大', '师', '兄', ',', '真', '爱...'晋', ',', '老', '房', '有', '喜', ',', '潘', '之', '琳']\n",
      "00:25:54.159848 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '演', '《', '破', '冰', '者', '》', '被', '称'... '老', '房', '有', '喜', ',', '潘', '之', '琳', '[SEP]']\n",
      "00:25:54.160121 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 47\n",
      "00:25:54.160319 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 4028, 517, 4788, 1102, 5442, 518, 6158, 49...439, 2791, 3300, 1599, 117, 4050, 722, 4432, 102]\n",
      "00:25:54.160540 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 4028,  517, 4788, 1102, 5442,  518...3300,        1599,  117, 4050,  722, 4432,  102])\n",
      "00:25:54.160761 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.161587 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1,        1, 1, 1, 1, 1, 1])\n",
      "00:25:54.162535 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.163909 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 4028,  517, 4788, 1102, 5442,  51...1, 1, 1, 1,        1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.010075\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 61\n",
      "00:25:54.166603 call        14     def __getitem__(self, idx):\n",
      "00:25:54.166672 line        15         if self.mode == \"test\":\n",
      "00:25:54.166709 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '《非自然死亡》豆瓣评分9.2 国产剧比日剧到底差在哪'\n",
      "New var:....... text_b = '豆瓣,非自然死亡,石原里美'\n",
      "New var:....... label = 0\n",
      "00:25:54.167212 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.167304 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.167377 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.167660 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['《', '非', '自', '然', '死', '亡', '》', '豆', '瓣', '评...'产', '剧', '比', '日', '剧', '到', '底', '差', '在', '哪']\n",
      "00:25:54.168293 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '非', '自', '然', '死', '亡', '》', '豆'... '比', '日', '剧', '到', '底', '差', '在', '哪', '[SEP]']\n",
      "00:25:54.168483 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 27\n",
      "00:25:54.168666 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['豆', '瓣', ',', '非', '自', '然', '死', '亡', ',', '石', '原', '里', '美']\n",
      "00:25:54.169097 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '《', '非', '自', '然', '死', '亡', '》', '豆'... '然', '死', '亡', ',', '石', '原', '里', '美', '[SEP]']\n",
      "00:25:54.169289 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 14\n",
      "00:25:54.169475 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 517, 7478, 5632, 4197, 3647, 767, 518, 648...197, 3647, 767, 117, 4767, 1333, 7027, 5401, 102]\n",
      "00:25:54.169712 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101,  517, 7478, 5632, 4197, 3647,  767... 767,  117,        4767, 1333, 7027, 5401,  102])\n",
      "00:25:54.169983 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.170544 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.171084 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.172019 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101,  517, 7478, 5632, 4197, 3647,  76... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007321\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 62\n",
      "00:25:54.173952 call        14     def __getitem__(self, idx):\n",
      "00:25:54.174022 line        15         if self.mode == \"test\":\n",
      "00:25:54.174059 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '这两个举动证明：黄圣依在教育孩子方面绝对是把好手'\n",
      "New var:....... text_b = '安迪,妈妈是超人,黄圣依'\n",
      "New var:....... label = 0\n",
      "00:25:54.174529 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.174622 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.174693 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.174871 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['这', '两', '个', '举', '动', '证', '明', '：', '黄', '圣...'孩', '子', '方', '面', '绝', '对', '是', '把', '好', '手']\n",
      "00:25:54.175485 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '两', '个', '举', '动', '证', '明', '：'... '方', '面', '绝', '对', '是', '把', '好', '手', '[SEP]']\n",
      "00:25:54.175734 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 26\n",
      "00:25:54.175953 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['安', '迪', ',', '妈', '妈', '是', '超', '人', ',', '黄', '圣', '依']\n",
      "00:25:54.176366 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '这', '两', '个', '举', '动', '证', '明', '：'... '妈', '是', '超', '人', ',', '黄', '圣', '依', '[SEP]']\n",
      "00:25:54.176557 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 13\n",
      "00:25:54.176744 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 6821, 697, 702, 715, 1220, 6395, 3209, 803...1968, 3221, 6631, 782, 117, 7942, 1760, 898, 102]\n",
      "00:25:54.176946 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 6821,  697,  702,  715, 1220, 6395...6631,  782,  117, 7942,        1760,  898,  102])\n",
      "00:25:54.177150 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.177686 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...    0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "00:25:54.178339 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.179292 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 6821,  697,  702,  715, 1220, 639... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(0))\n",
      "Elapsed time: 00:00:00.007206\n",
      "Starting var:.. self = <__main__.wordclassification_Dataset object at 0x7fc16db12358>\n",
      "Starting var:.. idx = 63\n",
      "00:25:54.181186 call        14     def __getitem__(self, idx):\n",
      "00:25:54.181254 line        15         if self.mode == \"test\":\n",
      "00:25:54.181290 line        19             text_a, text_b, label = self.df.iloc[idx, :].values\n",
      "New var:....... text_a = '安吉劈木板，小鱼儿一字马，胡可为啥这么爱晒娃？这回答机智了'\n",
      "New var:....... text_b = '安吉,跆拳道,男子汉,胡可,小鱼儿'\n",
      "New var:....... label = 0\n",
      "00:25:54.181765 line        21             label_id = label\n",
      "New var:....... label_id = 0\n",
      "00:25:54.181858 line        22             label_tensor = torch.tensor(label_id)\n",
      "New var:....... label_tensor = tensor(0)\n",
      "00:25:54.181975 line        25         word_pieces = [\"[CLS]\"]\n",
      "New var:....... word_pieces = ['[CLS]']\n",
      "00:25:54.182209 line        26         tokens_a = self.tokenizer.tokenize(text_a)\n",
      "New var:....... tokens_a = ['安', '吉', '劈', '木', '板', '，', '小', '鱼', '儿', '一...'爱', '晒', '娃', '？', '这', '回', '答', '机', '智', '了']\n",
      "00:25:54.182918 line        27         word_pieces += tokens_a + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '安', '吉', '劈', '木', '板', '，', '小', '鱼'... '娃', '？', '这', '回', '答', '机', '智', '了', '[SEP]']\n",
      "00:25:54.183146 line        28         len_a = len(word_pieces)\n",
      "New var:....... len_a = 31\n",
      "00:25:54.183394 line        31         tokens_b = self.tokenizer.tokenize(text_b)\n",
      "New var:....... tokens_b = ['安', '吉', ',', '跆', '拳', '道', ',', '男', '子', '汉', ',', '胡', '可', ',', '小', '鱼', '儿']\n",
      "00:25:54.183891 line        32         word_pieces += tokens_b + [\"[SEP]\"]\n",
      "Modified var:.. word_pieces = ['[CLS]', '安', '吉', '劈', '木', '板', '，', '小', '鱼'... '汉', ',', '胡', '可', ',', '小', '鱼', '儿', '[SEP]']\n",
      "00:25:54.184083 line        33         len_b = len(word_pieces) - len_a\n",
      "New var:....... len_b = 18\n",
      "00:25:54.184270 line        36         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
      "New var:....... ids = [101, 2128, 1395, 1207, 3312, 3352, 8024, 2207, ...727, 117, 5529, 1377, 117, 2207, 7824, 1036, 102]\n",
      "00:25:54.184472 line        37         tokens_tensor = torch.tensor(ids)\n",
      "New var:....... tokens_tensor = tensor([ 101, 2128, 1395, 1207, 3312, 3352, 8024...5529, 1377,  117, 2207, 7824, 1036,         102])\n",
      "00:25:54.184679 line        40         segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
      "00:25:54.185313 line        41                                         dtype=torch.long)\n",
      "New var:....... segments_tensor = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1])\n",
      "00:25:54.186050 line        43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "00:25:54.187074 return      43         return (tokens_tensor, segments_tensor, label_tensor)\n",
      "Return value:.. (tensor([ 101, 2128, 1395, 1207, 3312, 3352, 802...1, 1, 1, 1, 1, 1, 1, 1, 1,        1]), tensor(0))\n",
      "Elapsed time: 00:00:00.008081\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "outputs = model(input_ids=tokens_tensors, \n",
    "                token_type_ids=segments_tensors, \n",
    "                attention_mask=masks_tensors)\n",
    "\n",
    "logits = outputs[0]\n",
    "pred = torch.max(outputs.data, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,pred = torch.max(outputs.data, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 7, 0, 0, 2, 7, 7, 0, 7, 0, 1, 0, 7, 2, 7, 0, 7, 0, 1, 7, 0, 0, 0,\n",
       "        0, 7, 0, 7, 0, 0, 0, 2, 1, 0, 0, 7, 1, 0, 0, 2, 7, 0, 1, 2, 7, 0, 7, 1,\n",
       "        0, 0, 7, 0, 7, 5, 2, 0, 0, 7, 6, 7, 7, 0, 0, 0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.9298, 1.2026, 0.6502, 1.1211, 0.9899, 0.9910, 1.1116, 0.8522, 0.8552,\n",
       "        1.0018, 0.9979, 0.6841, 0.9379, 0.9852, 1.1593, 0.9813, 0.8237, 0.6305,\n",
       "        1.0953, 1.0506, 0.8455, 0.9373, 0.8580, 0.9218, 1.0175, 0.9351, 0.9047,\n",
       "        0.9147, 0.8417, 0.7850, 0.9282, 0.8050, 0.7320, 0.9619, 1.2122, 0.8165,\n",
       "        0.9746, 1.1539, 1.0139, 0.7487, 0.9252, 0.8604, 0.9838, 1.0982, 0.5970,\n",
       "        1.0163, 0.6914, 1.0407, 0.9922, 0.8459, 0.9055, 0.6855, 0.9879, 0.7230,\n",
       "        0.6339, 1.2602, 0.7797, 0.8352, 0.8497, 1.2148, 1.2435, 0.5974, 1.0332,\n",
       "        0.7556]),\n",
       "indices=tensor([7, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 7, 2, 7, 7, 0, 0, 7, 0, 0, 7, 0, 0, 0,\n",
       "        7, 1, 7, 5, 0, 0, 1, 2, 5, 0, 0, 7, 0, 0, 7, 2, 0, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 0, 1, 7, 2, 5, 2, 7, 7, 1, 6, 7, 0, 7, 7, 0]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(outputs.data, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = data[3]\n",
    "total = labels.size(0)\n",
    "(pred == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
